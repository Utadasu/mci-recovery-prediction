#!/usr/bin/env python3
"""
ğŸ¯ å¯¹æŠ—æ€§å¯¹æ¯”å­¦ä¹ æ¨¡å‹ - å¢å¼ºç‰ˆ v2.1
====================

ğŸ”¥ v2.1 è¿›åº¦æ¡ä¼˜åŒ–ç‰ˆæœ¬
ä¸»è¦æ”¹è¿›:
- âœ… ä¼˜åŒ–é‡å¤è­¦å‘Šä¿¡æ¯ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡æ˜¾ç¤º
- âœ… é™é»˜å¤„ç†æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œåªåœ¨epochçº§åˆ«æ˜¾ç¤ºå…³é”®ä¿¡æ¯  
- âœ… ç®€åŒ–è®­ç»ƒè¿‡ç¨‹è¾“å‡ºï¼Œæ›´æ¸…æ™°çš„æ‰¹æ¬¡ç›‘æ§
- âœ… å¢å¼ºè¿›åº¦æ¡ä¿¡æ¯å¯†åº¦ï¼ŒåŒ…å«æ›´å¤šæœ‰ç”¨æŒ‡æ ‡
- âœ… æ¯5ä¸ªæ‰¹æ¬¡æ›´æ–°è¿›åº¦æ¡ï¼Œå‡å°‘é¢‘ç¹åˆ·æ–°

æ ¸å¿ƒæ€æƒ³ï¼š
1. ä¿ç•™MMSE/CDR-SBç­‰è®¤çŸ¥è¯„ä¼°åˆ†æ•°ï¼ˆæœ‰ä»·å€¼çš„åŒ»å­¦ç‰¹å¾ï¼‰
2. é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒè®©æ–‡æœ¬ç¼–ç å™¨å­¦ä¹ "å»å"çš„è¡¨å¾
3. å¼ºåˆ¶å›¾åƒ-æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œå‡å°‘å¯¹è®¤çŸ¥åˆ†æ•°çš„ç›´æ¥ä¾èµ–
4. ä½¿ç”¨ç‰¹å¾è§£è€¦æŠ€æœ¯åˆ†ç¦»è¯Šæ–­ç›¸å…³å’Œè¯Šæ–­æ— å…³ç‰¹å¾

ğŸ”¥ v2.1æ–°å¢åŠŸèƒ½ï¼š
1. è‡ªé€‚åº”æŸå¤±æƒé‡å­¦ä¹  - åŠ¨æ€è°ƒæ•´å„æŸå¤±å‡½æ•°æƒé‡
2. MMSEåˆ†æ•°åˆ†æ¡¶å’Œéçº¿æ€§å˜æ¢ - å‡è½»è®¤çŸ¥åˆ†æ•°æ³„éœ²
3. æ­£è´Ÿæ ·æœ¬å¯¹æ„å»º - ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ æ•ˆæœ
4. é™é»˜æ•°å€¼ç¨³å®šæ€§å¤„ç† - æ¸…æ™°çš„è®­ç»ƒç›‘æ§ç•Œé¢

æŠ€æœ¯æ¶æ„:
- å›¾åƒç¼–ç å™¨: ImprovedResNetCBAM3D + æ™ºèƒ½ä¸‹é‡‡æ ·
- æ–‡æœ¬ç¼–ç å™¨: å¤šå…ƒå›å½’è®¤çŸ¥è¯„ä¼° + BERTç¼–ç   
- å¯¹æ¯”å­¦ä¹ : InfoNCEåŒå‘æŸå¤± + ç‰¹å¾è§£è€¦
- èåˆåˆ†ç±»: è‡ªé€‚åº”æƒé‡å­¦ä¹  + å›¾åƒä¸»å¯¼ç­–ç•¥

æ€§èƒ½ç›®æ ‡: åŸºäº94.74%æœ€ä½³å›¾åƒç¼–ç å™¨ï¼Œæå‡è‡³â‰¥95%å¤šæ¨¡æ€å‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import pickle
import os
from tqdm import tqdm
import json
import pandas as pd
import math
import re
from sklearn.preprocessing import StandardScaler
from transformers import AutoTokenizer, AutoModel
import logging
from datetime import datetime
import argparse  # æ–°å¢å‘½ä»¤è¡Œå‚æ•°è§£æ
from sklearn.model_selection import StratifiedKFold, LeaveOneOut
import openpyxl
import torch.cuda.amp as amp
from torch.cuda.amp import GradScaler
import warnings
import random

warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.functional')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # for multi-GPU.
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"ğŸŒ± éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


class AdaptiveLossWeights(nn.Module):
    """ğŸ”¥ è‡ªé€‚åº”æŸå¤±æƒé‡å­¦ä¹ æ¨¡å—"""
    
    def __init__(self, num_losses=7, device='cuda', initial_weights=None):
        super(AdaptiveLossWeights, self).__init__()
        self.num_losses = num_losses
        
        # ğŸ¯ å¯å­¦ä¹ çš„æŸå¤±æƒé‡å‚æ•°
        # ä½¿ç”¨logç©ºé—´å­¦ä¹ ï¼Œç¡®ä¿æƒé‡ä¸ºæ­£
        self.log_weights = nn.Parameter(torch.zeros(num_losses, device=device))
        
        # æŸå¤±åç§°æ˜ å°„
        self.loss_names = [
            'classification',    # 0: åˆ†ç±»æŸå¤±
            'alignment',         # 1: è·¨æ¨¡æ€å¯¹é½æŸå¤±  
            'contrastive',       # 2: å¯¹æ¯”æŸå¤±
            'reconstruction',    # 3: é‡æ„æŸå¤±
            'orthogonality',     # 4: æ­£äº¤æ€§æŸå¤±
            'diagnostic',        # 5: è¯Šæ–­æŸå¤±
            'dominance'          # 6: å›¾åƒä¸»å¯¼æŸå¤±
            # åˆ é™¤ 'text_suppression'
        ]
        
        # ğŸ”§ æƒé‡çº¦æŸèŒƒå›´
        self.min_weight = 0.001
        self.max_weight = 10.0
        
        print(f"ğŸ¯ è‡ªé€‚åº”æŸå¤±æƒé‡å­¦ä¹ å™¨åˆå§‹åŒ–:")
        print(f"   æŸå¤±æ•°é‡: {num_losses}")
        print(f"   æƒé‡èŒƒå›´: [{self.min_weight}, {self.max_weight}]")
        print(f"   æŸå¤±ç±»å‹: {self.loss_names}")
    
    def get_weights(self):
        """è·å–å½“å‰çš„æŸå¤±æƒé‡"""
        # ä»logç©ºé—´è½¬æ¢åˆ°å®é™…æƒé‡ï¼Œå¹¶åº”ç”¨çº¦æŸ
        weights = torch.exp(self.log_weights)
        weights = torch.clamp(weights, self.min_weight, self.max_weight)
        return weights
    
    def forward(self, losses_dict, epoch=0):
        """
        è®¡ç®—è‡ªé€‚åº”æŸå¤±æƒé‡
        
        Args:
            losses_dict: æŸå¤±å­—å…¸ï¼ŒåŒ…å«å„ç§æŸå¤±
            epoch: å½“å‰è®­ç»ƒè½®æ•°
            
        Returns:
            total_loss: åŠ æƒæ€»æŸå¤±
            weights_dict: å½“å‰æƒé‡å­—å…¸
        """
        # è·å–æ‰€æœ‰æŸå¤±åç§°
        loss_names = list(losses_dict.keys())
        
        # è·å–å½“å‰æƒé‡
        weights = self.get_weights()  # [num_losses]
        
        # ğŸ”¥ æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´åˆå§‹æƒé‡ - ç¡®ä¿åˆ†ç±»æŸå¤±çš„ä¸»å¯¼åœ°ä½
        if epoch < 5:
            # æ—©æœŸé˜¶æ®µ: åˆ†ç±»å’Œå¯¹é½å¹¶é‡
            weights_dict = {
                'classification': 1.5,     # åˆ†ç±»æŸå¤±æƒé‡
                'alignment': 1.0,          # å¯¹é½æŸå¤±æƒé‡
                'contrastive': 0.5,        # å¯¹æ¯”æŸå¤±æƒé‡
                'reconstruction': 0.5,     # é‡æ„æŸå¤±æƒé‡
                'orthogonality': 0.5,      # æ­£äº¤æ€§æŸå¤±æƒé‡
                'diagnostic': 0.3,         # è¯Šæ–­æŸå¤±æƒé‡
                'dominance': 0.8           # ç‰¹å¾ä¸»å¯¼æ€§æŸå¤±æƒé‡
            }
        elif epoch < 15:
            # ä¸­æœŸé˜¶æ®µ: å¼ºåŒ–å¯¹é½ï¼Œä½†åˆ†ç±»ä»æ˜¯ä¸»å¯¼
            weights_dict = {
                'classification': 1.5,     # åˆ†ç±»æŸå¤±æƒé‡
                'alignment': 1.5,          # å¯¹é½æŸå¤±æƒé‡
                'contrastive': 1.0,        # å¯¹æ¯”æŸå¤±æƒé‡
                'reconstruction': 0.7,     # é‡æ„æŸå¤±æƒé‡
                'orthogonality': 0.7,      # æ­£äº¤æ€§æŸå¤±æƒé‡
                'diagnostic': 0.5,         # è¯Šæ–­æŸå¤±æƒé‡
                'dominance': 1.0           # ç‰¹å¾ä¸»å¯¼æ€§æŸå¤±æƒé‡
            }
        else: # epoch >= 15
            # åæœŸé˜¶æ®µ: ç¨³å®šå¯¹é½ï¼Œåˆ†ç±»ä»»åŠ¡æƒé‡æœ€é«˜
            weights_dict = {
                'classification': 2.0,     # ğŸ”¥ ç¡®ä¿åˆ†ç±»ä»»åŠ¡æ‹¥æœ‰æœ€é«˜æƒé‡
                'alignment': 1.0,          # å¯¹é½æŸå¤±ä½œä¸ºè¾…åŠ©
                'contrastive': 0.8,        # å¯¹æ¯”æŸå¤±ä½œä¸ºè¾…åŠ©
                'reconstruction': 0.8,     # é‡æ„æŸå¤±æƒé‡
                'orthogonality': 0.8,      # æ­£äº¤æ€§æŸå¤±æƒé‡
                'diagnostic': 0.3,         # è¯Šæ–­æŸå¤±æƒé‡
                'dominance': 1.2           # ç‰¹å¾ä¸»å¯¼æ€§æŸå¤±æƒé‡
            }
        
        # æ„å»ºæŸå¤±å‘é‡
        loss_values = []
        adjusted_weights = []
        
        # ä¸ºæ¯ä¸ªæŸå¤±åº”ç”¨æƒé‡
        for i, loss_name in enumerate(self.loss_names):
            if loss_name in losses_dict:
                loss_values.append(losses_dict[loss_name])
                # åº”ç”¨é¢„å®šä¹‰æƒé‡ï¼ˆå¦‚æœæŸå¤±åç§°åŒ¹é…ï¼‰
                if loss_name in weights_dict:
                    adjusted_weights.append(weights_dict[loss_name])
                else:
                    adjusted_weights.append(weights[i])
            else:
                loss_values.append(torch.tensor(0.0, device=self.device))
                adjusted_weights.append(weights[i])
        
        # è½¬æ¢ä¸ºå¼ é‡
        loss_tensor = torch.stack(loss_values)
        adjusted_weights_tensor = torch.tensor(adjusted_weights, device=self.device)
        
        # è®¡ç®—åŠ æƒæ€»æŸå¤±
        total_loss = torch.sum(adjusted_weights_tensor * loss_tensor)
        
        # æ„å»ºæƒé‡å­—å…¸ç”¨äºç›‘æ§
        weights_monitor = {}
        for i, loss_name in enumerate(self.loss_names):
            if i < len(adjusted_weights):
                weights_monitor[f'{loss_name}_weight'] = adjusted_weights[i]
        
        return total_loss, weights_monitor


class CognitiveAssessmentProcessor(nn.Module):
    """ğŸ”¥ è®¤çŸ¥è¯„ä¼°å¤„ç†å™¨ - å¤šå…ƒå›å½’æ ¡æ­£ + CDR-SBæ•´åˆ"""
    
    def __init__(self, device='cuda'):
        super(CognitiveAssessmentProcessor, self).__init__()
        
        self.device = device
        
        # ğŸ¯ MMSEå¤šå…ƒå›å½’æ ¡æ­£å‚æ•° (åŸºäºå¾ªè¯åŒ»å­¦ç ”ç©¶)
        # åŸºäºCrum et al. (1993) JAMA - 18,056äººé˜Ÿåˆ—ç ”ç©¶
        # å¹´é¾„ã€æ€§åˆ«ã€æ•™è‚²å¯¹MMSEåˆ†æ•°çš„å¤šå…ƒå›å½’æ¨¡å‹
        self.mmse_regression_params = {
            'intercept': 29.1,           # åŸºçº¿æˆªè· (é«˜æ•™è‚²ç»„åŸºå‡†)
            'age_coef': -0.045,          # å¹´é¾„ç³»æ•° (æ¯å¹´-0.045åˆ†)
            'age_squared_coef': -0.0003, # å¹´é¾„å¹³æ–¹é¡¹ (éçº¿æ€§è€åŒ–æ•ˆåº”)
            'gender_coef': 0.1,          # æ€§åˆ«ç³»æ•° (åŸºäºå®é™…ç ”ç©¶ï¼Œå·®å¼‚å¾ˆå°)
            'education_coef': 0.35,      # æ•™è‚²ç³»æ•° (æ¯å¹´+0.35åˆ†)
            'education_squared_coef': -0.008  # æ•™è‚²å¹³æ–¹é¡¹ (è¾¹é™…é€’å‡æ•ˆåº”)
        }
        
        # ğŸ¯ CDR-SBåˆ†ç®±ç­–ç•¥ (åŸºäºMorris 1993åŸå§‹åˆ†çº§æ ‡å‡†)
        # å‚è€ƒ: Morris, J.C. (1993). Neurology, 43(11):2412-4
        # å®Œå…¨éµå¾ªåŸå§‹CDR-SBè¯„åˆ†ç³»ç»Ÿçš„ä¸¥é‡ç¨‹åº¦åˆ†çº§
        self.cdrsb_bins = {
            'normal': [0, 0.5],          # æ­£å¸¸ (CDR 0)
            'questionable': [0.5, 2.5],  # å¯ç–‘è®¤çŸ¥éšœç¢ (CDR 0.5)
            'mild': [2.5, 4.5],          # è½»åº¦ç—´å‘† (CDR 1)
            'moderate': [4.5, 9.0],      # ä¸­åº¦ç—´å‘† (CDR 2)
            'severe': [9.0, 18.0]        # é‡åº¦ç—´å‘† (CDR 3)
        }
        
        # ğŸ”§ MMSEå¤šå…ƒå›å½’æ ¡æ­£ç½‘ç»œ
        self.mmse_corrector = nn.Sequential(
            nn.Linear(5, 32),            # è¾“å…¥: [age, ageÂ², gender, education, educationÂ²]
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.Tanh(),
            nn.Linear(16, 1)             # è¾“å‡º: æ ¡æ­£å€¼
        )
        
        # ğŸ”§ MMSEç‰¹å¾ç¼–ç å™¨
        self.mmse_encoder = nn.Sequential(
            nn.Linear(2, 32),            # è¾“å…¥: [raw_score, corrected_score]
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 64)            # è¾“å‡º: 64ç»´MMSEç‰¹å¾
        )
        
        # ğŸ”§ CDR-SBåˆ†ç®±åµŒå…¥
        self.cdrsb_bin_embedding = nn.Embedding(5, 32)  # 5ä¸ªä¸¥é‡ç¨‹åº¦çº§åˆ«
        
        # ğŸ”§ CDR-SBè¿ç»­å€¼ç¼–ç å™¨
        self.cdrsb_encoder = nn.Sequential(
            nn.Linear(1, 16),
            nn.ReLU(),
            nn.Linear(16, 32)            # è¾“å‡º: 32ç»´CDR-SBç‰¹å¾
        )
        
        # ğŸ¯ å¤šæ¨¡æ€è®¤çŸ¥ç‰¹å¾èåˆå™¨
        self.cognitive_fusion = nn.Sequential(
            nn.Linear(64 + 32 + 32, 128), # MMSE(64) + CDR-SBåˆ†ç®±(32) + CDR-SBè¿ç»­(32)
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 16)            # æœ€ç»ˆè¾“å‡º: 16ç»´è®¤çŸ¥ç‰¹å¾
        )
        
        print(f"ğŸ¯ å¤šå…ƒå›å½’è®¤çŸ¥è¯„ä¼°å¤„ç†å™¨åˆå§‹åŒ–:")
        print(f"   MMSEå¤šå…ƒå›å½’æ ¡æ­£: å¹´é¾„ + å¹´é¾„Â² + æ€§åˆ« + æ•™è‚² + æ•™è‚²Â²")
        print(f"   CDR-SBåˆ†çº§: {list(self.cdrsb_bins.keys())}")
        print(f"   ç‰¹å¾èåˆ: MMSE(64) + CDR-SB(64) â†’ 16ç»´")
        print(f"   æ ¡æ­£å‚æ•°: {self.mmse_regression_params}")
    
    def extract_demographic_info(self, texts):
        """ä»æ–‡æœ¬ä¸­æå–äººå£ç»Ÿè®¡å­¦ä¿¡æ¯"""
        demographics = []
        
        for text in texts:
            demo_info = {'age': 70.0, 'gender': 0, 'education': 12.0}  # é»˜è®¤å€¼
            
            # ğŸ”¥ æå–å¹´é¾„
            age_patterns = [
                r'Age:\s*(\d+(?:\.\d+)?)\s*years',
                r'å¹´é¾„:\s*(\d+(?:\.\d+)?)\s*å²',
                r'age:\s*(\d+(?:\.\d+)?)'
            ]
            for pattern in age_patterns:
                match = re.search(pattern, text)
                if match:
                    demo_info['age'] = float(match.group(1))
                    break
            
            # ğŸ”¥ æå–æ€§åˆ« (0=ç”·æ€§, 1=å¥³æ€§)
            gender_patterns = [
                r'Gender:\s*(male|female)',
                r'æ€§åˆ«:\s*(ç”·|å¥³)',
                r'gender:\s*(male|female)'
            ]
            for pattern in gender_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    gender_str = match.group(1).lower()
                    if gender_str in ['female', 'å¥³']:
                        demo_info['gender'] = 1
                    else:
                        demo_info['gender'] = 0
                    break
            
            # ğŸ”¥ æå–æ•™è‚²å¹´é™
            edu_patterns = [
                r'Education:\s*(\d+(?:\.\d+)?)\s*years',
                r'æ•™è‚²:\s*(\d+(?:\.\d+)?)\s*å¹´',
                r'education:\s*(\d+(?:\.\d+)?)'
            ]
            for pattern in edu_patterns:
                match = re.search(pattern, text)
                if match:
                    demo_info['education'] = float(match.group(1))
                    break
            
            # ğŸ”§ æ•°æ®èŒƒå›´æ£€æŸ¥
            demo_info['age'] = max(18.0, min(120.0, demo_info['age']))
            demo_info['education'] = max(0.0, min(25.0, demo_info['education']))
            
            demographics.append(demo_info)
        
        return demographics
    
    def extract_mmse_scores(self, texts):
        """ä»æ–‡æœ¬ä¸­æå–MMSEåˆ†æ•°"""
        mmse_scores = []
        
        for text in texts:
            patterns = [
                r'Mini-Mental State Examination \(MMSE\):\s*\[(\d+(?:\.\d+)?)/30\]',
                r'MMSE:\s*\[(\d+(?:\.\d+)?)/30\]',
                r'MMSE:\s*(\d+(?:\.\d+)?)',
                r'Mini-Mental State Examination.*?(\d+(?:\.\d+)?)'
            ]
            
            mmse_score = None
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    mmse_score = float(match.group(1))
                    break
            
            if mmse_score is None:
                mmse_score = 15.0  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
            
            mmse_score = max(0.0, min(30.0, mmse_score))
            mmse_scores.append(mmse_score)
        
        return torch.tensor(mmse_scores, dtype=torch.float32, device=self.device)
    
    def extract_cdrsb_scores(self, texts):
        """ä»æ–‡æœ¬ä¸­æå–CDR-SBåˆ†æ•°"""
        cdrsb_scores = []
        
        for text in texts:
            patterns = [
                r'Clinical Dementia Rating - Sum of Boxes \(CDR-SB\):\s*\[(\d+(?:\.\d+)?)\]',
                r'CDR-SB:\s*\[(\d+(?:\.\d+)?)\]',
                r'CDR-SB:\s*(\d+(?:\.\d+)?)',
                r'Clinical Dementia Rating.*?(\d+(?:\.\d+)?)'
            ]
            
            cdrsb_score = None
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    cdrsb_score = float(match.group(1))
                    break
            
            if cdrsb_score is None:
                cdrsb_score = 2.0  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
            
            cdrsb_score = max(0.0, min(18.0, cdrsb_score))
            cdrsb_scores.append(cdrsb_score)
        
        return torch.tensor(cdrsb_scores, dtype=torch.float32, device=self.device)
    
    def compute_mmse_correction(self, demographics):
        """
        åŸºäºå¤šå› ç´ å›å½’æ¨¡å‹è®¡ç®—MMSEæ ¡æ­£å€¼
        
        Args:
            demographics: List[Dict] äººå£ç»Ÿè®¡å­¦ä¿¡æ¯
            
        Returns:
            corrections: Tensor [B] æ ¡æ­£å€¼
        """
        batch_size = len(demographics)
        regression_features = torch.zeros(batch_size, 5, device=self.device)
        
        for i, demo in enumerate(demographics):
            age = demo['age']
            gender = demo['gender']
            education = demo['education']
            
            # æ„å»ºå›å½’ç‰¹å¾ [age, ageÂ², gender, education, educationÂ²]
            regression_features[i, 0] = age
            regression_features[i, 1] = age ** 2
            regression_features[i, 2] = gender
            regression_features[i, 3] = education
            regression_features[i, 4] = education ** 2
        
        # ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œéçº¿æ€§æ ¡æ­£ (æ¯”çº¿æ€§å›å½’æ›´çµæ´»)
        corrections = self.mmse_corrector(regression_features).squeeze()
        return corrections
    
    def get_cdrsb_bins(self, cdrsb_scores):
        """å°†CDR-SBåˆ†æ•°è½¬æ¢ä¸ºåˆ†ç®±æ ‡ç­¾"""
        bins = torch.zeros(len(cdrsb_scores), dtype=torch.long, device=self.device)
        
        for i, score in enumerate(cdrsb_scores):
            score_val = score.item()
            
            if 0 <= score_val < 0.5:
                bins[i] = 0  # normal
            elif 0.5 <= score_val < 2.5:
                bins[i] = 1  # questionable
            elif 2.5 <= score_val < 4.5:
                bins[i] = 2  # mild
            elif 4.5 <= score_val < 9.0:
                bins[i] = 3  # moderate
            else:  # >= 9.0
                bins[i] = 4  # severe
        
        return bins
    
    def forward(self, texts):
        """
        å¤šå…ƒå›å½’è®¤çŸ¥è¯„ä¼°å¤„ç†
        
        Args:
            texts: List[str] æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            cognitive_features: [B, 16] èåˆè®¤çŸ¥ç‰¹å¾
        """
        # ğŸ”¥ Step 1: æå–æ‰€æœ‰è®¤çŸ¥å’Œäººå£ç»Ÿè®¡å­¦ä¿¡æ¯
        demographics = self.extract_demographic_info(texts)
        mmse_scores = self.extract_mmse_scores(texts)      # [B]
        cdrsb_scores = self.extract_cdrsb_scores(texts)    # [B]
        
        # ğŸ”¥ Step 2: å¤šå…ƒå›å½’MMSEæ ¡æ­£
        mmse_corrections = self.compute_mmse_correction(demographics)  # [B]
        corrected_mmse = mmse_scores - mmse_corrections  # å¤šå…ƒå›å½’æ ¡æ­£
        
        # æ ‡å‡†åŒ–åˆ°[-1, 1]èŒƒå›´
        normalized_raw = (mmse_scores - 15.0) / 15.0
        normalized_corrected = (corrected_mmse - 15.0) / 15.0
        
        # MMSEç‰¹å¾ç¼–ç  (åŒ…å«åŸå§‹åˆ†æ•°å’Œæ ¡æ­£åˆ†æ•°)
        mmse_input = torch.stack([normalized_raw, normalized_corrected], dim=1)  # [B, 2]
        mmse_features = self.mmse_encoder(mmse_input)  # [B, 64]
        
        # ğŸ”¥ Step 3: CDR-SBåŒè·¯å¾„å¤„ç†
        # 3.1 åˆ†ç®±è·¯å¾„
        cdrsb_bins = self.get_cdrsb_bins(cdrsb_scores)  # [B]
        cdrsb_bin_features = self.cdrsb_bin_embedding(cdrsb_bins)  # [B, 32]
        
        # 3.2 è¿ç»­å€¼è·¯å¾„
        normalized_cdrsb = (cdrsb_scores - 4.5) / 4.5  # æ ‡å‡†åŒ–åˆ°[-1, 1]
        cdrsb_continuous_features = self.cdrsb_encoder(normalized_cdrsb.unsqueeze(1))  # [B, 32]
        
        # ğŸ”¥ Step 4: å¤šæ¨¡æ€è®¤çŸ¥ç‰¹å¾èåˆ
        combined_features = torch.cat([
            mmse_features,              # [B, 64] å¤šå…ƒå›å½’æ ¡æ­£MMSEç‰¹å¾
            cdrsb_bin_features,         # [B, 32] CDR-SBåˆ†ç®±ç‰¹å¾
            cdrsb_continuous_features   # [B, 32] CDR-SBè¿ç»­ç‰¹å¾
        ], dim=1)  # [B, 128]
        
        cognitive_features = self.cognitive_fusion(combined_features)  # [B, 16]
        
        return cognitive_features


# ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªåˆ«å
MMSEProcessor = CognitiveAssessmentProcessor


class ContrastiveSampler:
    """ğŸ”¥ å¯¹æ¯”å­¦ä¹ æ­£è´Ÿæ ·æœ¬æ„å»ºå™¨"""
    
    def __init__(self, temperature=0.05, hard_negative_ratio=0.3):  # ğŸ”¥ æ¸©åº¦ä»0.1é™ä½åˆ°0.05ï¼Œå¢å¼ºADæ ·æœ¬å¯¹é½
        self.temperature = temperature
        self.hard_negative_ratio = hard_negative_ratio
        
        print(f"ğŸ¯ å¯¹æ¯”å­¦ä¹ é‡‡æ ·å™¨åˆå§‹åŒ–:")
        print(f"   æ¸©åº¦å‚æ•°: {temperature} (ğŸ”¥ ä¼˜åŒ–å - é™ä½æ¸©åº¦å¢å¼ºADæ ·æœ¬å¯¹é½)")
        print(f"   å›°éš¾è´Ÿæ ·æœ¬æ¯”ä¾‹: {hard_negative_ratio}")
    
    def create_positive_pairs(self, image_features, text_features, labels):
        """åˆ›å»ºæ­£æ ·æœ¬å¯¹"""
        batch_size = image_features.size(0)
        
        # åŒç±»æ ·æœ¬ä½œä¸ºæ­£æ ·æœ¬å¯¹
        positive_pairs = []
        positive_labels = []
        
        for i in range(batch_size):
            for j in range(i + 1, batch_size):
                if labels[i] == labels[j]:
                    # å›¾åƒ-æ–‡æœ¬æ­£æ ·æœ¬å¯¹
                    positive_pairs.append((image_features[i], text_features[j]))
                    positive_pairs.append((image_features[j], text_features[i]))
                    positive_labels.extend([1, 1])
        
        return positive_pairs, positive_labels
    
    def create_negative_pairs(self, image_features, text_features, labels):
        """åˆ›å»ºè´Ÿæ ·æœ¬å¯¹"""
        batch_size = image_features.size(0)
        
        # ä¸åŒç±»æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬å¯¹
        negative_pairs = []
        negative_labels = []
        
        for i in range(batch_size):
            for j in range(batch_size):
                if labels[i] != labels[j]:
                    # å›¾åƒ-æ–‡æœ¬è´Ÿæ ·æœ¬å¯¹
                    negative_pairs.append((image_features[i], text_features[j]))
                    negative_labels.append(0)
        
        return negative_pairs, negative_labels
    
    def compute_contrastive_loss(self, image_features, text_features, labels):
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆä¿®å¤æ•°å€¼ç¨³å®šæ€§å’Œå°æ‰¹æ¬¡é—®é¢˜ï¼‰
        
        Args:
            image_features: [B, D] å›¾åƒç‰¹å¾
            text_features: [B, D] æ–‡æœ¬ç‰¹å¾  
            labels: [B] æ ‡ç­¾
            
        Returns:
            contrastive_loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        batch_size = image_features.size(0)
        device = image_features.device
        
        # ğŸ”§ æ•°å€¼ç¨³å®šæ€§é¢„æ£€æŸ¥
        if not torch.isfinite(image_features).all() or not torch.isfinite(text_features).all():
            # ğŸ”¥ é™é»˜å¤„ç†ï¼Œé¿å…é‡å¤è­¦å‘Š
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        # ğŸ”¥ ç¡®ä¿ç‰¹å¾å·²æ ‡å‡†åŒ–
        image_features = F.normalize(image_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)
        
        # ğŸ¯ è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ - æ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
        # å›¾åƒåˆ°æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        sim_i2t = torch.matmul(image_features, text_features.t()) / self.temperature  # [B, B]
        # æ–‡æœ¬åˆ°å›¾åƒçš„ç›¸ä¼¼åº¦  
        sim_t2i = torch.matmul(text_features, image_features.t()) / self.temperature  # [B, B]
        
        # ğŸ”¥ é™åˆ¶ç›¸ä¼¼åº¦èŒƒå›´ï¼Œé˜²æ­¢expçˆ†ç‚¸
        sim_i2t = torch.clamp(sim_i2t, min=-10.0, max=10.0)
        sim_t2i = torch.clamp(sim_t2i, min=-10.0, max=10.0)
        
        # ğŸ”§ åˆ›å»ºæ ‡ç­¾æ©ç  - ä¿®å¤æ­£è´Ÿæ ·æœ¬åˆ¤æ–­é€»è¾‘
        labels_expanded = labels.unsqueeze(1)  # [B, 1]
        labels_matrix = labels_expanded == labels_expanded.t()  # [B, B] åŒç±»ä¸ºTrue
        
        # ğŸ”¥ ä¿®å¤ï¼šæ­£æ ·æœ¬æ©ç åº”è¯¥æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±ï¼‰
        identity_mask = torch.eye(batch_size, device=device, dtype=torch.bool)
        pos_mask_i2t = labels_matrix & (~identity_mask)  # åŒç±»ä½†éè‡ªå·±
        neg_mask_i2t = (~labels_matrix)  # ä¸åŒç±»
        
        # ğŸ”§ æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬å¯¹
        pos_count = pos_mask_i2t.sum().item()
        neg_count = neg_mask_i2t.sum().item()
        
        # ğŸ”¥ æ”¹è¿›çš„å¤„ç†ç­–ç•¥ï¼šå¦‚æœç¼ºå°‘æ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬ï¼Œä½¿ç”¨ç®€åŒ–çš„InfoNCE
        if pos_count == 0 or neg_count == 0:
            # ğŸ”§ é™é»˜å¤„ç†ï¼Œåªåœ¨epochå¼€å§‹æ—¶æ˜¾ç¤ºä¸€æ¬¡è­¦å‘Š
            if not hasattr(self, '_small_batch_warning_shown'):
                print(f"ğŸ”§ æ£€æµ‹åˆ°å°æ‰¹æ¬¡æˆ–å•ä¸€ç±»åˆ«æ‰¹æ¬¡ï¼Œä½¿ç”¨ç®€åŒ–å¯¹æ¯”å­¦ä¹ ")
                self._small_batch_warning_shown = True
            
            # ğŸ¯ ä½¿ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼šå›¾åƒä¸æ–‡æœ¬ç‰¹å¾çš„å¯¹é½
            # ä¸åŒºåˆ†æ­£è´Ÿæ ·æœ¬ï¼Œç›´æ¥æœ€å¤§åŒ–å¯¹åº”æ ·æœ¬çš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦
            diagonal_sim = torch.diag(sim_i2t)  # [B] å¯¹åº”æ ·æœ¬çš„ç›¸ä¼¼åº¦
            
            # ä½¿ç”¨ç®€å•çš„å¯¹é½æŸå¤±ï¼šæœ€å¤§åŒ–å¯¹åº”æ ·æœ¬çš„ç›¸ä¼¼åº¦
            alignment_loss = -torch.mean(diagonal_sim)
            
            return alignment_loss
        
        # ğŸ”¥ æ ‡å‡†InfoNCEæŸå¤±è®¡ç®— - ä½¿ç”¨æ•°å€¼ç¨³å®šçš„æ–¹æ³•
        try:
            # ğŸ¯ å›¾åƒåˆ°æ–‡æœ¬çš„å¯¹æ¯”æŸå¤±
            pos_mask_float = pos_mask_i2t.float()
            neg_mask_float = neg_mask_i2t.float()
            
            # è®¡ç®—æ¯è¡Œçš„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
            loss_i2t_list = []
            for i in range(batch_size):
                # è·å–ç¬¬iè¡Œçš„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
                pos_sims = sim_i2t[i][pos_mask_i2t[i]]  # ç¬¬iä¸ªå›¾åƒä¸åŒç±»æ–‡æœ¬çš„ç›¸ä¼¼åº¦
                neg_sims = sim_i2t[i][neg_mask_i2t[i]]  # ç¬¬iä¸ªå›¾åƒä¸å¼‚ç±»æ–‡æœ¬çš„ç›¸ä¼¼åº¦
                
                if len(pos_sims) == 0:
                    # å¦‚æœæ²¡æœ‰æ­£æ ·æœ¬ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                    loss_i2t_list.append(torch.tensor(0.0, device=device))
                    continue
                
                if len(neg_sims) == 0:
                    # å¦‚æœæ²¡æœ‰è´Ÿæ ·æœ¬ï¼Œåªè®¡ç®—æ­£æ ·æœ¬æŸå¤±
                    loss_i2t_list.append(-torch.mean(pos_sims))
                    continue
                
                # ğŸ”¥ ä½¿ç”¨log-sum-expæŠ€å·§è®¡ç®—InfoNCEæŸå¤±
                # InfoNCE: -log(exp(pos) / (exp(pos) + sum(exp(neg))))
                # = -pos + log(exp(pos) + sum(exp(neg)))
                # = -pos + logsumexp([pos, neg])
                
                all_sims = torch.cat([pos_sims, neg_sims])  # åˆå¹¶æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
                log_sum_exp = torch.logsumexp(all_sims, dim=0)
                
                # å¯¹äºå¤šä¸ªæ­£æ ·æœ¬ï¼Œå–å¹³å‡
                pos_avg = torch.mean(pos_sims)
                loss_i = -(pos_avg - log_sum_exp)
                loss_i2t_list.append(loss_i)
            
            loss_i2t = torch.stack(loss_i2t_list)
            
            # ğŸ¯ æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹æ¯”æŸå¤± - åŒæ ·å¤„ç†
            loss_t2i_list = []
            for i in range(batch_size):
                pos_sims = sim_t2i[i][pos_mask_i2t[i]]  # æ³¨æ„ï¼šä½¿ç”¨ç›¸åŒçš„æ©ç 
                neg_sims = sim_t2i[i][neg_mask_i2t[i]]
                
                if len(pos_sims) == 0:
                    loss_t2i_list.append(torch.tensor(0.0, device=device))
                    continue
                
                if len(neg_sims) == 0:
                    loss_t2i_list.append(-torch.mean(pos_sims))
                    continue
                
                all_sims = torch.cat([pos_sims, neg_sims])
                log_sum_exp = torch.logsumexp(all_sims, dim=0)
                pos_avg = torch.mean(pos_sims)
                loss_i = -(pos_avg - log_sum_exp)
                loss_t2i_list.append(loss_i)
            
            loss_t2i = torch.stack(loss_t2i_list)
            
        except Exception as e:
            # ğŸ”¥ å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            if not hasattr(self, '_fallback_warning_shown'):
                print(f"ğŸ”§ å¯¹æ¯”å­¦ä¹ è®¡ç®—å¼‚å¸¸ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
                self._fallback_warning_shown = True
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„å¯¹é½æŸå¤±
            diagonal_sim = torch.diag(sim_i2t)
            return -torch.mean(diagonal_sim)
        
        # ğŸ”§ æœ€ç»ˆæ•°å€¼æ£€æŸ¥ï¼ˆé™é»˜å¤„ç†ï¼‰
        if not torch.isfinite(loss_i2t).all():
            loss_i2t = torch.zeros_like(loss_i2t)
        
        if not torch.isfinite(loss_t2i).all():
            loss_t2i = torch.zeros_like(loss_t2i)
        
        # ğŸ”§ æ€»å¯¹æ¯”æŸå¤±
        contrastive_loss = (torch.mean(loss_i2t) + torch.mean(loss_t2i)) / 2
        
        # æœ€ç»ˆæ£€æŸ¥ï¼ˆé™é»˜å¤„ç†ï¼‰
        if not torch.isfinite(contrastive_loss):
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        return contrastive_loss


class FeatureDisentanglementLoss(nn.Module):
    """ç‰¹å¾è§£è€¦æŸå¤± - åˆ†ç¦»è¯Šæ–­ç›¸å…³å’Œæ— å…³ç‰¹å¾"""
    
    def __init__(self, feature_dim=512, disentangle_dim=256):
        super(FeatureDisentanglementLoss, self).__init__()
        
        self.feature_dim = feature_dim
        self.disentangle_dim = disentangle_dim
        
        # ç‰¹å¾åˆ†ç¦»å™¨ï¼šå°†512ç»´ç‰¹å¾åˆ†ä¸ºä¸¤éƒ¨åˆ†
        # è¯Šæ–­ç›¸å…³ç‰¹å¾ (256ç»´) å’Œ è¯Šæ–­æ— å…³ç‰¹å¾ (256ç»´)
        self.diagnostic_projector = nn.Sequential(
            nn.Linear(feature_dim, disentangle_dim),
            nn.LayerNorm(disentangle_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.non_diagnostic_projector = nn.Sequential(
            nn.Linear(feature_dim, disentangle_dim),
            nn.LayerNorm(disentangle_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # è¯Šæ–­é¢„æµ‹å™¨ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        self.diagnostic_classifier = nn.Sequential(
            nn.Linear(disentangle_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 2)  # AD vs CN
        )
        
        # é‡æ„å™¨ï¼ˆç¡®ä¿ä¿¡æ¯ä¸ä¸¢å¤±ï¼‰
        self.reconstructor = nn.Sequential(
            nn.Linear(disentangle_dim * 2, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU()
        )
    
    def forward(self, text_features, labels):
        """
        ç‰¹å¾è§£è€¦å‰å‘ä¼ æ’­
        
        Args:
            text_features: [B, 512] æ–‡æœ¬ç‰¹å¾
            labels: [B] çœŸå®æ ‡ç­¾
            
        Returns:
            dict: åŒ…å«å„ç§æŸå¤±å’Œç‰¹å¾çš„å­—å…¸
        """
        batch_size = text_features.size(0)
        
        # 1. ç‰¹å¾åˆ†ç¦»
        diagnostic_features = self.diagnostic_projector(text_features)      # [B, 256] è¯Šæ–­ç›¸å…³
        non_diagnostic_features = self.non_diagnostic_projector(text_features)  # [B, 256] è¯Šæ–­æ— å…³
        
        # 2. è¯Šæ–­é¢„æµ‹ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        diagnostic_logits = self.diagnostic_classifier(diagnostic_features)
        
        # 3. ç‰¹å¾é‡æ„ï¼ˆç¡®ä¿ä¿¡æ¯ä¿æŒï¼‰
        combined_features = torch.cat([diagnostic_features, non_diagnostic_features], dim=1)
        reconstructed_features = self.reconstructor(combined_features)
        
        # 4. è®¡ç®—å„ç§æŸå¤±
        
        # è¯Šæ–­æŸå¤±ï¼ˆå¸Œæœ›è¯Šæ–­ç›¸å…³ç‰¹å¾èƒ½é¢„æµ‹æ ‡ç­¾ï¼‰
        diagnostic_loss = F.cross_entropy(diagnostic_logits, labels)
        
        # é‡æ„æŸå¤±ï¼ˆç¡®ä¿ä¿¡æ¯ä¸ä¸¢å¤±ï¼‰
        reconstruction_loss = F.mse_loss(reconstructed_features, text_features)
        
        # æ­£äº¤æ€§æŸå¤±ï¼ˆç¡®ä¿ä¸¤éƒ¨åˆ†ç‰¹å¾ç›¸äº’ç‹¬ç«‹ï¼‰
        # è®¡ç®—è¯Šæ–­å’Œéè¯Šæ–­ç‰¹å¾çš„ç›¸å…³æ€§ï¼Œå¸Œæœ›å…¶ä¸º0
        diagnostic_norm = F.normalize(diagnostic_features, p=2, dim=1)
        non_diagnostic_norm = F.normalize(non_diagnostic_features, p=2, dim=1)
        
        # æ‰¹æ¬¡å†…ç›¸å…³æ€§
        correlation = torch.sum(diagnostic_norm * non_diagnostic_norm, dim=1).mean()
        orthogonality_loss = correlation ** 2  # å¸Œæœ›ç›¸å…³æ€§ä¸º0
        
        return {
            'diagnostic_features': diagnostic_features,
            'non_diagnostic_features': non_diagnostic_features,
            'reconstructed_features': reconstructed_features,
            'diagnostic_logits': diagnostic_logits,
            'diagnostic_loss': diagnostic_loss,
            'reconstruction_loss': reconstruction_loss,
            'orthogonality_loss': orthogonality_loss
        }


class AdversarialTextEncoder(nn.Module):
    """
    ğŸ”¥ å¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç å™¨ (V2.2 - æ”¯æŒæ¶ˆèå®éªŒ)
    - é›†æˆBERTã€è®¤çŸ¥è¯„ä¼°ã€ç‰¹å¾èåˆå’Œå¯¹æŠ—æ€§æŠ•å½±
    - å¯é€šè¿‡ 'use_cognitive_features' å¼€å…³æ§åˆ¶æ˜¯å¦ä½¿ç”¨è®¤çŸ¥åˆ†æ•°
    """
    def __init__(self, feature_dim=512, device='cuda', max_length=512, use_cognitive_features=True):
        super(AdversarialTextEncoder, self).__init__()
        
        self.device = device
        self.max_length = max_length
        self.feature_dim = feature_dim
        self.use_cognitive_features = use_cognitive_features
        
        # 1. BERTæ¨¡å‹å’Œåˆ†è¯å™¨
        self.bert_model_name = 'bert-base-uncased'
        self.bert_model = AutoModel.from_pretrained(self.bert_model_name)
        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)
        
        # 2. è®¤çŸ¥è¯„ä¼°å¤„ç†å™¨
        self.mmse_processor = CognitiveAssessmentProcessor(device=device)
        
        # 3. BERTä¸è®¤çŸ¥ç‰¹å¾èåˆå±‚
        # è¾“å…¥ç»´åº¦: BERT(768) + è®¤çŸ¥(16) = 784
        self.bert_mmse_fusion = nn.Sequential(
            nn.Linear(768 + 16, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 768)
        )
        
        # 4. å¯¹æŠ—æ€§æŠ•å½±å±‚ (å°†768ç»´ç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€çš„512ç»´ç©ºé—´)
        self.adversarial_projection = nn.Sequential(
            nn.Linear(768, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, feature_dim)
        )
        
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.bert_mmse_fusion.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
        for m in self.adversarial_projection.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

    def encode_texts(self, texts):
        """
        æ ¸å¿ƒæ–‡æœ¬ç¼–ç æµç¨‹ (V2.2 - æ”¯æŒæ¶ˆè)
        """
        # æ­¥éª¤ 1: BERTç¼–ç 
        inputs = self.bert_tokenizer(
            texts, 
            return_tensors='pt', 
            padding=True, 
            truncation=True, 
            max_length=self.max_length
        ).to(self.device)
        
        bert_features = self.bert_model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask']
        )[0][:, 0, :] # (B, 768)

        # æ­¥éª¤ 2: è®¤çŸ¥å¤„ç† (å¦‚æœå¯ç”¨)
        if self.use_cognitive_features:
            cognitive_features = self.mmse_processor(texts)
            # ç¡®ä¿åœ¨æ‰¹å¤„ç†å¤§å°ä¸º1æ—¶ï¼Œè®¤çŸ¥ç‰¹å¾ä¹Ÿèƒ½æ­£ç¡®å¯¹é½
            if bert_features.size(0) == 1 and cognitive_features.size(0) > 1:
                cognitive_features = cognitive_features[0].unsqueeze(0)

            # æ­¥éª¤ 3: ç‰¹å¾æ‹¼æ¥
            features_to_fuse = torch.cat([bert_features, cognitive_features], dim=1) # (B, 784)
            # æ­¥éª¤ 4: BERT-è®¤çŸ¥èåˆ
            fused_features = self.bert_mmse_fusion(features_to_fuse) # (B, 768)
        else:
            # å¦‚æœç¦ç”¨è®¤çŸ¥ç‰¹å¾ï¼Œåˆ™è·³è¿‡æ‹¼æ¥å’Œèåˆï¼Œç›´æ¥ä½¿ç”¨BERTç‰¹å¾
            fused_features = bert_features # (B, 768)

        # æ­¥éª¤ 5: å¯¹æŠ—æ€§æŠ•å½±
        projected_features = self.adversarial_projection(fused_features) # (B, 512)
        
        return projected_features

    def forward(self, texts, labels=None):
        """
        å‰å‘ä¼ æ’­ (V2.2 ç®€åŒ–)
        """
        text_features_512d = self.encode_texts(texts)
        return text_features_512d


class AdversarialContrastiveModel(nn.Module):
    """å¯¹æŠ—æ€§å¯¹æ¯”å­¦ä¹ æ¨¡å‹ - å›¾åƒä¸»å¯¼åˆ†ç±»ï¼Œæ–‡æœ¬è¾…åŠ©å¯¹é½"""
    
    def __init__(self, image_model_path, feature_dim=512, num_classes=2, device='cuda', use_cognitive_features=True, use_disentanglement=True):
        super(AdversarialContrastiveModel, self).__init__()
        
        self.device = device
        self.feature_dim = feature_dim # ä¿å­˜feature_dim
        self.num_classes = num_classes # ä¿å­˜num_classes
        self.warning_collector = [] # ğŸ”¥ æ–°å¢: ç”¨äºæ”¶é›†è­¦å‘Šä¿¡æ¯
        
        # å›¾åƒç¼–ç å™¨ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½†å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼‰
        from optimized_contrastive_learning import ImprovedImageEncoder
        self.image_encoder = ImprovedImageEncoder(image_model_path, feature_dim, device)
        
        # ğŸ¯ å¢å¼ºå¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = AdversarialTextEncoder(feature_dim, device, use_cognitive_features=use_cognitive_features, use_disentanglement=use_disentanglement)
        
        # ğŸ”¥ æ–°å¢ï¼šè‡ªé€‚åº”æŸå¤±æƒé‡å­¦ä¹ å™¨
        self.adaptive_weights = AdaptiveLossWeights(num_losses=7, device=device)  # åˆ é™¤text_suppressionï¼Œä»8æ”¹ä¸º7
        
        # ğŸ”¥ æ–°å¢ï¼šå¯¹æ¯”å­¦ä¹ é‡‡æ ·å™¨ (ä¼˜åŒ–å‚æ•°)
        self.contrastive_sampler = ContrastiveSampler(temperature=0.05) # ä½¿ç”¨ä¹‹å‰è°ƒæ•´çš„0.05
        
        # ğŸ”¥ å…³é”®ï¼šè·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼ˆå¼ºåˆ¶å›¾åƒ-æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼‰
        self.cross_modal_aligner = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.05),
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        # ğŸ”¥ æ–°çš„å›¾åƒåˆ†ç±»å¤´: ç›´æ¥åœ¨å›¾åƒç‰¹å¾ä¸Šè¿›è¡Œåˆ†ç±»
        self.image_classifier = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(feature_dim // 2, num_classes)
        )
        
        # ğŸ”§ ç¡®ä¿æ‰€æœ‰ç»„ä»¶åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.cross_modal_aligner.to(device)
        self.image_classifier.to(device) # æ–°å¢
        
        print(f"ğŸ¯ å›¾åƒä¸»å¯¼åˆ†ç±»æ¨¡å‹é…ç½®:")
        print(f"   ğŸ”¥ å›¾åƒç¼–ç å™¨è¾“å‡ºç›´æ¥ç”¨äºåˆ†ç±»")
        print(f"   ğŸ”¥ æ–‡æœ¬ç‰¹å¾ç”¨äºè¾…åŠ©å¯¹æ¯”å­¦ä¹ å¯¹é½")
        print(f"   ğŸ”¥ è‡ªé€‚åº”æŸå¤±æƒé‡å­¦ä¹ ")
        print(f"   ğŸ”¥ å¯¹æ¯”å­¦ä¹ æ­£è´Ÿæ ·æœ¬æ„å»º (æ¸©åº¦: {self.contrastive_sampler.temperature})")
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # åˆå§‹åŒ– cross_modal_aligner å’Œæ–°çš„ image_classifier
        for m in [self.cross_modal_aligner, self.image_classifier]: 
            for layer in m.modules():
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.constant_(layer.bias, 0)
    
    def preprocess_features_for_alignment(self, image_features, text_features):
        """ğŸ”¥ å¯¹é½å‰çš„ç‰¹å¾é¢„å¤„ç† - æå‡å¯¹é½æ•ˆæœï¼ˆä¿®å¤æ•°å€¼ç¨³å®šæ€§ï¼‰"""
        
        batch_size = image_features.size(0)
        
        # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ä¿®å¤ï¼šå¤„ç†å°æ‰¹æ¬¡æƒ…å†µ
        if batch_size == 1:
            # å¯¹äºå•æ ·æœ¬æ‰¹æ¬¡ï¼Œç›´æ¥ä½¿ç”¨L2æ ‡å‡†åŒ–ï¼Œè·³è¿‡æ ‡å‡†å·®è®¡ç®—
            image_processed = F.normalize(image_features, p=2, dim=1)
            text_processed = F.normalize(text_features, p=2, dim=1)
            return image_processed, text_processed
        
        # 1. ç‰¹å¾å»ä¸­å¿ƒåŒ– - æ·»åŠ æ•°å€¼æ£€æŸ¥
        image_mean = image_features.mean(dim=0, keepdim=True)
        text_mean = text_features.mean(dim=0, keepdim=True)
        
        # æ£€æŸ¥å‡å€¼æ˜¯å¦ä¸ºæœ‰é™å€¼
        if not torch.isfinite(image_mean).all() or not torch.isfinite(text_mean).all():
            # ğŸ”¥ é™é»˜å¤„ç†ï¼Œé¿å…é‡å¤è­¦å‘Š
            if 'mean_finite' not in self.warning_collector: self.warning_collector.append('mean_finite')
            return F.normalize(image_features, p=2, dim=1), F.normalize(text_features, p=2, dim=1)
        
        image_centered = image_features - image_mean
        text_centered = text_features - text_mean
        
        # 2. ç‰¹å¾å°ºåº¦æ ‡å‡†åŒ– - ä¿®å¤æ ‡å‡†å·®è®¡ç®—
        # ğŸ”¥ ä½¿ç”¨unbiased=Falseé¿å…å°æ‰¹æ¬¡ä¸‹çš„æ•°å€¼é—®é¢˜
        image_std = image_features.std(dim=0, keepdim=True, unbiased=False) + 1e-6  # å¢å¤§epsilon
        text_std = text_features.std(dim=0, keepdim=True, unbiased=False) + 1e-6
        
        # ğŸ”§ é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ ‡å‡†å·®è¿‡å°ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆï¼ˆé™é»˜å¤„ç†ï¼‰
        min_std_threshold = 1e-4
        if (image_std < min_std_threshold).any() or (text_std < min_std_threshold).any():
            # ğŸ”¥ ä½¿ç”¨é™é»˜æ›¿ä»£æ–¹æ¡ˆï¼Œä»…åœ¨epochçº§åˆ«è®°å½•ä¸€æ¬¡
            if 'std_threshold' not in self.warning_collector: self.warning_collector.append('std_threshold')
            return F.normalize(image_features, p=2, dim=1), F.normalize(text_features, p=2, dim=1)
        
        # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦ä¸ºæœ‰é™å€¼
        if not torch.isfinite(image_std).all() or not torch.isfinite(text_std).all():
            # ğŸ”¥ é™é»˜å¤„ç†ï¼Œé¿å…é‡å¤è­¦å‘Š
            if 'std_finite' not in self.warning_collector: self.warning_collector.append('std_finite')
            return F.normalize(image_features, p=2, dim=1), F.normalize(text_features, p=2, dim=1)
        
        image_normalized = image_centered / image_std
        text_normalized = text_centered / text_std
        
        # 3. ç‰¹å¾ç»´åº¦å¹³è¡¡ (ç¡®ä¿ä¸¤ç§ç‰¹å¾åœ¨ç›¸åŒå°ºåº¦) - æ·»åŠ ç¨³å®šæ€§æ£€æŸ¥
        image_scale = torch.norm(image_normalized, p=2, dim=1, keepdim=True) + 1e-8
        text_scale = torch.norm(text_normalized, p=2, dim=1, keepdim=True) + 1e-8
        
        # æ£€æŸ¥èŒƒæ•°æ˜¯å¦ä¸ºæœ‰é™å€¼
        if not torch.isfinite(image_scale).all() or not torch.isfinite(text_scale).all():
            # ğŸ”¥ é™é»˜å¤„ç†ï¼Œé¿å…é‡å¤è­¦å‘Š
            if 'scale_finite' not in self.warning_collector: self.warning_collector.append('scale_finite')
            return F.normalize(image_features, p=2, dim=1), F.normalize(text_features, p=2, dim=1)
        
        # ä½¿ç”¨å‡ ä½•å¹³å‡ä½œä¸ºç›®æ ‡å°ºåº¦ - æ·»åŠ æ•°å€¼ä¿æŠ¤
        target_scale = torch.sqrt(torch.clamp(image_scale * text_scale, min=1e-8))
        
        # ğŸ”§ æ·»åŠ æœ€ç»ˆæ•°å€¼æ£€æŸ¥
        image_scale_safe = torch.clamp(image_scale, min=1e-8)
        text_scale_safe = torch.clamp(text_scale, min=1e-8)
        target_scale_safe = torch.clamp(target_scale, min=1e-8)
        
        image_balanced = image_normalized * (target_scale_safe / image_scale_safe)
        text_balanced = text_normalized * (target_scale_safe / text_scale_safe)
        
        # ğŸ”¥ æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è¾“å‡ºæ²¡æœ‰NaNæˆ–Infï¼ˆé™é»˜å¤„ç†ï¼‰
        if not torch.isfinite(image_balanced).all() or not torch.isfinite(text_balanced).all():
            if 'balance_finite' not in self.warning_collector: self.warning_collector.append('balance_finite')
            return F.normalize(image_features, p=2, dim=1), F.normalize(text_features, p=2, dim=1)
        
        return image_balanced, text_balanced

    def improved_alignment_loss(self, text_features, image_features, epoch=0, labels=None):
        """ğŸ”¥ æ”¹è¿›çš„æ¸è¿›å¼è·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼ˆä¿®å¤æ•°å€¼ç¨³å®šæ€§ï¼‰"""
        
        # ğŸ¯ æ¸è¿›å¼å‚æ•°è°ƒæ•´ - ä¿®å¤æ¸©åº¦å‚æ•°è¿‡ä½é—®é¢˜
        if epoch < 5:
            margin = 0.8        # æ—©æœŸéå¸¸å®½æ¾
            temperature = 0.3   # ğŸ”¥ æé«˜åˆå§‹æ¸©åº¦ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
            alignment_weight = 0.5  # ğŸ”¥ å¢åŠ æ—©æœŸæƒé‡
        elif epoch < 10:
            margin = 0.5        # é€æ­¥æ”¶ç´§
            temperature = 0.2   # ğŸ”¥ é™ä½æ¸©åº¦
            alignment_weight = 0.7  # ğŸ”¥ å¢åŠ æƒé‡
        elif epoch < 20:
            margin = 0.3        # ä¸­æœŸé€‚ä¸­
            temperature = 0.15  # ğŸ”¥ é™ä½æ¸©åº¦
            alignment_weight = 0.8  # ğŸ”¥ å¢åŠ æƒé‡
        else:
            margin = 0.2        # åæœŸä¸¥æ ¼
            temperature = 0.1   # ğŸ”¥ é™ä½æ¸©åº¦
            alignment_weight = 1.0  # ğŸ”¥ æœ€å¤§æƒé‡
        
        # ğŸ”¥ ç‰¹å¾é¢„å¤„ç†ï¼ˆå·²ä¿®å¤æ•°å€¼ç¨³å®šæ€§ï¼‰
        try:
            image_processed, text_processed = self.preprocess_features_for_alignment(
                image_features, text_features
            )
        except Exception as e:
            if 'preprocess_failed' not in self.warning_collector: self.warning_collector.append('preprocess_failed')
            image_processed = F.normalize(image_features, p=2, dim=1)
            text_processed = F.normalize(text_features, p=2, dim=1)
        
        # ğŸ¯ æ–¹æ¡ˆ1: æ”¹è¿›çš„æ¸©åº¦è°ƒèŠ‚ç›¸ä¼¼åº¦å¯¹é½ - æ·»åŠ æ•°å€¼ä¿æŠ¤
        text_norm = F.normalize(text_processed, p=2, dim=1)
        image_norm = F.normalize(image_processed, p=2, dim=1)
        
        # æ£€æŸ¥æ ‡å‡†åŒ–ç»“æœ
        if not torch.isfinite(text_norm).all() or not torch.isfinite(image_norm).all():
            if 'norm_finite' not in self.warning_collector: self.warning_collector.append('norm_finite')
            return torch.tensor(0.0, device=text_features.device, requires_grad=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ - æ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
        sim_matrix = torch.matmul(text_norm, image_norm.t()) / temperature  # [B, B]
        
        # ğŸ”¥ é™åˆ¶ç›¸ä¼¼åº¦çŸ©é˜µçš„èŒƒå›´ï¼Œé˜²æ­¢expçˆ†ç‚¸
        sim_matrix = torch.clamp(sim_matrix, min=-10.0, max=10.0)
        
        # å¯¹è§’çº¿å…ƒç´ æ˜¯å¯¹åº”æ ·æœ¬çš„ç›¸ä¼¼åº¦
        diag_sim = torch.diag(sim_matrix)  # [B]
        
        # ğŸ”¥ ä¸ºADæ ·æœ¬æ·»åŠ é¢å¤–æƒé‡
        if labels is not None:
            # åˆ›å»ºæ ·æœ¬æƒé‡å‘é‡ (é»˜è®¤æƒé‡ä¸º1.0)
            sample_weights = torch.ones_like(diag_sim)
            
            # ADæ ·æœ¬æƒé‡æå‡åˆ°2.0 (ADæ ‡ç­¾ä¸º1) - ğŸ”¥ æƒé‡ä»5.0é™è‡³2.0ï¼Œé¿å…è¿‡åº¦åç§»
            ad_mask = (labels == 1)
            if ad_mask.sum() > 0:
                sample_weights[ad_mask] = 2.0
                
            # åº”ç”¨æ ·æœ¬æƒé‡åˆ°å¯¹è§’çº¿ç›¸ä¼¼åº¦
            weighted_diag_sim = diag_sim * sample_weights
        else:
            weighted_diag_sim = diag_sim
        
        # ğŸ”¥ InfoNCEé£æ ¼çš„å¯¹é½æŸå¤± (æ›´å¹³æ»‘) - å¢å¼ºæ•°å€¼ç¨³å®šæ€§
        # ä½¿ç”¨æ›´å¤§çš„epsilonå€¼
        eps = 1e-6
        exp_diag = torch.exp(diag_sim)
        exp_all = torch.sum(torch.exp(sim_matrix), dim=1)  # æ¯è¡Œæ±‚å’Œ
        
        # æ£€æŸ¥expå€¼æ˜¯å¦æœ‰é™
        if not torch.isfinite(exp_diag).all() or not torch.isfinite(exp_all).all():
            if 'exp_finite' not in self.warning_collector: self.warning_collector.append('exp_finite')
            # ä½¿ç”¨log-sum-expæŠ€å·§
            log_sum_exp = torch.logsumexp(sim_matrix, dim=1)
            
            # ğŸ”¥ ä½¿ç”¨åŠ æƒå¯¹è§’çº¿ç›¸ä¼¼åº¦
            if labels is not None:
                infonce_alignment_loss = -torch.mean(weighted_diag_sim - log_sum_exp)
            else:
                infonce_alignment_loss = -torch.mean(diag_sim - log_sum_exp)
        else:
            # æ·»åŠ æ›´å¤§çš„æ•°å€¼ç¨³å®šæ€§
            infonce_alignment_loss = -torch.log(exp_diag / (exp_all + eps) + eps).mean()
            
            # ğŸ”¥ å¦‚æœæœ‰æ ‡ç­¾ï¼Œæ·»åŠ é¢å¤–çš„åŠ æƒæŸå¤±
            if labels is not None:
                ad_mask = (labels == 1)
                if ad_mask.sum() > 0:
                    # å•ç‹¬è®¡ç®—ADæ ·æœ¬çš„æŸå¤±å¹¶ç»™äºˆé¢å¤–æƒé‡
                    ad_exp_diag = exp_diag[ad_mask]
                    ad_exp_all = exp_all[ad_mask]
                    ad_loss = -torch.log(ad_exp_diag / (ad_exp_all + eps) + eps).mean()
                    
                    # å°†ADæŸå¤±ä¸æ€»æŸå¤±ç»“åˆ - ğŸ”¥ æƒé‡ä»4.0é™è‡³2.0
                    infonce_alignment_loss = infonce_alignment_loss + 2.0 * ad_loss
        
        # ğŸ¯ æ–¹æ¡ˆ2: æ¸è¿›å¼è·ç¦»æŸå¤± (æ›´å®½æ¾çš„margin)
        feature_distance = F.mse_loss(text_processed, image_processed, reduction='none').mean(dim=1)
        
        # ğŸ”¥ ä¸ºADæ ·æœ¬æ·»åŠ é¢å¤–æƒé‡
        if labels is not None:
            ad_mask = (labels == 1)
            if ad_mask.sum() > 0:
                # å•ç‹¬è®¡ç®—ADæ ·æœ¬çš„è·ç¦»æŸå¤±
                ad_distance = feature_distance[ad_mask]
                non_ad_distance = feature_distance[~ad_mask]
                
                # åˆ†åˆ«åº”ç”¨margin
                ad_loss = F.relu(ad_distance - margin * 0.5).mean()  # ğŸ”¥ ADæ ·æœ¬ä½¿ç”¨æ›´å°çš„margin
                non_ad_loss = F.relu(non_ad_distance - margin).mean()
                
                # ç»„åˆæŸå¤±ï¼ŒADæŸå¤±æƒé‡æ›´é«˜ - ğŸ”¥ æƒé‡ä»4.0é™è‡³2.0
                distance_loss = non_ad_loss + 2.0 * ad_loss
            else:
                distance_loss = F.relu(feature_distance - margin).mean()
        else:
            distance_loss = F.relu(feature_distance - margin).mean()
        
        # ğŸ”¥ æ–¹æ¡ˆ3: ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤± (ç›´æ¥ä¼˜åŒ–) - æ·»åŠ æ•°å€¼ä¿æŠ¤
        cosine_sim = F.cosine_similarity(text_norm, image_norm, dim=1)
        # ç¡®ä¿ä½™å¼¦ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´å†…
        cosine_sim = torch.clamp(cosine_sim, min=-1.0, max=1.0)
        
        # ğŸ”¥ ä¸ºADæ ·æœ¬æ·»åŠ é¢å¤–æƒé‡
        if labels is not None:
            ad_mask = (labels == 1)
            if ad_mask.sum() > 0:
                # å•ç‹¬è®¡ç®—ADæ ·æœ¬çš„ä½™å¼¦æŸå¤±
                ad_cosine = cosine_sim[ad_mask]
                non_ad_cosine = cosine_sim[~ad_mask]
                
                # åˆ†åˆ«è®¡ç®—æŸå¤±
                ad_loss = (1.0 - ad_cosine).mean()
                non_ad_loss = (1.0 - non_ad_cosine).mean()
                
                # ç»„åˆæŸå¤±ï¼ŒADæŸå¤±æƒé‡æ›´é«˜ - ğŸ”¥ æƒé‡ä»4.0é™è‡³2.0
                cosine_loss = non_ad_loss + 2.0 * ad_loss
            else:
                cosine_loss = (1.0 - cosine_sim).mean()
        else:
            cosine_loss = (1.0 - cosine_sim).mean()
        
        # ğŸ”§ æ£€æŸ¥æ‰€æœ‰æŸå¤±æ˜¯å¦ä¸ºæœ‰é™å€¼
        losses_to_check = [infonce_alignment_loss, distance_loss, cosine_loss]
        for i, loss_val in enumerate(losses_to_check):
            if not torch.isfinite(loss_val):
                # ğŸ”¥ é™é»˜å¤„ç†éæœ‰é™å€¼æŸå¤±
                losses_to_check[i] = torch.tensor(0.0, device=text_features.device, requires_grad=True)
        
        infonce_alignment_loss, distance_loss, cosine_loss = losses_to_check
        
        # ğŸ¯ åŠ¨æ€æƒé‡ç»„åˆ (æ¸è¿›å¼ç­–ç•¥)
        if epoch < 5:
            # æ—©æœŸ: ä»¥è·ç¦»æŸå¤±ä¸ºä¸»ï¼Œå»ºç«‹åŸºæœ¬å¯¹åº”å…³ç³»
            total_loss = 0.3 * infonce_alignment_loss + 0.5 * distance_loss + 0.2 * cosine_loss
        elif epoch < 15:
            # ä¸­æœŸ: å¹³è¡¡ä¸‰ç§æŸå¤±
            total_loss = 0.4 * infonce_alignment_loss + 0.4 * distance_loss + 0.2 * cosine_loss
        else:
            # åæœŸ: ä»¥InfoNCEå¯¹é½ä¸ºä¸»ï¼Œç²¾ç»†è°ƒæ•´
            total_loss = 0.6 * infonce_alignment_loss + 0.3 * distance_loss + 0.1 * cosine_loss
        
        # ğŸ”¥ åº”ç”¨æ¸è¿›å¼æƒé‡ - æœ€ç»ˆæ•°å€¼æ£€æŸ¥ï¼ˆé™é»˜å¤„ç†ï¼‰
        final_loss = total_loss * alignment_weight
        
        if not torch.isfinite(final_loss):
            # ğŸ”¥ é™é»˜è¿”å›é›¶æŸå¤±ï¼Œé¿å…é‡å¤è­¦å‘Š
            return torch.tensor(0.0, device=text_features.device, requires_grad=True)
        
        return final_loss
    
    def forward(self, images, texts, labels=None, mode='both', epoch=0, inference_mode=False):
        """
        å‰å‘ä¼ æ’­ - å›¾åƒä¸»å¯¼åˆ†ç±»ï¼Œæ–‡æœ¬è¾…åŠ©å¯¹é½
        
        Args:
            images: [B, 3, 113, 137, 113] å›¾åƒæ•°æ®
            texts: List[str] æ–‡æœ¬æ•°æ®åˆ—è¡¨
            labels: [B] çœŸå®æ ‡ç­¾ (æ¨ç†æ—¶ä¸ºNone)
            mode: 'classification', 'losses', 'both'
            epoch: å½“å‰è®­ç»ƒè½®æ•°
            inference_mode: bool æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼ (æµ‹è¯•æ—¶ä¸ºTrue)
        
        Returns:
            dict: åŒ…å«ä¸åŒè¾“å‡ºçš„å­—å…¸
        """
        # ğŸ”§ ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        images = images.to(self.device)
        if labels is not None:
            labels = labels.to(self.device)
        
        # ğŸ¯ æ¨ç†æ¨¡å¼æ£€æŸ¥
        if inference_mode:
            # æ¨ç†æ¨¡å¼ï¼šä¸ä½¿ç”¨æ ‡ç­¾ï¼Œåªè¿›è¡Œåˆ†ç±»é¢„æµ‹
            assert labels is None or not self.training, "æ¨ç†æ¨¡å¼ä¸‹ä¸åº”è¯¥ä½¿ç”¨æ ‡ç­¾è¿›è¡Œè®­ç»ƒ"
            mode = 'classification'  # å¼ºåˆ¶è®¾ç½®ä¸ºåˆ†ç±»æ¨¡å¼
        
        # 1. å›¾åƒç¼–ç 
        image_features = self.image_encoder(images)  # [B, 512]
        
        # 2. å¢å¼ºå¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç 
        if inference_mode:
            # ğŸ”¥ æ¨ç†æ¨¡å¼ï¼šä¸ä¼ å…¥æ ‡ç­¾ï¼Œä½¿ç”¨é»˜è®¤æƒé‡
            text_results = self.text_encoder(texts, labels=None)
        else:
            # ğŸ”¥ è®­ç»ƒ/éªŒè¯æ¨¡å¼ï¼šä¼ å…¥æ ‡ç­¾è¿›è¡Œç‰¹å¾è§£è€¦
            text_results = self.text_encoder(texts, labels)
        
        text_features = text_results['text_features']  # [B, 512]
        text_weights = text_results['text_weights']    # [B, 1]
        
        results = {
            'image_features': image_features,
            'text_features': text_features,
            'text_weights': text_weights
        }
        
        # 3. è·¨æ¨¡æ€å¯¹é½ï¼ˆç”¨äºè®¡ç®—å¯¹é½æŸå¤±ï¼‰
        # å³ä½¿æœ€ç»ˆåˆ†ç±»åªç”¨å›¾åƒï¼Œå¯¹é½ä¹Ÿæ˜¯å¿…è¦çš„ï¼Œä»¥ä½¿å›¾åƒç‰¹å¾å­¦ä¹ åˆ°æ–‡æœ¬ä¿¡æ¯
        aligned_text_raw = self.cross_modal_aligner(text_features)
        # ğŸ”¥ æ·»åŠ æ®‹å·®è¿æ¥ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
        aligned_text_features = aligned_text_raw + text_features  # æ®‹å·®è¿æ¥
        
        if not inference_mode and labels is not None:
            # ğŸ”¥ å¯¹é½æŸå¤±: ä½¿ç”¨åŸå§‹å›¾åƒç‰¹å¾å’Œå¯¹é½åçš„æ–‡æœ¬ç‰¹å¾
            # ç¡®ä¿ä¼ å…¥configä»¥è·å–ad_weight
            current_config = {
                'ad_weight': self.adaptive_weights.get_weights()[0],
                'alignment_weight': self.adaptive_weights.get_weights()[1],
                'contrastive_weight': self.adaptive_weights.get_weights()[2],
                'reconstruction_weight': self.adaptive_weights.get_weights()[3],
                'orthogonality_weight': self.adaptive_weights.get_weights()[4],
                'diagnostic_weight': self.adaptive_weights.get_weights()[5],
                'dominance_weight': self.adaptive_weights.get_weights()[6]
                # åˆ é™¤text_suppression_weight
            }
            alignment_loss = self.improved_alignment_loss(aligned_text_features, image_features.detach(), epoch, labels)
            results['alignment_loss'] = alignment_loss
            
            # ğŸ”¥ å¯¹æ¯”å­¦ä¹ æŸå¤±: ä½¿ç”¨åŸå§‹å›¾åƒç‰¹å¾å’ŒåŸå§‹æ–‡æœ¬ç‰¹å¾
            contrastive_loss = self.contrastive_sampler.compute_contrastive_loss(
                image_features, text_features, labels
            )
            results['contrastive_loss'] = contrastive_loss
        
        # 4. åˆ†ç±» (å›¾åƒä¸»å¯¼)
        # ç›´æ¥ä½¿ç”¨å›¾åƒç‰¹å¾è¿›è¡Œåˆ†ç±»
        logits = self.image_classifier(image_features)
        results['logits'] = logits
        
        # 5. è®¡ç®—æ€»æŸå¤± (åªåœ¨è®­ç»ƒ/éªŒè¯æ¨¡å¼ä¸”æœ‰æ ‡ç­¾æ—¶)
        if mode in ['losses', 'both'] and labels is not None and not inference_mode:
            classification_loss = F.cross_entropy(results['logits'], labels)
            results['classification_loss'] = classification_loss
            
            # æ–‡æœ¬ç¼–ç å™¨ç›¸å…³çš„æŸå¤± (æ¥è‡ªtext_results)
            if 'diagnostic_loss' in text_results:
                results['reconstruction_loss'] = text_results.get('reconstruction_loss', torch.tensor(0.0, device=self.device))
                results['orthogonality_loss'] = text_results.get('orthogonality_loss', torch.tensor(0.0, device=self.device))
                results['diagnostic_loss'] = text_results.get('diagnostic_loss', torch.tensor(0.0, device=self.device))
            
            # åªä¿ç•™dominance_lossï¼Œç¡®ä¿å›¾åƒç‰¹å¾ä¸»å¯¼
            if 'non_diagnostic_features' in text_results:
                image_norm = torch.norm(image_features, p=2, dim=1).mean()
                non_diag_text_norm = torch.norm(text_results['non_diagnostic_features'], p=2, dim=1).mean()
                results['dominance_loss'] = F.relu(non_diag_text_norm - image_norm * 0.2) # æ–‡æœ¬ç‰¹å¾èŒƒæ•°ä¸åº”è¿œè¶…å›¾åƒ
                
                # åˆ é™¤text_suppression_lossçš„è®¡ç®—
            
            losses_dict = {
                'classification': classification_loss,
                'alignment': results.get('alignment_loss', torch.tensor(0.0, device=self.device)),
                'contrastive': results.get('contrastive_loss', torch.tensor(0.0, device=self.device)),
                'reconstruction': results.get('reconstruction_loss', torch.tensor(0.0, device=self.device)),
                'orthogonality': results.get('orthogonality_loss', torch.tensor(0.0, device=self.device)),
                'diagnostic': results.get('diagnostic_loss', torch.tensor(0.0, device=self.device)),
                'dominance': results.get('dominance_loss', torch.tensor(0.0, device=self.device))
                # åˆ é™¤text_suppression
            }
            
            total_loss, weights_dict = self.adaptive_weights(losses_dict, epoch)
            results['total_loss'] = total_loss
            results['adaptive_weights'] = weights_dict
        
        return results


class AdversarialMultiModalDataset(Dataset):
    """å¯¹æŠ—æ€§å¤šæ¨¡æ€æ•°æ®é›†"""
    
    def __init__(self, images, texts, labels):
        """
        Args:
            images: numpy array [N, 3, 113, 137, 113] å›¾åƒæ•°æ®
            texts: List[str] æ–‡æœ¬æ•°æ®åˆ—è¡¨
            labels: numpy array [N] æ ‡ç­¾æ•°æ®
        """
        self.images = torch.FloatTensor(images)
        self.texts = texts  # ä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
        self.labels = torch.LongTensor(labels)
        
        assert len(self.images) == len(self.texts) == len(self.labels)
        print(f"ğŸ“Š å¯¹æŠ—æ€§æ•°æ®é›†åˆ›å»º: {len(self.labels)} æ ·æœ¬")
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return {
            'image': self.images[idx],
            'text': self.texts[idx],  # è¿”å›åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
            'label': self.labels[idx],
            'index': idx
        }


class AdversarialContrastiveTrainer:
    """å¯¹æŠ—æ€§å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, model, device, config):
        self.model = model.to(device)
        self.device = device
        self.config = config
        
        # å·®å¼‚åŒ–å­¦ä¹ ç‡ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config['num_epochs']
        )
        
        print(f"ğŸ¯ å¯¹æŠ—æ€§è®­ç»ƒå™¨é…ç½®:")
        print(f"   å­¦ä¹ ç‡: {config['learning_rate']}")
        print(f"   æƒé‡è¡°å‡: {config['weight_decay']}")
        print(f"   æ¢¯åº¦è£å‰ª: {config['gradient_clip']}")
    
    def _create_optimizer(self):
        """åˆ›å»ºå·®å¼‚åŒ–å­¦ä¹ ç‡ä¼˜åŒ–å™¨"""
        param_groups = [
            # å›¾åƒç¼–ç å™¨æŠ•å½±å±‚ - ä¸­ç­‰å­¦ä¹ ç‡
            {
                'params': self.model.image_encoder.projection.parameters(),
                'lr': self.config['learning_rate'] * 1.5,
                'name': 'image_projection'
            },
            # å¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç å™¨ - é«˜å­¦ä¹ ç‡
            {
                'params': self.model.text_encoder.adversarial_projection.parameters(),
                'lr': self.config['learning_rate'] * 2,
                'name': 'text_adversarial_projection'
            },
            # ç‰¹å¾è§£è€¦æ¨¡å— - é«˜å­¦ä¹ ç‡
            {
                'params': self.model.text_encoder.feature_disentangler.parameters(),
                'lr': self.config['learning_rate'] * 2,
                'name': 'feature_disentangler'
            },
            # BERTå‚æ•° - ä½å­¦ä¹ ç‡
            {
                'params': self.model.text_encoder.bert_model.parameters(),
                'lr': self.config['learning_rate'] * 0.1,
                'name': 'bert_backbone'
            },
            # è·¨æ¨¡æ€å¯¹é½å™¨ - æ›´å°çš„å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒ
            {
                'params': self.model.cross_modal_aligner.parameters(),
                'lr': self.config['learning_rate'] * 0.3,  # ğŸ”¥ ä»1.0é™åˆ°0.3ï¼Œæ›´ç¨³å®šçš„å¯¹é½å­¦ä¹ 
                'name': 'cross_modal_aligner'
            },
            # å›¾åƒåˆ†ç±»å™¨ - æ ‡å‡†å­¦ä¹ ç‡
            {
                'params': self.model.image_classifier.parameters(),
                'lr': self.config['learning_rate'],
                'name': 'image_classifier'
            }
        ]
        
        return torch.optim.AdamW(param_groups, weight_decay=self.config['weight_decay'])
    
    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - å¢å¼ºå¯¹æŠ—æ€§ç‰ˆæœ¬ï¼ˆä¼˜åŒ–è¿›åº¦æ¡æ ¼å¼ï¼‰"""
        self.model.train()
        
        # ğŸ”¥ åœ¨epochå¼€å§‹æ—¶æ¸…ç©ºè­¦å‘Š
        self.model.warning_collector.clear()

        total_loss = 0.0
        total_classification_loss = 0.0
        total_alignment_loss = 0.0
        total_contrastive_loss = 0.0
        correct = 0
        total = 0
        
        # ğŸ”¥ ä¼˜åŒ–è¿›åº¦æ¡æ˜¾ç¤º - ä½¿ç”¨æ ‡å‡†æ ¼å¼
        desc = f'Epoch {epoch+1:02d} [Train]'
        pbar = tqdm(dataloader, desc=desc, leave=True, ncols=120, 
                   bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {postfix}]')
        
        for batch_idx, batch in enumerate(pbar):
            try:
                images = batch['image'].to(self.device)
                texts = batch['text']  # æ–‡æœ¬åˆ—è¡¨
                labels = batch['label'].to(self.device)
                
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(images, texts, labels=labels, mode='both', epoch=epoch)
                
                # æå–æŸå¤±
                total_batch_loss = outputs['total_loss']
                classification_loss = outputs['classification_loss']
                alignment_loss = outputs['alignment_loss']
                contrastive_loss = outputs.get('contrastive_loss', torch.tensor(0.0))
                
                # åå‘ä¼ æ’­
                total_batch_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config.get('gradient_clip', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['gradient_clip']
                    )
                
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                total_classification_loss += classification_loss.item()
                total_alignment_loss += alignment_loss.item()
                total_contrastive_loss += contrastive_loss.item()
                
                # é¢„æµ‹ç»Ÿè®¡
                logits = outputs['logits']
                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # ğŸ”¥ å®æ—¶æ›´æ–°è¿›åº¦æ¡
                current_acc = 100. * correct / total if total > 0 else 0
                
                # ğŸ”¥ æ ‡å‡†è¿›åº¦æ¡æ ¼å¼ï¼šLoss=X.XXXX, Acc=XX.X%
                pbar.set_postfix_str(f'Loss={total_batch_loss.item():.4f}, Acc={current_acc:.1f}%')
                
                # å­¦ä¹ ç‡è°ƒæ•´
                if hasattr(self, 'scheduler') and self.scheduler is not None:
                    self.scheduler.step()
                    
            except Exception as e:
                print(f"\nâš ï¸  è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
        
        # ğŸ”¥ åœ¨epochç»“æŸåæ‰“å°æ”¶é›†åˆ°çš„è­¦å‘Š
        if self.model.warning_collector:
            tqdm.write(f"\nEpoch {epoch+1} Warnings: {list(set(self.model.warning_collector))}")
            self.model.warning_collector.clear() # æ¸…ç©ºè­¦å‘Š

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(dataloader)
        avg_classification_loss = total_classification_loss / len(dataloader)
        avg_alignment_loss = total_alignment_loss / len(dataloader)
        avg_contrastive_loss = total_contrastive_loss / len(dataloader)
        accuracy = 100. * correct / total
        current_lr = self.optimizer.param_groups[0]['lr']
        
        return {
            'total_loss': avg_loss,
            'classification_loss': avg_classification_loss,
            'alignment_loss': avg_alignment_loss,
            'contrastive_loss': avg_contrastive_loss,
            'accuracy': accuracy / 100.0,
            'learning_rate': current_lr
        }
    
    def evaluate(self, dataloader, inference_mode=False):
        """
        è¯„ä¼°æ¨¡å‹ - å¢å¼ºå¯¹æŠ—æ€§ç‰ˆæœ¬ï¼ˆä¼˜åŒ–è¿›åº¦æ¡æ ¼å¼ï¼‰
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            inference_mode: bool æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼
                - False: éªŒè¯æ¨¡å¼ï¼Œæœ‰æ ‡ç­¾ï¼Œè®¡ç®—æŸå¤±
                - True: æµ‹è¯•æ¨¡å¼ï¼Œå¯èƒ½æ— æ ‡ç­¾ï¼Œåªé¢„æµ‹
        """
        self.model.eval()
        
        total_loss = 0.0
        total_classification_loss = 0.0
        total_alignment_loss = 0.0
        total_contrastive_loss = 0.0
        correct = 0
        total = 0
        all_predictions = []
        all_labels = []
        all_text_weights = []  # ğŸ”¥ æ–°å¢ï¼šæ”¶é›†æ–‡æœ¬æƒé‡
        
        # ğŸ”¥ é‡ç½®å¹¶æ¸…ç©ºè­¦å‘Šæ”¶é›†å™¨
        self.model.warning_collector.clear()
        
        # ğŸ”¥ è¿›åº¦æ¡ä¼˜åŒ–ï¼šæ ‡å‡†æ ¼å¼
        desc = 'ğŸ”® Inference' if inference_mode else '[Val]'
        
        with torch.no_grad():
            # ğŸ”¥ è®¾ç½®é™é»˜æ¨¡å¼ï¼Œé¿å…é‡å¤è­¦å‘Š
            if hasattr(self.model, '_std_warning_shown'):
                self.model._std_warning_shown = True
                
            pbar = tqdm(dataloader, desc=desc, leave=True, ncols=120,
                       bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {postfix}]')
            for batch_idx, batch in enumerate(pbar):
                try:
                    images = batch['image'].to(self.device)
                    texts = batch['text']  # æ–‡æœ¬åˆ—è¡¨
                    labels = batch.get('label', None)  # æ¨ç†æ¨¡å¼å¯èƒ½æ²¡æœ‰æ ‡ç­¾
                    
                    if labels is not None:
                        labels = labels.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = self.model(images, texts, labels=labels, mode='both', inference_mode=inference_mode)
                    
                    # è·å–é¢„æµ‹ç»“æœ
                    logits = outputs['logits']
                    _, predicted = torch.max(logits.data, 1)
                    
                    # ğŸ”¥ åªåœ¨éªŒè¯æ¨¡å¼è®¡ç®—æŸå¤±
                    if not inference_mode and labels is not None:
                        total_batch_loss = outputs.get('total_loss', torch.tensor(0.0))
                        classification_loss = outputs.get('classification_loss', torch.tensor(0.0))
                        alignment_loss = outputs.get('alignment_loss', torch.tensor(0.0))
                        contrastive_loss = outputs.get('contrastive_loss', torch.tensor(0.0))
                        
                        total_loss += total_batch_loss.item()
                        total_classification_loss += classification_loss.item()
                        total_alignment_loss += alignment_loss.item()
                        total_contrastive_loss += contrastive_loss.item()
                    
                    # ğŸ”¥ åªåœ¨æœ‰çœŸå®æ ‡ç­¾æ—¶è®¡ç®—å‡†ç¡®ç‡
                    if labels is not None:
                        total += labels.size(0)
                        correct += (predicted == labels).sum().item()
                        all_labels.extend(labels.cpu().numpy())
                    else:
                        # æ¨ç†æ¨¡å¼æ— æ ‡ç­¾æ—¶ï¼Œè®°å½•ä¸º-1
                        total += len(predicted)
                        all_labels.extend([-1] * len(predicted))
                    
                    all_predictions.extend(predicted.cpu().numpy())
                    
                    # ğŸ”¥ æ”¶é›†æ–‡æœ¬æƒé‡ä¿¡æ¯
                    text_weights = outputs.get('text_weights', torch.tensor([[0.5]] * len(predicted)))
                    all_text_weights.extend(text_weights.cpu().numpy().flatten())
                    
                    # ğŸ”¥ æ¯5ä¸ªæ‰¹æ¬¡æ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡
                    if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(pbar):
                        if labels is not None:
                            current_acc = 100. * correct / total if total > 0 else 0
                            if not inference_mode:
                                # éªŒè¯æ¨¡å¼ï¼šæ˜¾ç¤ºæŸå¤±å’Œå‡†ç¡®ç‡
                                pbar.set_postfix_str(f'Loss={total_batch_loss.item():.4f}, Acc={current_acc:.1f}%')
                            else:
                                # æ¨ç†æ¨¡å¼ï¼šåªæ˜¾ç¤ºå‡†ç¡®ç‡
                                pbar.set_postfix_str(f'Acc={current_acc:.1f}%')
                        else:
                            # æ— æ ‡ç­¾æ¨ç†æ¨¡å¼
                            pbar.set_postfix_str(f'Batch={batch_idx+1}/{len(dataloader)}')
                            
                except Exception as e:
                    print(f"\nâš ï¸  æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        # ğŸ”¥ åœ¨è¯„ä¼°ç»“æŸåæ‰“å°æ”¶é›†åˆ°çš„è­¦å‘Š
        if self.model.warning_collector:
            tqdm.write(f"\nEvaluation Warnings: {list(set(self.model.warning_collector))}")
            self.model.warning_collector.clear()

        # è®¡ç®—æŒ‡æ ‡
        if inference_mode:
            # æ¨ç†æ¨¡å¼ï¼šå¯èƒ½æ²¡æœ‰çœŸå®æ ‡ç­¾
            if all(label != -1 for label in all_labels):
                # æœ‰æ ‡ç­¾ï¼šè®¡ç®—å‡†ç¡®ç‡
                accuracy = 100. * correct / total
                report = classification_report(all_labels, all_predictions, target_names=['CN', 'AD'], output_dict=True)
                conf_matrix = confusion_matrix(all_labels, all_predictions)
            else:
                # æ— æ ‡ç­¾ï¼šæ— æ³•è®¡ç®—å‡†ç¡®ç‡
                accuracy = None
                report = None
                conf_matrix = None
        else:
            # éªŒè¯æ¨¡å¼ï¼šå¿…é¡»æœ‰æ ‡ç­¾
            accuracy = 100. * correct / total
            report = classification_report(all_labels, all_predictions, target_names=['CN', 'AD'], output_dict=True)
            conf_matrix = confusion_matrix(all_labels, all_predictions)
        
        # ğŸ”¥ æ–‡æœ¬æƒé‡ç»Ÿè®¡
        avg_text_weight = np.mean(all_text_weights)
        text_weight_std = np.std(all_text_weights)
        
        result = {
            'predictions': all_predictions,
            'labels': all_labels,
            'avg_text_weight': avg_text_weight,  # ğŸ”¥ æ–°å¢
            'text_weight_std': text_weight_std,   # ğŸ”¥ æ–°å¢
            'inference_mode': inference_mode
        }
        
        # ğŸ”¥ åªåœ¨éªŒè¯æ¨¡å¼è¿”å›æŸå¤±
        if not inference_mode:
            result.update({
                'loss': total_loss / len(dataloader),
                'classification_loss': total_classification_loss / len(dataloader),
                'alignment_loss': total_alignment_loss / len(dataloader),
                'contrastive_loss': total_contrastive_loss / len(dataloader),
            })
        
        # ğŸ”¥ åªåœ¨æœ‰æ ‡ç­¾æ—¶è¿”å›å‡†ç¡®ç‡æŒ‡æ ‡
        if accuracy is not None:
            result.update({
                'accuracy': accuracy / 100.0,
                'classification_report': report,
                'confusion_matrix': conf_matrix,
            })
        
        return result
    
    def inference(self, dataloader):
        """
        ğŸ”¥ ä¸“é—¨çš„æ¨ç†å‡½æ•° - ç”¨äºç‹¬ç«‹æµ‹è¯•é›†
        
        Args:
            dataloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯èƒ½æ²¡æœ‰æ ‡ç­¾ï¼‰
            
        Returns:
            dict: æ¨ç†ç»“æœ
        """
        print("ğŸ”® å¼€å§‹æ¨ç†æ¨¡å¼ - ç‹¬ç«‹æµ‹è¯•é›†é¢„æµ‹")
        return self.evaluate(dataloader, inference_mode=True)


def load_text_data_with_cognitive_scores(text_data_dir):
    """
    åŠ è½½åŒ…å«è®¤çŸ¥è¯„ä¼°åˆ†æ•°çš„æ–‡æœ¬æ•°æ® - ä¿ç•™MMSE/CDR-SBç­‰æœ‰ä»·å€¼ä¿¡æ¯
    
    Args:
        text_data_dir: æ–‡æœ¬æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        all_texts: List[str] åŒ…å«è®¤çŸ¥åˆ†æ•°çš„æ–‡æœ¬æ•°æ®
        all_labels: numpy array æ ‡ç­¾
        patient_ids: List[str] æ‚£è€…IDåˆ—è¡¨
    """
    print(f"ğŸ“ åŠ è½½åŒ…å«è®¤çŸ¥è¯„ä¼°åˆ†æ•°çš„æ–‡æœ¬æ•°æ®...")
    
    # æ–‡ä»¶è·¯å¾„ - V3.5 æ›´æ–°
    ad_file = os.path.join(text_data_dir, 'ad_metadata.xlsx')
    cn_file = os.path.join(text_data_dir, 'cn_metadata.xlsx')
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(ad_file):
        raise FileNotFoundError(f"ADæ–‡ä»¶ä¸å­˜åœ¨: {ad_file}")
    if not os.path.exists(cn_file):
        raise FileNotFoundError(f"CNæ–‡ä»¶ä¸å­˜åœ¨: {cn_file}")
    
    # åŠ è½½æ•°æ®
    ad_df = pd.read_excel(ad_file)
    cn_df = pd.read_excel(cn_file)
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   ADæ ·æœ¬: {len(ad_df)} è¡Œ")
    print(f"   CNæ ·æœ¬: {len(cn_df)} è¡Œ")
    
    def create_rich_clinical_text(row):
        """
        åˆ›å»ºä¸°å¯Œçš„ä¸´åºŠæ–‡æœ¬æè¿° - ä¿ç•™è®¤çŸ¥åˆ†æ•°
        
        ğŸ¯ ç­–ç•¥ï¼šä¿ç•™æœ‰ä»·å€¼çš„åŒ»å­¦ä¿¡æ¯ï¼Œä½†é€šè¿‡å¯¹æŠ—è®­ç»ƒå‡è½»æ³„éœ²å½±å“
        """
        text_parts = []
        
        # åŸºæœ¬äººå£ç»Ÿè®¡å­¦ä¿¡æ¯
        if 'Age' in row and pd.notna(row['Age']):
            text_parts.append(f"Age: {row['Age']} years")
        
        if 'Gender' in row and pd.notna(row['Gender']):
            gender = "male" if row['Gender'] == 1 else "female"
            text_parts.append(f"Gender: {gender}")
        
        if 'Edu' in row and pd.notna(row['Edu']):
            text_parts.append(f"Education: {row['Edu']} years")
        
        # ğŸ¯ ä¿ç•™è®¤çŸ¥è¯„ä¼°åˆ†æ•°ï¼ˆæœ‰ä»·å€¼çš„åŒ»å­¦ç‰¹å¾ï¼Œä½¿ç”¨å®Œæ•´åŒ»å­¦æœ¯è¯­ï¼‰
        cognitive_scores = []
        
        if 'MMSE' in row and pd.notna(row['MMSE']):
            mmse = row['MMSE']
            # ğŸ”¥ ä½¿ç”¨å®Œæ•´åŒ»å­¦æœ¯è¯­å…¨ç§°
            cognitive_scores.append(f"Mini-Mental State Examination (MMSE): [{int(mmse)}/30]")
        
        if 'CDRSB' in row and pd.notna(row['CDRSB']):
            cdrsb = row['CDRSB']
            # ğŸ”¥ ä½¿ç”¨å®Œæ•´åŒ»å­¦æœ¯è¯­å…¨ç§°
            cognitive_scores.append(f"Clinical Dementia Rating - Sum of Boxes (CDR-SB): [{cdrsb}]")
        
        # æ·»åŠ å…¶ä»–è®¤çŸ¥æµ‹è¯•åˆ†æ•°
        additional_scores = []
        for col in row.index:
            if col in ['ADAS11', 'ADAS13', 'RAVLT_immediate', 'RAVLT_learning', 'RAVLT_forgetting']:
                if pd.notna(row[col]):
                    additional_scores.append(f"{col}: {row[col]}")
        
        # ç»„åˆæ‰€æœ‰ä¿¡æ¯
        if cognitive_scores:
            text_parts.append("Cognitive assessments: " + ", ".join(cognitive_scores))
        
        if additional_scores:
            text_parts.append("Additional tests: " + ", ".join(additional_scores))
        
        # ğŸš¨ å…³é”®ï¼šä¸åŒ…å«æ˜ç¡®çš„è¯Šæ–­ä¿¡æ¯
        final_text = "Clinical profile: " + " | ".join(text_parts)
        
        return final_text
    
    # å¤„ç†ADæ•°æ®
    ad_texts = []
    ad_patient_ids = []
    for idx, row in ad_df.iterrows():
        text = create_rich_clinical_text(row)
        ad_texts.append(text)
        
        # æå–æ‚£è€…ID
        if 'NAME' in row and pd.notna(row['NAME']):
            ad_patient_ids.append(str(row['NAME']))
        else:
            ad_patient_ids.append(f"AD_{idx}")
    
    # å¤„ç†CNæ•°æ®
    cn_texts = []
    cn_patient_ids = []
    for idx, row in cn_df.iterrows():
        text = create_rich_clinical_text(row)
        cn_texts.append(text)
        
        # æå–æ‚£è€…ID
        if 'NAME' in row and pd.notna(row['NAME']):
            cn_patient_ids.append(str(row['NAME']))
        else:
            cn_patient_ids.append(f"CN_{idx}")
    
    # åˆå¹¶æ•°æ®
    all_texts = ad_texts + cn_texts
    all_labels = np.array([1] * len(ad_texts) + [0] * len(cn_texts))  # AD=1, CN=0
    patient_ids = ad_patient_ids + cn_patient_ids
    
    print(f"âœ… ä¸°å¯Œæ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_texts)}")
    print(f"   ADæ ·æœ¬: {len(ad_texts)}, CNæ ·æœ¬: {len(cn_texts)}")
    print(f"   ç¤ºä¾‹ADæ–‡æœ¬: {all_texts[0][:200]}...")
    print(f"   ç¤ºä¾‹CNæ–‡æœ¬: {all_texts[len(ad_texts)][:200]}...")
    
    return all_texts, all_labels, patient_ids


def load_multiple_mcic_files(data_dir):
    """
    ğŸ”„ åŠ è½½å¤šä¸ªMCIcæ ¼å¼æ–‡ä»¶å¹¶åˆå¹¶
    
    é€‚ç”¨äºæ‚¨æœ‰å¤šä¸ªç»„åˆ«æ–‡ä»¶çš„æƒ…å†µ:
    - mcic_metadata.xlsx
    - mcinc_metadata.xlsx
    
    Args:
        data_dir: åŒ…å«æ•°æ®æ–‡ä»¶çš„ç›®å½•
    
    Returns:
        all_texts: åˆå¹¶çš„æ–‡æœ¬åˆ—è¡¨
        all_labels: åˆå¹¶çš„æ ‡ç­¾æ•°ç»„
        patient_ids: åˆå¹¶çš„æ‚£è€…IDåˆ—è¡¨
    """
    print(f"ğŸ”„ æ‰«æç›®å½•ä¸­çš„MCIc/MCIncå…ƒæ•°æ®æ–‡ä»¶: {data_dir}")
    
    # ğŸ¯ V3.5 æ›´æ–°ï¼šç›´æ¥æŸ¥æ‰¾ç‰¹å®šçš„å…ƒæ•°æ®æ–‡ä»¶
    mcic_file = os.path.join(data_dir, 'mcic_metadata.xlsx')
    mcinc_file = os.path.join(data_dir, 'mcinc_metadata.xlsx')
    
    all_files = []
    if os.path.exists(mcic_file):
        all_files.append(mcic_file)
    if os.path.exists(mcinc_file):
        all_files.append(mcinc_file)
    
    if not all_files:
        raise FileNotFoundError(f"âŒ åœ¨ç›®å½• {data_dir} ä¸­æœªæ‰¾åˆ° mcic_metadata.xlsx æˆ– mcinc_metadata.xlsx")
    
    print(f"ğŸ“ æ‰¾åˆ° {len(all_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶:")
    for file in all_files:
        print(f"   - {os.path.basename(file)}")
    
    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®
    combined_texts = []
    combined_labels = []
    combined_patient_ids = []
    
    for file_path in all_files:
        try:
            texts, labels, ids = load_mcic_format_data(file_path)
            combined_texts.extend(texts)
            combined_labels.extend(labels)
            combined_patient_ids.extend(ids)
            print(f"âœ… æˆåŠŸåŠ è½½: {os.path.basename(file_path)} ({len(texts)} æ ·æœ¬)")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {os.path.basename(file_path)} - {e}")
            continue
    
    # è½¬æ¢æ ‡ç­¾ä¸ºnumpyæ•°ç»„
    combined_labels = np.array(combined_labels)
    
    # æœ€ç»ˆç»Ÿè®¡
    label_counts = np.bincount(combined_labels)
    print(f"\nğŸ¯ åˆå¹¶æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(combined_texts)}")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: é˜´æ€§={label_counts[0]}, é˜³æ€§={label_counts[1] if len(label_counts) > 1 else 0}")
    
    return combined_texts, combined_labels, combined_patient_ids


# ç¤ºä¾‹ä½¿ç”¨å‡½æ•°
def example_usage_mcic_data():
    """
    ğŸ¯ MCIcæ•°æ®æ ¼å¼ä½¿ç”¨ç¤ºä¾‹
    """
    # æ–¹æ³•1: åŠ è½½å•ä¸ªæ–‡ä»¶
    # texts, labels, ids = load_mcic_format_data("./data/mcic_clinical_data.xlsx")
    
    # æ–¹æ³•2: åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    # texts, labels, ids = load_multiple_mcic_files("./æ–‡æœ¬ç¼–ç å™¨/")
    
    # æ–¹æ³•3: åœ¨ä¸»è®­ç»ƒå‡½æ•°ä¸­ä½¿ç”¨
    """
    # åœ¨main()å‡½æ•°ä¸­æ›¿æ¢åŸæœ‰çš„æ•°æ®åŠ è½½:
    
    print("ğŸ”„ åŠ è½½MCIcæ ¼å¼ä¸´åºŠæ•°æ®...")
    texts, labels, patient_ids = load_multiple_mcic_files(config['text_data_dir'])
    
    # ç„¶åç»§ç»­ä½¿ç”¨ç°æœ‰çš„æ•°æ®å¤„ç†æµç¨‹...
    """
    pass


def get_best_image_model_path():
    """æ™ºèƒ½è·å–æœ€ä½³å›¾åƒæ¨¡å‹è·¯å¾„"""
    cv_models = [
        ('./models/contrastive/fold_0_best_model.pth', 'ç¬¬0æŠ˜æœ€ä½³æ¨¡å‹ (94.74%)'),
    ]
    
    for model_path, description in cv_models:
        if os.path.exists(model_path):
            print(f"âœ… ä½¿ç”¨å›¾åƒç¼–ç å™¨: {description}")
            return model_path
    
    # Fallback to a default path if none of the specific fold models exist.
    default_path = './models/contrastive/best_contrastive_image_encoder.pth'
    if os.path.exists(default_path):
        print(f"âœ… ä½¿ç”¨å›¾åƒç¼–ç å™¨: å¤‡ç”¨æ€»ä½“æœ€ä½³æ¨¡å‹")
        return default_path

    raise FileNotFoundError("âŒ å…³é”®é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨æ¨¡å‹ã€‚è¯·å…ˆè¿è¡Œå›¾åƒç¼–ç å™¨è®­ç»ƒã€‚")


def main():
    """ä¸»å‡½æ•° - è§£æå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒ"""
    parser = argparse.ArgumentParser(description="å¯¹æŠ—æ€§å¯¹æ¯”å­¦ä¹ è®­ç»ƒè„šæœ¬ (V2.2)")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', type=str, default='standard', 
                       choices=['standard', 'mcic', 'cv', 'mcic-cv'],
                       help='è®­ç»ƒæ¨¡å¼: standard(æ ‡å‡†), mcic(MCIcæ•°æ®), cv(äº¤å‰éªŒè¯), mcic-cv(MCIc+äº¤å‰éªŒè¯)')
    parser.add_argument('--epochs', type=int, default=30, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--no-cv', action='store_true', help='ç¦ç”¨äº¤å‰éªŒè¯ï¼Œä½¿ç”¨å•æ¬¡è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--fp16', action='store_true', help='å¯ç”¨FP16æ··åˆç²¾åº¦è®­ç»ƒ')
    
    # V2.2 æ¶ˆèå®éªŒå¼€å…³
    parser.add_argument('--no-cognitive-features', action='store_true', help='æ¶ˆè: ç¦ç”¨æ–‡æœ¬ç¼–ç å™¨ä¸­çš„è®¤çŸ¥åˆ†æ•°ç‰¹å¾')
    parser.add_argument('--no-disentanglement', action='store_true', help='æ¶ˆè: ç¦ç”¨å¯¹æ¯”å­¦ä¹ ä¸­çš„ç‰¹å¾è§£è€¦å’Œå¯¹æŠ—æŸå¤±')

    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cuda', 'cpu'], help='è®¾å¤‡é€‰æ‹© (é»˜è®¤:auto)')
    parser.add_argument('--save-dir', type=str, default='./models/adversarial/', help='æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤:./models/adversarial/)')

    args = parser.parse_args()

    # â—ï¸ä¸ºäº†ç¡®ä¿æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ï¼Œåœ¨æ­¤è®¾ç½®å…¨å±€éšæœºç§å­
    set_seed(42)

    # æ™ºèƒ½è®¾å¤‡é€‰æ‹©
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    print(f"ğŸ’» è¿è¡Œè®¾å¤‡: {device}")
    
    # ğŸ¯ é…ç½®å‚æ•° (æ ¹æ®æ¨¡å¼è°ƒæ•´)
    config = {
        # æ•°æ®è·¯å¾„é…ç½® - V3.5 æ›´æ–°
        'image_data_dir': '/root/autodl-tmp/DATA_MCI/',
        'text_data_dir': '/root/autodl-tmp/DATA_MCI/', # æ–‡æœ¬å…ƒæ•°æ®ä¹Ÿåœ¨æ­¤ç›®å½•ä¸‹
        'save_dir': args.save_dir,
        
        # ğŸ”¥ æ¨¡å‹è·¯å¾„ - ä½¿ç”¨æœ€ä½³æ¨¡å‹
        'image_model_path': './models/contrastive/fold_0_best_model.pth',  # ç¬¬0æŠ˜æœ€ä½³æ¨¡å‹
        
        # è®­ç»ƒå‚æ•°
        'batch_size': args.batch_size,
        'num_epochs': args.epochs,
        'learning_rate': 1e-4,
        'weight_decay': 1e-2,
        'gradient_clip': 0.5,
        
        # 5æŠ˜äº¤å‰éªŒè¯å‚æ•°
        'cv_folds': 5,
        'random_state': 42,
        'use_cv': not args.no_cv,  # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®š
        
        # æ¨¡å¼ç‰¹å®šé…ç½®
        'mode': args.mode,
        'mcic_data': 'mcic' in args.mode,  # æ˜¯å¦ä½¿ç”¨MCIcæ•°æ®æ ¼å¼
        
        'device': device,
        'use_fp16': args.fp16,
        # V2.2 æ¶ˆèå¼€å…³
        'use_cognitive_features': not args.no_cognitive_features,
        'use_disentanglement': not args.no_disentanglement,
    }
    
    # ğŸ”¥ æ ¹æ®æ¨¡å¼è°ƒæ•´é…ç½®
    if config['mcic_data']:
        print("ğŸ¯ MCIcæ•°æ®ä¸“ç”¨é…ç½®:")
        print(f"   ğŸ“ ä½¿ç”¨MCIcæ ¼å¼çš„ä¸´åºŠæ–‡æœ¬æ•°æ®")
        print(f"   ğŸ§  ä¸“é—¨é’ˆå¯¹è®¤çŸ¥è¯„ä¼°åˆ†æ•°å¤„ç†")
        print(f"   ğŸ“Š é€‚é…MCIcæ•°æ®çš„ç‰¹å¾æå–")
        
        # MCIcç‰¹å®šé…ç½®
        config.update({
            'mcic_text_format': True,
            'cognitive_assessment_enhanced': True,
            'specialized_mcic_processing': True
        })
    
    print(f"\nğŸ¯ æ ¸å¿ƒæŠ€æœ¯ç‰¹æ€§:")
    print(f"   ğŸ§  å¤šå…ƒå›å½’è®¤çŸ¥è¯„ä¼°æ ¡æ­£")
    print(f"   ğŸ”— å¼ºåˆ¶å›¾åƒ-æ–‡æœ¬ç‰¹å¾å¯¹é½")
    print(f"   ğŸ›¡ï¸ å¼ºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ")
    if config['use_cv']:
        print(f"   ğŸ“Š 5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯ï¼Œç¡®ä¿æ‚£è€…çº§åˆ«åˆ†å‰²")
    else:
        print(f"   ğŸ”„ å•æ¬¡è®­ç»ƒæ¨¡å¼")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config['save_dir'], exist_ok=True)
    
    try:
        if config['use_cv']:
            # ğŸ¯ ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯æ¨¡å¼
            if config['mcic_data']:
                print("ğŸ”„ MCIcæ•°æ® + 5æŠ˜äº¤å‰éªŒè¯æ¨¡å¼")
                cv_results = run_mcic_adversarial_cross_validation(config)
            else:
                print("ğŸ”„ æ ‡å‡†æ•°æ® + 5æŠ˜äº¤å‰éªŒè¯æ¨¡å¼") 
                cv_results = run_adversarial_cross_validation(config)
            
            if cv_results:
                print(f"\nğŸ‰ 5æŠ˜äº¤å‰éªŒè¯å®Œæˆ!")
                print(f"ğŸ“ˆ å¹³å‡å‡†ç¡®ç‡: {cv_results['mean_accuracy']:.4f} Â± {cv_results['std_accuracy']:.4f}")
                print(f"ğŸ“Š æœ€ä½³æŠ˜å‡†ç¡®ç‡: {max(cv_results['fold_accuracies']):.4f}")
                
                # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
                cv_save_path = os.path.join(config['save_dir'], f"{'mcic_' if config['mcic_data'] else ''}adversarial_cv_results.json")
                with open(cv_save_path, 'w', encoding='utf-8') as f:
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨convert_numpy_typeså¤„ç†æ•°æ®
                    serializable_results = convert_numpy_types(cv_results)
                    json.dump(serializable_results, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ äº¤å‰éªŒè¯ç»“æœå·²ä¿å­˜: {cv_save_path}")
            else:
                print("âŒ äº¤å‰éªŒè¯å¤±è´¥")
        else:
            # ğŸ”„ ä¼ ç»Ÿå•æ¬¡è®­ç»ƒæ¨¡å¼
            if config['mcic_data']:
                print("ğŸ”„ MCIcæ•°æ®å•æ¬¡è®­ç»ƒæ¨¡å¼")
                success = run_mcic_single_training(config)
            else:
                print("ğŸ”„ æ ‡å‡†æ•°æ®å•æ¬¡è®­ç»ƒæ¨¡å¼")
                success = run_standard_single_training(config)
            
            if success:
                print("âœ… å•æ¬¡è®­ç»ƒå®Œæˆ")
            else:
                print("âŒ å•æ¬¡è®­ç»ƒå¤±è´¥")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def run_adversarial_cross_validation(config):
    """åœ¨æ ‡å‡†AD/CNæ•°æ®é›†ä¸Šè¿è¡Œ5æŠ˜äº¤å‰éªŒè¯"""
    print("ğŸ”„ å¼€å§‹åœ¨æ ‡å‡† AD/CN æ•°æ®é›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯...")
    
    # 1. åŠ è½½æ•°æ®
    print("--- æ­¥éª¤ 1: åŠ è½½æ–‡æœ¬å’Œå›¾åƒæ•°æ® ---")
    texts, labels, patient_ids = load_text_data_with_cognitive_scores(config['text_data_dir'])
    
    # å‡è®¾å›¾åƒæ•°æ®å·²ç»é¢„å¤„ç†å¹¶ä¿å­˜ä¸º .pkl æ–‡ä»¶
    image_data_path = os.path.join(config['image_data_dir'], 'preprocessed_images.pkl')
    if not os.path.exists(image_data_path):
        raise FileNotFoundError(f"âŒ å…³é”®é”™è¯¯: æœªæ‰¾åˆ°é¢„å¤„ç†çš„å›¾åƒæ•°æ®æ–‡ä»¶ {image_data_path}ã€‚è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ã€‚")
    
    with open(image_data_path, 'rb') as f:
        image_data = pickle.load(f)
    
    # æ ¹æ® patient_ids åŒ¹é…å›¾åƒæ•°æ®
    images = np.array([image_data[pid] for pid in patient_ids if pid in image_data])
    
    # è¿‡æ»¤æ‰æ²¡æœ‰å¯¹åº”å›¾åƒçš„æ–‡æœ¬æ•°æ®
    valid_indices = [i for i, pid in enumerate(patient_ids) if pid in image_data]
    texts = [texts[i] for i in valid_indices]
    labels = labels[valid_indices]

    if len(images) != len(texts):
        raise ValueError("âŒ å›¾åƒå’Œæ–‡æœ¬æ•°æ®æ ·æœ¬æ•°ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(labels)} ä¸ªåŒ¹é…æ ·æœ¬")

    # 2. åˆå§‹åŒ–äº¤å‰éªŒè¯
    kfold = StratifiedKFold(n_splits=config['cv_folds'], shuffle=True, random_state=config['random_state'])
    
    all_fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(np.zeros(len(labels)), labels)):
        print(f"\n{'='*20} Fold {fold + 1}/{config['cv_folds']} {'='*20}")
        
        # 3. åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
        model = AdversarialContrastiveModel(
            image_model_path=config.get('image_model_path', get_best_image_model_path()),
            device=config['device'],
            use_cognitive_features=config.get('use_cognitive_features', True),
            use_disentanglement=config.get('use_disentanglement', True)
        ).to(config['device'])
        
        trainer = AdversarialContrastiveTrainer(model, config['device'], config)
        
        # 4. åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
        train_dataset = AdversarialMultiModalDataset(images[train_idx], [texts[i] for i in train_idx], labels[train_idx])
        val_dataset = AdversarialMultiModalDataset(images[val_idx], [texts[i] for i in val_idx], labels[val_idx])
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)
        
        # 5. è®­ç»ƒå’Œè¯„ä¼°
        best_val_acc = 0
        history = []

        for epoch in range(config['num_epochs']):
            train_metrics = trainer.train_epoch(train_loader, epoch)
            val_metrics = trainer.evaluate(val_loader)
            
            print(f"Epoch {epoch+1:02d} | Train Acc: {train_metrics['accuracy']:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | Total Loss: {train_metrics['total_loss']:.4f}")
            
            epoch_history = {**train_metrics, **{'val_'+k: v for k, v in val_metrics.items()}}
            history.append(epoch_history)
            
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                save_path = os.path.join(config['save_dir'], f"standard_fold_{fold}_best_model.pth")
                torch.save(model.state_dict(), save_path)
                print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path} (ACC: {best_val_acc:.4f})")

        all_fold_results.append({'fold': fold, 'best_accuracy': best_val_acc, 'history': history})
        
    # 6. æ±‡æ€»äº¤å‰éªŒè¯ç»“æœ
    fold_accuracies = [r['best_accuracy'] for r in all_fold_results]
    mean_accuracy = np.mean(fold_accuracies)
    std_accuracy = np.std(fold_accuracies)
    
    return {
        'fold_results': all_fold_results,
        'fold_accuracies': fold_accuracies,
        'mean_accuracy': mean_accuracy,
        'std_accuracy': std_accuracy
    }


def run_mcic_adversarial_cross_validation(config):
    """åœ¨MCIc/MCIncæ•°æ®é›†ä¸Šè¿è¡Œ5æŠ˜äº¤å‰éªŒè¯"""
    print("ğŸ”„ å¼€å§‹åœ¨ MCIc/MCInc æ•°æ®é›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯...")
    
    # 1. åŠ è½½æ•°æ®
    print("--- æ­¥éª¤ 1: åŠ è½½MCIcæ ¼å¼æ–‡æœ¬å’Œå›¾åƒæ•°æ® ---")
    texts, labels, patient_ids = load_multiple_mcic_files(config['text_data_dir'])
    
    # å‡è®¾å›¾åƒæ•°æ®å·²ç»é¢„å¤„ç†å¹¶ä¿å­˜ä¸º .pkl æ–‡ä»¶
    image_data_path = os.path.join(config['image_data_dir'], 'mcic_preprocessed_images.pkl')
    if not os.path.exists(image_data_path):
        raise FileNotFoundError(f"âŒ å…³é”®é”™è¯¯: æœªæ‰¾åˆ°MCIcé¢„å¤„ç†çš„å›¾åƒæ•°æ®æ–‡ä»¶ {image_data_path}ã€‚è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ã€‚")
    
    with open(image_data_path, 'rb') as f:
        image_data = pickle.load(f)
        
    # æ ¹æ® patient_ids åŒ¹é…å›¾åƒæ•°æ®
    images = np.array([image_data[pid] for pid in patient_ids if pid in image_data])
    
    # è¿‡æ»¤æ‰æ²¡æœ‰å¯¹åº”å›¾åƒçš„æ–‡æœ¬æ•°æ®
    valid_indices = [i for i, pid in enumerate(patient_ids) if pid in image_data]
    texts = [texts[i] for i in valid_indices]
    labels = labels[valid_indices]

    if len(images) != len(texts):
        raise ValueError("âŒ å›¾åƒå’Œæ–‡æœ¬æ•°æ®æ ·æœ¬æ•°ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(labels)} ä¸ªåŒ¹é…æ ·æœ¬")
    
    # 2. åˆå§‹åŒ–äº¤å‰éªŒè¯
    kfold = StratifiedKFold(n_splits=config['cv_folds'], shuffle=True, random_state=config['random_state'])
    
    all_fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(np.zeros(len(labels)), labels)):
        print(f"\n{'='*20} Fold {fold + 1}/{config['cv_folds']} {'='*20}")
        
        # 3. åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
        model = AdversarialContrastiveModel(
            image_model_path=get_best_image_model_path(),
            device=config['device'],
            use_cognitive_features=config.get('use_cognitive_features', True),
            use_disentanglement=config.get('use_disentanglement', True)
        ).to(config['device'])
        
        trainer = AdversarialContrastiveTrainer(model, config['device'], config)

        # 4. åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
        train_dataset = AdversarialMultiModalDataset(images[train_idx], [texts[i] for i in train_idx], labels[train_idx])
        val_dataset = AdversarialMultiModalDataset(images[val_idx], [texts[i] for i in val_idx], labels[val_idx])
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)
        
        # 5. è®­ç»ƒå’Œè¯„ä¼°
        best_val_acc = 0
        history = []

        for epoch in range(config['num_epochs']):
            train_metrics = trainer.train_epoch(train_loader, epoch)
            val_metrics = trainer.evaluate(val_loader)
            
            print(f"Epoch {epoch+1:02d} | Train Acc: {train_metrics['accuracy']:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | Total Loss: {train_metrics['total_loss']:.4f}")
            
            epoch_history = {**train_metrics, **{'val_'+k: v for k, v in val_metrics.items()}}
            history.append(epoch_history)
            
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                save_path = os.path.join(config['save_dir'], f"mcic_fold_{fold}_best_model.pth")
                torch.save(model.state_dict(), save_path)
                print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path} (ACC: {best_val_acc:.4f})")
    
        all_fold_results.append({'fold': fold, 'best_accuracy': best_val_acc, 'history': history})
        
    # 6. æ±‡æ€»äº¤å‰éªŒè¯ç»“æœ
    fold_accuracies = [r['best_accuracy'] for r in all_fold_results]
    mean_accuracy = np.mean(fold_accuracies)
    std_accuracy = np.std(fold_accuracies)
    
    return {
        'fold_results': all_fold_results,
        'fold_accuracies': fold_accuracies,
        'mean_accuracy': mean_accuracy,
        'std_accuracy': std_accuracy
    }


def run_mcic_single_training(config):
    """åœ¨MCIc/MCIncæ•°æ®é›†ä¸Šè¿è¡Œå•æ¬¡è®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹åœ¨ MCIc/MCInc æ•°æ®é›†ä¸Šè¿›è¡Œå•æ¬¡è®­ç»ƒ...")

    # 1. åŠ è½½æ•°æ®
    print("--- æ­¥éª¤ 1: åŠ è½½MCIcæ ¼å¼æ–‡æœ¬å’Œå›¾åƒæ•°æ® ---")
    texts, labels, patient_ids = load_multiple_mcic_files(config['text_data_dir'])
    
    image_data_path = os.path.join(config['image_data_dir'], 'mcic_preprocessed_images.pkl')
    if not os.path.exists(image_data_path):
        raise FileNotFoundError(f"âŒ å…³é”®é”™è¯¯: æœªæ‰¾åˆ°MCIcé¢„å¤„ç†çš„å›¾åƒæ•°æ®æ–‡ä»¶ {image_data_path}ã€‚")
    
    with open(image_data_path, 'rb') as f:
        image_data = pickle.load(f)
    
    images = np.array([image_data[pid] for pid in patient_ids if pid in image_data])
    valid_indices = [i for i, pid in enumerate(patient_ids) if pid in image_data]
    texts = [texts[i] for i in valid_indices]
    labels = labels[valid_indices]
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(labels)} ä¸ªåŒ¹é…æ ·æœ¬")
    
    # 2. åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    model = AdversarialContrastiveModel(
        image_model_path=get_best_image_model_path(),
        device=config['device'],
        use_cognitive_features=config.get('use_cognitive_features', True),
        use_disentanglement=config.get('use_disentanglement', True)
    ).to(config['device'])

    trainer = AdversarialContrastiveTrainer(model, config['device'], config)
    
    # 3. åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
    dataset = AdversarialMultiModalDataset(images, texts, labels)
    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)
    
    # 4. è®­ç»ƒ
    for epoch in range(config['num_epochs']):
        train_metrics = trainer.train_epoch(dataloader, epoch)
        print(f"Epoch {epoch+1:02d} | Train Acc: {train_metrics['accuracy']:.4f} | Total Loss: {train_metrics['total_loss']:.4f}")

    # 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_path = os.path.join(config['save_dir'], "best_mcic_adversarial_model.pth")
    torch.save(model.state_dict(), save_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    return True


def run_standard_single_training(config):
    """åœ¨æ ‡å‡†AD/CNæ•°æ®é›†ä¸Šè¿è¡Œå•æ¬¡è®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹åœ¨æ ‡å‡† AD/CN æ•°æ®é›†ä¸Šè¿›è¡Œå•æ¬¡è®­ç»ƒ...")
    
    # 1. åŠ è½½æ•°æ®
    print("--- æ­¥éª¤ 1: åŠ è½½æ ‡å‡†æ–‡æœ¬å’Œå›¾åƒæ•°æ® ---")
    texts, labels, patient_ids = load_text_data_with_cognitive_scores(config['text_data_dir'])
    
    image_data_path = os.path.join(config['image_data_dir'], 'preprocessed_images.pkl')
    if not os.path.exists(image_data_path):
        raise FileNotFoundError(f"âŒ å…³é”®é”™è¯¯: æœªæ‰¾åˆ°é¢„å¤„ç†çš„å›¾åƒæ•°æ®æ–‡ä»¶ {image_data_path}ã€‚")
        
    with open(image_data_path, 'rb') as f:
        image_data = pickle.load(f)
    
    images = np.array([image_data[pid] for pid in patient_ids if pid in image_data])
    valid_indices = [i for i, pid in enumerate(patient_ids) if pid in image_data]
    texts = [texts[i] for i in valid_indices]
    labels = labels[valid_indices]
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(labels)} ä¸ªåŒ¹é…æ ·æœ¬")

    # 2. åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    model = AdversarialContrastiveModel(
        image_model_path=config.get('image_model_path', get_best_image_model_path()),
        device=config['device'],
        use_cognitive_features=config.get('use_cognitive_features', True),
        use_disentanglement=config.get('use_disentanglement', True)
    ).to(config['device'])

    trainer = AdversarialContrastiveTrainer(model, config['device'], config)

    # 3. åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
    dataset = AdversarialMultiModalDataset(images, texts, labels)
    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)

    # 4. è®­ç»ƒ
    for epoch in range(config['num_epochs']):
        train_metrics = trainer.train_epoch(dataloader, epoch)
        print(f"Epoch {epoch+1:02d} | Train Acc: {train_metrics['accuracy']:.4f} | Total Loss: {train_metrics['total_loss']:.4f}")

    # 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_path = os.path.join(config['save_dir'], "best_standard_adversarial_model.pth")
    torch.save(model.state_dict(), save_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {save_path}")

    return True


def convert_numpy_types(obj):
    """
    ğŸ”§ é€’å½’è½¬æ¢numpyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç”¨äºJSONåºåˆ—åŒ–
    
    Args:
        obj: å¾…è½¬æ¢çš„å¯¹è±¡
        
    Returns:
        è½¬æ¢åçš„å¯¹è±¡
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj


if __name__ == "__main__":
    main() 