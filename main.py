import os
import torch
import torch.nn as nn
import json
import numpy as np
import random
from torch.utils.data import Subset, DataLoader

from data_utils import create_tissue_specific_dataset, create_data_loaders
from advanced_trainer import AdvancedTrainer, train_advanced_models, train_improved_resnet, train_fusion_with_improved_models
from optimized_models import create_improved_resnet3d
from early_fusion import train_early_fusion_model, train_hierarchical_swin_model
from early_fusion_fixed import train_memory_optimized_early_fusion
from quick_finetune import quick_finetune_model
from deep_architecture_finetune import deep_architecture_finetune

# è®¾ç½®CUDAå†…å­˜åˆ†é…å™¨å¹¶ä¼˜åŒ–æ€§èƒ½
torch.cuda.set_per_process_memory_fraction(0.95)
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–CUDAæ€§èƒ½
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
os.environ["TORCH_CUDNN_V8_API_ENABLED"] = "1"



def train_improved_tissue_models(data_path, device, export_path='./models'):
    """ä½¿ç”¨æ”¹è¿›çš„ResNetCBAM3Dæ¨¡å‹è®­ç»ƒå„ä¸ªç»„ç»‡ç±»å‹çš„æ¨¡å‹"""
    tissue_types = ['CSF', 'GRAY', 'WHITE']
    
    # åˆ›å»ºå¯¼å‡ºç›®å½•
    os.makedirs(export_path, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨å­—å…¸
    data_loaders = {}
    
    for tissue_type in tissue_types:
        print(f"\nå‡†å¤‡ {tissue_type} ç»„ç»‡çš„æ•°æ®...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = create_tissue_specific_dataset(data_path, tissue_type)
        
        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_data_loaders(dataset, batch_size=16)
        
        # ä¿å­˜æ•°æ®åŠ è½½å™¨
        data_loaders[f'train_{tissue_type}'] = train_loader
        data_loaders[f'val_{tissue_type}'] = val_loader
        
        print(f"{tissue_type} æ•°æ®é›†ç»Ÿè®¡:")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_loader.dataset)}")
        
    # ä½¿ç”¨æ”¹è¿›çš„è®­ç»ƒå‡½æ•°è®­ç»ƒæ¨¡å‹
    results = train_improved_resnet(
        data_loaders=data_loaders,
        device=device,
        save_dir=export_path,
        tissue_types=tissue_types
    )
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n===== æ”¹è¿›æ¨¡å‹è®­ç»ƒç»“æœ =====")
    for tissue_type, result in results.items():
        print(f"{tissue_type}: æœ€ä½³éªŒè¯å‡†ç¡®ç‡ = {result['best_val_acc']:.2f}%, "
              f"æœ€ä½³è½®æ¬¡ = {result['best_epoch']}")
    
    return results

def train_improved_fusion(data_loaders, device, export_path='./models'):
    """è®­ç»ƒæ”¹è¿›çš„èåˆæ¨¡å‹"""
    tissue_types = ['CSF', 'GRAY', 'WHITE']
    
    # å‡†å¤‡æ¨¡å‹è·¯å¾„
    model_paths = {}
    for tissue_type in tissue_types:
        model_path = f"{export_path}/best_improved_resnet_{tissue_type}.pth"
        if os.path.exists(model_path):
            model_paths[tissue_type] = model_path
            print(f"æ‰¾åˆ° {tissue_type} é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° {tissue_type} é¢„è®­ç»ƒæ¨¡å‹")
    
    if not model_paths:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯·å…ˆè®­ç»ƒå•ä¸ªæ”¹è¿›æ¨¡å‹ã€‚")
        return None, 0
    
    # è®­ç»ƒèåˆæ¨¡å‹
    fusion_model, best_val_acc = train_fusion_with_improved_models(
        data_loaders=data_loaders,
        device=device,
        model_paths=model_paths,
        tissue_types=tissue_types,
        save_dir=export_path
    )
    
    print(f"\nèåˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    return fusion_model, best_val_acc

def create_unified_dataset(data_path):
    """åˆ›å»ºç»Ÿä¸€çš„æ•°æ®é›†ï¼Œç¡®ä¿æ¯ä¸ªæ‚£è€…çš„ä¸‰ä¸ªæ¨¡æ€éƒ½è¢«åŠ è½½"""
    # åˆ›å»ºå„ä¸ªç»„ç»‡ç±»å‹çš„æ•°æ®é›†
    print("\n====== åˆ›å»ºæ•°æ®é›† ======")
    modality_datasets = {}
    
    for tissue_type in ['CSF', 'GRAY', 'WHITE']:
        print(f"\nåŠ è½½ {tissue_type} æ•°æ®é›†...")
        modality_datasets[tissue_type] = create_tissue_specific_dataset(data_path, tissue_type)
    
    # åˆ›å»ºæ‚£è€…IDåˆ°å„æ¨¡æ€æ ·æœ¬ç´¢å¼•çš„æ˜ å°„
    patient_modalities = {}
    
    # å¤„ç†æ¯ä¸ªæ¨¡æ€çš„æ•°æ®é›†
    for tissue_type, dataset in modality_datasets.items():
        for i, patient_id in enumerate(dataset.patient_ids):
            if patient_id not in patient_modalities:
                patient_modalities[patient_id] = {'label': dataset.labels[i], 'modalities': {}}
            
            # ä¿å­˜è¯¥æ‚£è€…è¯¥æ¨¡æ€çš„ç´¢å¼•
            patient_modalities[patient_id]['modalities'][tissue_type] = i
    
    # ç­›é€‰å‡ºæ‹¥æœ‰æ‰€æœ‰ä¸‰ä¸ªæ¨¡æ€çš„æ‚£è€…
    complete_patients = []
    for patient_id, info in patient_modalities.items():
        if len(info['modalities']) == 3:  # æ‚£è€…æœ‰å…¨éƒ¨ä¸‰ä¸ªæ¨¡æ€
            complete_patients.append(patient_id)
    
    print(f"\næ‹¥æœ‰å…¨éƒ¨ä¸‰ä¸ªæ¨¡æ€çš„æ‚£è€…æ•°: {len(complete_patients)}")
    print(f"æ‹¥æœ‰éƒ¨åˆ†æ¨¡æ€çš„æ‚£è€…æ•°: {len(patient_modalities) - len(complete_patients)}")
    
    if len(complete_patients) == 0:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æ‹¥æœ‰å…¨éƒ¨ä¸‰ä¸ªæ¨¡æ€çš„æ‚£è€…ï¼")
    
    return modality_datasets, complete_patients, patient_modalities

def create_patient_aware_splits(modality_datasets, complete_patients, patient_modalities):
    """åˆ›å»ºè€ƒè™‘æ‚£è€…æ•´ä½“çš„æ•°æ®é›†åˆ’åˆ†ï¼Œä¿è¯AD/CNåˆ†å¸ƒå¹³è¡¡"""
    # æŒ‰æ ‡ç­¾(AD/CN)åˆ†ç»„
    ad_patients = []
    cn_patients = []
    
    for patient_id in complete_patients:
        if patient_modalities[patient_id]['label'] == 0:  # AD
            ad_patients.append(patient_id)
        else:  # CN
            cn_patients.append(patient_id)
    
    print(f"\næŒ‰ç–¾ç—…åˆ†ç»„çš„æ‚£è€…ç»Ÿè®¡:")
    print(f"ADæ‚£è€…æ€»æ•°: {len(ad_patients)}")
    print(f"CNæ‚£è€…æ€»æ•°: {len(cn_patients)}")
    
    # éšæœºæ‰“ä¹±ï¼Œä½†ä¿æŒç–¾ç—…ç±»åˆ«ç‹¬ç«‹
    random.shuffle(ad_patients)
    random.shuffle(cn_patients)
    
    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†åˆ«åˆ’åˆ†
    train_ad = ad_patients[:int(len(ad_patients)*0.7)]
    val_ad = ad_patients[int(len(ad_patients)*0.7):int(len(ad_patients)*0.85)]
    test_ad = ad_patients[int(len(ad_patients)*0.85):]
    
    train_cn = cn_patients[:int(len(cn_patients)*0.7)]
    val_cn = cn_patients[int(len(cn_patients)*0.7):int(len(cn_patients)*0.85)]
    test_cn = cn_patients[int(len(cn_patients)*0.85):]
    
    # åˆå¹¶ä¿æŒå¹³è¡¡çš„åˆ’åˆ†
    train_patients = train_ad + train_cn
    val_patients = val_ad + val_cn
    test_patients = test_ad + test_cn
    
    # æ‰“ä¹±åˆå¹¶åçš„åˆ—è¡¨ï¼Œä¿æŒAD/CNæ¯”ä¾‹ä½†éšæœºæ’åº
    random.shuffle(train_patients)
    random.shuffle(val_patients)
    random.shuffle(test_patients)
    
    print(f"\næŒ‰ç–¾ç—…å¹³è¡¡çš„æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯:")
    print(f"è®­ç»ƒé›†: AD={len(train_ad)}, CN={len(train_cn)}, æ€»è®¡={len(train_patients)}")
    print(f"éªŒè¯é›†: AD={len(val_ad)}, CN={len(val_cn)}, æ€»è®¡={len(val_patients)}")
    print(f"æµ‹è¯•é›†: AD={len(test_ad)}, CN={len(test_cn)}, æ€»è®¡={len(test_patients)}")
    
    # ä¸ºæ¯ä¸ªç»„ç»‡ç±»å‹åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loaders = {}
    val_loaders = {}
    test_loaders = {}
    
    for tissue_type, dataset in modality_datasets.items():
        # åˆ›å»ºç´¢å¼•
        train_indices = [patient_modalities[pid]['modalities'][tissue_type] 
                         for pid in train_patients if tissue_type in patient_modalities[pid]['modalities']]
        val_indices = [patient_modalities[pid]['modalities'][tissue_type] 
                         for pid in val_patients if tissue_type in patient_modalities[pid]['modalities']]
        test_indices = [patient_modalities[pid]['modalities'][tissue_type] 
                         for pid in test_patients if tissue_type in patient_modalities[pid]['modalities']]
        
        # åˆ›å»ºå­é›†
        train_dataset = Subset(dataset, train_indices)
        val_dataset = Subset(dataset, val_indices)
        test_dataset = Subset(dataset, test_indices)
        
        print(f"\n{tissue_type} æ•°æ®é›†:")
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_indices)}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_indices)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_indices)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loaders[tissue_type] = DataLoader(
            train_dataset,
            batch_size=32,
            shuffle=True,
            num_workers=8,
            pin_memory=True
        )
        
        val_loaders[tissue_type] = DataLoader(
            val_dataset,
            batch_size=32,
            shuffle=False,
            num_workers=8,
            pin_memory=True
        )
        
        test_loaders[tissue_type] = DataLoader(
            test_dataset,
            batch_size=32,
            shuffle=False,
            num_workers=8,
            pin_memory=True
        )
    
    return train_loaders, val_loaders, test_loaders

def train_quick_validation_model(data_loaders, device, save_dir='./models'):
    """
    å¿«é€ŸéªŒè¯æ¨¡å‹ - ç”¨äºè°ƒè¯•å’ŒéªŒè¯è®­ç»ƒæµç¨‹
    ä½¿ç”¨è½»é‡çº§æ¶æ„ï¼Œå¿«é€Ÿæ”¶æ•›ï¼Œä¾¿äºå‘ç°é—®é¢˜
    """
    import torch.optim as optim
    from torch.cuda import amp
    from tqdm import tqdm
    import os
    import numpy as np
    
    print("\n===== å¿«é€ŸéªŒè¯æ¨¡å‹è®­ç»ƒ =====")
    
    # åˆ›å»ºè½»é‡çº§éªŒè¯æ¨¡å‹
    class QuickValidationModel(nn.Module):
        def __init__(self, in_channels=3, num_classes=2):
            super(QuickValidationModel, self).__init__()
            
            # æç®€çš„ç‰¹å¾æå–å™¨
            self.features = nn.Sequential(
                # ç¬¬ä¸€å±‚ - å¤§å¹…é™é‡‡æ ·
                nn.Conv3d(in_channels, 8, kernel_size=7, stride=4, padding=3),
                nn.BatchNorm3d(8),
                nn.ReLU(inplace=True),
                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),
                
                # ç¬¬äºŒå±‚
                nn.Conv3d(8, 16, kernel_size=5, stride=2, padding=2),
                nn.BatchNorm3d(16),
                nn.ReLU(inplace=True),
                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),
                
                # ç¬¬ä¸‰å±‚
                nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm3d(32),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool3d((1, 1, 1))
            )
            
            # ç®€å•åˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.Linear(32, 16),
                nn.ReLU(inplace=True),
                nn.Dropout(0.3),
                nn.Linear(16, num_classes)
            )
            
            # æƒé‡åˆå§‹åŒ–
            self._initialize_weights()
        
        def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Conv3d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                elif isinstance(m, nn.BatchNorm3d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)
        
        def forward(self, x):
            x = self.features(x)
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            return x
    
    # åˆ›å»ºæ—©æœŸèåˆæ•°æ®åŠ è½½å™¨
    from early_fusion import create_early_fusion_loaders
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨å­—å…¸
    train_data_loaders = {f'train_{k}': v for k, v in data_loaders['train'].items()}
    val_data_loaders = {f'val_{k}': v for k, v in data_loaders['val'].items()}
    
    fusion_loaders = create_early_fusion_loaders(
        {**train_data_loaders, **val_data_loaders}, 
        batch_size=32,  # ä»8æ”¹ä¸º32ï¼Œå……åˆ†åˆ©ç”¨32GB GPUæ˜¾å­˜
        debug=True
    )
    
    train_loader = fusion_loaders['train']
    val_loader = fusion_loaders['val']
    
    # åˆ›å»ºæ¨¡å‹
    model = QuickValidationModel(in_channels=3, num_classes=2).to(device)
    
    # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
    sample_batch, sample_labels = next(iter(train_loader))
    sample_batch = sample_batch.to(device)
    with torch.no_grad():
        sample_output = model(sample_batch)
        print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {sample_output.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{sample_output.min().item():.3f}, {sample_output.max().item():.3f}]")
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œè¾ƒé«˜å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
    optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=0.01)
    
    # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-5)
    
    # æ£€æŸ¥æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ
    train_labels = []
    for _, labels in train_loader:
        train_labels.extend(labels.numpy())
    
    label_counts = np.bincount(train_labels)
    print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {label_counts}")
    
    # ä½¿ç”¨å¹³è¡¡çš„ç±»åˆ«æƒé‡
    if len(label_counts) == 2 and min(label_counts) > 0:
        total_samples = sum(label_counts)
        class_weights = torch.FloatTensor([
            total_samples / (2 * label_counts[0]),
            total_samples / (2 * label_counts[1])
        ]).to(device)
        print(f"ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights}")
    else:
        class_weights = None
        print("ä½¿ç”¨å‡åŒ€æƒé‡")
    
    # äº¤å‰ç†µæŸå¤±
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = amp.GradScaler()
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 25  # å¿«é€Ÿè®­ç»ƒ
    best_val_acc = 0.0
    best_model_state = None
    patience = 8  # è¾ƒçŸ­çš„è€å¿ƒå€¼
    no_improve_epochs = 0
    
    # è®­ç»ƒç»Ÿè®¡
    stats = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'lr': []
    }
    
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"å¼€å§‹å¿«é€ŸéªŒè¯è®­ç»ƒï¼Œæ€»è½®æ¬¡: {num_epochs}")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Quick Val]')
        
        for inputs, labels in train_pbar:
            inputs = inputs.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with amp.autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            # ç»Ÿè®¡
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            train_pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        # æ··æ·†çŸ©é˜µ
        conf_matrix = torch.zeros(2, 2, dtype=torch.long)
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
                
                # æ›´æ–°æ··æ·†çŸ©é˜µ
                for t, p in zip(labels.view(-1), predicted.view(-1)):
                    conf_matrix[t.long(), p.long()] += 1
        
        # è®¡ç®—æŒ‡æ ‡
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        val_acc_per_class = []
        for i in range(2):
            correct = conf_matrix[i, i].item()
            total = conf_matrix[i, :].sum().item()
            val_acc_per_class.append(100.0 * correct / max(1, total))
        
        # è®°å½•ç»Ÿè®¡
        stats['train_loss'].append(avg_train_loss)
        stats['train_acc'].append(train_acc)
        stats['val_loss'].append(avg_val_loss)
        stats['val_acc'].append(val_acc)
        stats['lr'].append(optimizer.param_groups[0]['lr'])
        
        # æ‰“å°ä¿¡æ¯
        print(f'\nEpoch [{epoch+1}/{num_epochs}] - Quick Validation:')
        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'Val Acc per class: {val_acc_per_class}')
        print(f'Learning Rate: {optimizer.param_groups[0]["lr"]:.6f}')
        print(f'æ··æ·†çŸ©é˜µ:\n{conf_matrix}')
        
        # æ£€æŸ¥æ˜¯å¦ä¸¤ä¸ªç±»åˆ«éƒ½æœ‰é¢„æµ‹
        both_classes_predicted = conf_matrix[0, 0] > 0 and conf_matrix[1, 1] > 0
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': avg_val_loss,
                'stats': stats,
                'conf_matrix': conf_matrix.tolist()
            }
            
            torch.save(best_model_state, f'{save_dir}/best_quick_validation_model.pth')
            print(f'ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%')
            no_improve_epochs = 0
        else:
            no_improve_epochs += 1
        
        # æ—©åœæ£€æŸ¥
        if no_improve_epochs >= patience:
            print(f'æ—©åœåœ¨epoch {epoch+1}')
            break
        
        # å¦‚æœæ¨¡å‹å¼€å§‹é¢„æµ‹ä¸¤ä¸ªç±»åˆ«ï¼Œè¯´æ˜è®­ç»ƒæ­£å¸¸
        if both_classes_predicted:
            print("âœ“ æ¨¡å‹æ­£å¸¸ï¼šèƒ½å¤Ÿé¢„æµ‹ä¸¤ä¸ªç±»åˆ«")
        else:
            print("âš  è­¦å‘Šï¼šæ¨¡å‹åªé¢„æµ‹å•ä¸€ç±»åˆ«")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state['model_state_dict'])
        print(f'å·²åŠ è½½æœ€ä½³å¿«é€ŸéªŒè¯æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
    
    return {
        'model': model,
        'best_val_acc': best_val_acc,
        'best_epoch': best_model_state['epoch'] if best_model_state else -1,
        'model_path': f'{save_dir}/best_quick_validation_model.pth',
        'stats': stats,
        'final_conf_matrix': conf_matrix.tolist()
    }

def main():
    # è®¾ç½®éšæœºç§å­
    seed = 42
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è¯·æ±‚ç”¨æˆ·è¾“å…¥æ•°æ®ç›®å½•
    print("\nè¯·è¾“å…¥æ•°æ®ç›®å½•è·¯å¾„:")
    data_root = input("æ•°æ®æ ¹ç›®å½•è·¯å¾„: ").strip()
    if not data_root:
        # é»˜è®¤è·¯å¾„ - éµå¾ªMCI_DATAè§„èŒƒ
        data_root = "/root/autodl-tmp/MCI_DATA"

    # è®¾ç½®æ•°æ®è·¯å¾„ - æ›´æ–°ä¸ºç”¨æˆ·è¾“å…¥æˆ–é»˜è®¤è·¯å¾„ï¼Œéµå¾ªMCI_DATAè§„èŒƒ
    data_path = {
        'ad_dir': os.path.join(data_root, "totalAD"),
        'cn_dir': os.path.join(data_root, "totalCN")
    }
    
    # éªŒè¯åŸºç¡€ç›®å½•æ˜¯å¦å­˜åœ¨
    print("\néªŒè¯åŸºç¡€ç›®å½•:")
    print(f"ADåŸºç¡€ç›®å½•: {data_path['ad_dir']}")
    print(f"ADåŸºç¡€ç›®å½•å­˜åœ¨: {os.path.exists(data_path['ad_dir'])}")
    print(f"CNåŸºç¡€ç›®å½•: {data_path['cn_dir']}")
    print(f"CNåŸºç¡€ç›®å½•å­˜åœ¨: {os.path.exists(data_path['cn_dir'])}")
    
    if not os.path.exists(data_path['ad_dir']):
        raise ValueError(f"ADåŸºç¡€ç›®å½•ä¸å­˜åœ¨: {data_path['ad_dir']}")
    if not os.path.exists(data_path['cn_dir']):
        raise ValueError(f"CNåŸºç¡€ç›®å½•ä¸å­˜åœ¨: {data_path['cn_dir']}")
    
    # åˆ—å‡ºåŸºç¡€ç›®å½•çš„å†…å®¹
    print("\nADåŸºç¡€ç›®å½•å†…å®¹:")
    for item in os.listdir(data_path['ad_dir']):
        print(f"  - {item}")
    
    print("\nCNåŸºç¡€ç›®å½•å†…å®¹:")
    for item in os.listdir(data_path['cn_dir']):
        print(f"  - {item}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n===== åˆ›å»ºæ•°æ®åŠ è½½å™¨ =====")
    try:
        # åˆ›å»ºç»Ÿä¸€æ•°æ®é›†
        modality_datasets, complete_patients, patient_modalities = create_unified_dataset(data_path)
        
        # åˆ›å»ºæ‚£è€…æ„ŸçŸ¥çš„æ•°æ®åˆ’åˆ†
        train_loaders, val_loaders, test_loaders = create_patient_aware_splits(
            modality_datasets, complete_patients, patient_modalities
        )
        
        # ç»„ç»‡æ•°æ®åŠ è½½å™¨ä¸ºæœŸæœ›çš„æ ¼å¼
        data_loaders = {
            'train': train_loaders,
            'val': val_loaders,
            'test': test_loaders
        }
        
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"âœ… è®­ç»ƒåŠ è½½å™¨: {list(train_loaders.keys())}")
        print(f"âœ… éªŒè¯åŠ è½½å™¨: {list(val_loaders.keys())}")
        print(f"âœ… æµ‹è¯•åŠ è½½å™¨: {list(test_loaders.keys())}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # è®¾ç½®æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_dir = './models'
    os.makedirs(model_save_dir, exist_ok=True)
    
    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    all_results = []
    
    # è¯¢é—®ç”¨æˆ·é€‰æ‹©æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•
    print("\nè¯·é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹:")
    print("1. è®­ç»ƒæ‰€æœ‰ç»„ç»‡ç±»å‹çš„æ¨¡å‹ (CSF, GRAY, WHITE)")
    print("3. è®­ç»ƒæ—©æœŸèåˆæ¨¡å‹")
    print("6. å±‚æ¬¡åŒ–Swin-Transformer - é˜Ÿå‹æå‡ºçš„æ–°æ¶æ„ (æœ€æ–°)")
    print("7. å†…å­˜ä¼˜åŒ–ç‰ˆæ—©æœŸèåˆæ¨¡å‹è®­ç»ƒ - ä¿®å¤ç‰ˆ (æ¨è)")
    print("8. æ·±åº¦æ¶æ„å¾®è°ƒ - æ›´æ·±å±‚æ¬¡çš„ä¿¡æ¯ä¿ç•™ä¼˜åŒ–")
    print("0. é€€å‡º")
    model_choice = input("è¯·è¾“å…¥é€‰é¡¹ (0,1,3,6-8): ").strip()
    
    if model_choice == "1":
        # ä½¿ç”¨åŸå§‹çš„é«˜çº§è®­ç»ƒå‡½æ•°
        print("\n====== è®­ç»ƒæ‰€æœ‰ç»„ç»‡ç±»å‹çš„æ¨¡å‹ ======")
        results, fusion_model = train_advanced_models(data_loaders, device, fusion_type='adaptive')
        
        # æ‰“å°å„ä¸ªæ¨¡å‹çš„æ€§èƒ½
        print("\n====== å•ä¸€æ¨¡å‹æ€§èƒ½ ======")
        model_accuracies = {}
        for model_name, result in results.items():
            accuracy = result['best_val_acc']
            model_accuracies[model_name] = accuracy
            print(f"{model_name} æ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        # ä¿å­˜ç»“æœ
        try:
            results_file = 'model_results.json'
            with open(results_file, 'w') as f:
                json.dump(model_accuracies, f, indent=4)
            print(f"\nç»“æœå·²ä¿å­˜åˆ° {results_file}")
        except Exception as e:
            print(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {str(e)}")
        
        all_results.append({
            'type': 'æ‰€æœ‰ç»„ç»‡ç±»å‹æ¨¡å‹',
            'results': results
        })
    
    elif model_choice == "3":
        print("=== è®­ç»ƒæ—©æœŸèåˆæ¨¡å‹ ===")
        from early_fusion import train_early_fusion_model, create_early_fusion_loaders
        
        # åˆ›å»ºæ—©æœŸèåˆæ•°æ®åŠ è½½å™¨
        fusion_loaders = create_early_fusion_loaders(data_loaders, batch_size=4, debug=True)
        
        # è®­ç»ƒæ—©æœŸèåˆæ¨¡å‹
        fusion_results = train_early_fusion_model(
            fusion_loaders,  # ä¼ é€’æ­£ç¡®æ ¼å¼çš„æ•°æ®åŠ è½½å™¨
            device,
            save_dir=model_save_dir
        )
        
        all_results.append({
            'type': 'æ—©æœŸèåˆ',
            'results': fusion_results
        })
    
    elif model_choice == "6":
        print("=== è®­ç»ƒå±‚æ¬¡åŒ–Swin-Transformeræ¨¡å‹ ===")
        from early_fusion import train_hierarchical_swin_model
        model_info = train_hierarchical_swin_model(data_loaders, device, save_dir=model_save_dir)
        all_results.append({
            'type': 'å±‚æ¬¡åŒ–Swin-Transformer',
            'results': model_info
        })
        
    elif model_choice == "7":
        print("=== å†…å­˜ä¼˜åŒ–ç‰ˆæ—©æœŸèåˆæ¨¡å‹è®­ç»ƒ ===")
        # ä½¿ç”¨ä¿®å¤ç‰ˆè®­ç»ƒå‡½æ•°
        model_info = train_memory_optimized_early_fusion(data_loaders, device, save_dir=model_save_dir)
        all_results.append({
            'type': 'å†…å­˜ä¼˜åŒ–æ—©æœŸèåˆ',
            'results': model_info
        })
    
    elif model_choice == "8":
        print("=== æ·±åº¦æ¶æ„å¾®è°ƒ - æ›´æ·±å±‚æ¬¡çš„ä¿¡æ¯ä¿ç•™ä¼˜åŒ– ===")
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹
        model_path = "./models/best_memory_optimized_early_fusion.pth"
        if not os.path.exists(model_path):
            print("âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            print("   å»ºè®®é€‰æ‹©é€‰é¡¹7è¿›è¡Œè®­ç»ƒ")
        else:
            print(f"âœ… æ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
            print("ğŸ”§ å°†åŸºäºç°æœ‰æ¨¡å‹åˆ›å»ºå¢å¼ºç‰ˆæ¶æ„...")
            
            # è¯¢é—®è®­ç»ƒè½®æ¬¡
            try:
                epochs = int(input("è¯·è¾“å…¥è®­ç»ƒè½®æ¬¡ (æ¨è10-15è½®): ").strip() or "10")
                if epochs < 5 or epochs > 30:
                    print("è½®æ¬¡åº”åœ¨5-30ä¹‹é—´ï¼Œä½¿ç”¨é»˜è®¤å€¼10")
                    epochs = 10
            except ValueError:
                print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼10")
                epochs = 10
            
            # æ‰§è¡Œæ·±åº¦æ¶æ„å¾®è°ƒ
            best_acc = deep_architecture_finetune(model_path, data_loaders, device, epochs=epochs)
            
            if best_acc:
                all_results.append({
                    'type': f'æ·±åº¦æ¶æ„å¾®è°ƒ({epochs}è½®)',
                    'results': {'best_val_acc': best_acc}
                })
    
    elif model_choice == "0":
        print("é€€å‡ºç¨‹åº")
        return
    
    else:
        print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")
        return
    
    # è®­ç»ƒå®Œæˆåçš„ç»“æœæ±‡æ€»
    print("\n" + "="*50)
    print("           è®­ç»ƒå®Œæˆ - ç»“æœæ±‡æ€»")
    print("="*50)
    
    if all_results:
        for result in all_results:
            result_type = result['type']
            result_data = result['results']
            
            print(f"\nğŸ”¹ {result_type}:")
            if isinstance(result_data, dict):
                if 'best_val_acc' in result_data:
                    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result_data['best_val_acc']:.2f}%")
                if 'best_epoch' in result_data:
                    print(f"   æœ€ä½³è½®æ¬¡: {result_data['best_epoch']}")
                if 'model_path' in result_data and result_data['model_path']:
                    print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {result_data['model_path']}")
            else:
                print(f"   ç»“æœ: {result_data}")
        
        print(f"\nâœ… æ‰€æœ‰è®­ç»ƒä»»åŠ¡å·²å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•: {model_save_dir}")
    else:
        print("âš ï¸ æ²¡æœ‰è®­ç»ƒä»»åŠ¡å®Œæˆ")

if __name__ == "__main__":
    main() 