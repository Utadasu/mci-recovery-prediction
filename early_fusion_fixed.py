#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæ—©æœŸèåˆè®­ç»ƒè„šæœ¬
è§£å†³äº†å†…å­˜ä¸è¶³ã€ç±»åˆ«ä¸å¹³è¡¡å’ŒBatchNormé—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.cuda import amp
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import os
from tqdm import tqdm
from datetime import datetime
import json
from sklearn.metrics import balanced_accuracy_score, f1_score

def create_memory_optimized_early_fusion_loaders(data_loaders, gpu_memory_gb=32, debug=True):
    """
    åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„æ—©æœŸèåˆæ•°æ®åŠ è½½å™¨
    
    å‚æ•°:
    - data_loaders: åŒ…å«å„ç»„ç»‡ç±»å‹è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨çš„å­—å…¸
    - gpu_memory_gb: GPUæ˜¾å­˜å¤§å°(GB)
    - debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    
    è¿”å›:
    - åŒ…å«æ—©æœŸèåˆè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨çš„å­—å…¸
    """
    print(f"\n===== åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„æ—©æœŸèåˆæ•°æ®é›† =====")
    print(f"æ£€æµ‹åˆ°GPUæ˜¾å­˜: {gpu_memory_gb:.1f}GB")
    
    # æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
    if gpu_memory_gb >= 30:  # 32GBæ˜¾å­˜
        batch_size = 4
        num_workers = 4
        print("ğŸ”¥ ä½¿ç”¨é«˜ç«¯GPUé…ç½®")
    elif gpu_memory_gb >= 20:  # 24GBæ˜¾å­˜
        batch_size = 2
        num_workers = 2
        print("âš¡ ä½¿ç”¨ä¸­é«˜ç«¯GPUé…ç½®")
    elif gpu_memory_gb >= 10:  # 16GBæ˜¾å­˜
        batch_size = 2
        num_workers = 2
        print("ğŸ¯ ä½¿ç”¨ä¸­ç«¯GPUé…ç½®")
    else:  # 8GBåŠä»¥ä¸‹
        batch_size = 1
        num_workers = 1
        print("ğŸ’» ä½¿ç”¨å…¥é—¨GPUé…ç½®")
    
    print(f"æœ€ç»ˆé…ç½®: batch_size={batch_size}, num_workers={num_workers}")
    
    # å¯¼å…¥æ•°æ®é›†ç±»
    from early_fusion import HierarchicalEarlyFusionDataset
    
    # æ£€æŸ¥æ•°æ®åŠ è½½å™¨ç»“æ„å¹¶æå–æ­£ç¡®çš„åŠ è½½å™¨
    if 'train' in data_loaders and 'val' in data_loaders:
        # æ–°çš„æ•°æ®ç»“æ„: {'train': {...}, 'val': {...}, 'test': {...}}
        train_loaders = data_loaders['train']
        val_loaders = data_loaders['val']
        
        train_csf_loader = train_loaders['CSF']
        train_grey_loader = train_loaders['GRAY']
        train_white_loader = train_loaders['WHITE']
        
        val_csf_loader = val_loaders['CSF']
        val_grey_loader = val_loaders['GRAY']
        val_white_loader = val_loaders['WHITE']
        
        print("âœ… ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„æ ¼å¼")
        
    elif 'train_CSF' in data_loaders:
        # æ—§çš„æ•°æ®ç»“æ„: {'train_CSF': ..., 'val_CSF': ..., ...}
        train_csf_loader = data_loaders['train_CSF']
        train_grey_loader = data_loaders['train_GRAY']
        train_white_loader = data_loaders['train_WHITE']
        
        val_csf_loader = data_loaders['val_CSF']
        val_grey_loader = data_loaders['val_GRAY']
        val_white_loader = data_loaders['val_WHITE']
        
        print("âœ… ä½¿ç”¨æ—§çš„æ•°æ®ç»“æ„æ ¼å¼")
        
    else:
        raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ•°æ®åŠ è½½å™¨æ ¼å¼ã€‚å¯ç”¨é”®: {list(data_loaders.keys())}")
    
    # åˆ›å»ºèåˆæ•°æ®é›†
    train_fusion_dataset = HierarchicalEarlyFusionDataset(
        train_csf_loader,
        train_grey_loader,
        train_white_loader,
        debug=debug
    )
    
    val_fusion_dataset = HierarchicalEarlyFusionDataset(
        val_csf_loader,
        val_grey_loader,
        val_white_loader,
        debug=debug
    )
    
    # åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    train_fusion_loader = torch.utils.data.DataLoader(
        train_fusion_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=False,  # ç¦ç”¨ä»¥èŠ‚çœå†…å­˜
        persistent_workers=False,  # ç¦ç”¨æŒä¹…åŒ–worker
        drop_last=True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
    )
    
    val_fusion_loader = torch.utils.data.DataLoader(
        val_fusion_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=False,
        persistent_workers=False,
        drop_last=False
    )
    
    # éªŒè¯èåˆåçš„æ•°æ®åŠ è½½å™¨
    print(f"âœ… è®­ç»ƒèåˆåŠ è½½å™¨: æ‰¹æ¬¡å¤§å°={train_fusion_loader.batch_size}, æ ·æœ¬æ•°={len(train_fusion_dataset)}")
    print(f"âœ… éªŒè¯èåˆåŠ è½½å™¨: æ‰¹æ¬¡å¤§å°={val_fusion_loader.batch_size}, æ ·æœ¬æ•°={len(val_fusion_dataset)}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    try:
        sample_batch, sample_labels = next(iter(train_fusion_loader))
        print(f"âœ… èåˆåæ‰¹æ¬¡å½¢çŠ¶: {sample_batch.shape}, æ ‡ç­¾å½¢çŠ¶: {sample_labels.shape}")
        print(f"âœ… æ ‡ç­¾åˆ†å¸ƒ: {torch.bincount(sample_labels)}")
        
        # ä¼°ç®—å†…å­˜ä½¿ç”¨
        batch_memory = sample_batch.numel() * 4 / 1024**2  # MB
        print(f"âœ… å•æ‰¹æ¬¡å†…å­˜ä¼°ç®—: {batch_memory:.1f}MB")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        raise
    
    return {
        'train': train_fusion_loader,
        'val': val_fusion_loader,
        'batch_size': batch_size,
        'num_workers': num_workers
    }

def train_memory_optimized_early_fusion(data_loaders, device, save_dir='./models'):
    """
    å†…å­˜ä¼˜åŒ–ç‰ˆçš„æ—©æœŸèåˆæ¨¡å‹è®­ç»ƒ
    
    è§£å†³çš„é—®é¢˜:
    1. GPUå†…å­˜ä¸è¶³ -> è‡ªé€‚åº”æ‰¹æ¬¡å¤§å° + æ¢¯åº¦ç´¯ç§¯
    2. ç±»åˆ«ä¸å¹³è¡¡ -> æ”¹è¿›çš„æŸå¤±å‡½æ•° + ç±»åˆ«æƒé‡
    3. BatchNormé—®é¢˜ -> LayerNormæ›¿ä»£
    """
    print(f"\n===== å†…å­˜ä¼˜åŒ–ç‰ˆæ—©æœŸèåˆæ¨¡å‹è®­ç»ƒ =====")
    
    # å†…å­˜ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # æ£€æŸ¥GPUæ˜¾å­˜
    if torch.cuda.is_available():
        gpu_properties = torch.cuda.get_device_properties(device)
        total_memory = gpu_properties.total_memory / 1024**3
        print(f"GPU: {gpu_properties.name}")
        print(f"æ€»æ˜¾å­˜: {total_memory:.1f}GB")
        torch.cuda.empty_cache()
    else:
        total_memory = 4  # CPUé»˜è®¤å€¼
    
    # åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    fusion_loaders = create_memory_optimized_early_fusion_loaders(
        data_loaders, 
        gpu_memory_gb=total_memory,
        debug=True
    )
    
    train_loader = fusion_loaders['train']
    val_loader = fusion_loaders['val']
    batch_size = fusion_loaders['batch_size']
    
    # æ ¹æ®æ˜¾å­˜é€‰æ‹©æ¨¡å‹é…ç½®
    if total_memory >= 30:  # 32GB
        base_channels = 12  # ä»8å¢åŠ åˆ°12ï¼Œæå‡æ¨¡å‹å®¹é‡
        accumulation_steps = 2
        print("ğŸ”¥ ä½¿ç”¨å¢å¼ºæ ‡å‡†æ¨¡å‹é…ç½®")
    elif total_memory >= 20:  # 24GB
        base_channels = 8  # ä»6å¢åŠ åˆ°8
        accumulation_steps = 3  # å‡å°‘ç´¯ç§¯æ­¥æ•°ä»¥åˆ©ç”¨æ›´å¤§æ¨¡å‹
        print("âš¡ ä½¿ç”¨å¢å¼ºç´§å‡‘æ¨¡å‹é…ç½®")
    else:  # 16GBåŠä»¥ä¸‹
        base_channels = 6
        accumulation_steps = 8
        print("ğŸ’» ä½¿ç”¨è¶…ç´§å‡‘æ¨¡å‹é…ç½®")
    
    effective_batch_size = batch_size * accumulation_steps
    print(f"é…ç½®: base_channels={base_channels}, æ¢¯åº¦ç´¯ç§¯={accumulation_steps}æ­¥")
    print(f"ç­‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    
    # åˆ›å»ºå†…å­˜ä¼˜åŒ–æ¨¡å‹
    from optimized_models import create_improved_resnet3d
    
    model = create_improved_resnet3d(
        in_channels=3,
        device=device,
        base_channels=base_channels,
        dropout_rate=0.3
    )
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼æµ‹è¯•ï¼Œç„¶ååˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    model.eval()
    test_input = torch.randn(1, 3, 113, 137, 113).to(device)
    with torch.no_grad():
        test_output = model(test_input)
    print(f"âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ: {test_input.shape} -> {test_output.shape}")
    del test_input, test_output
    torch.cuda.empty_cache()
    
    model.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    try:
        train_stats = train_loader.dataset.get_stats()
        ad_count = train_stats['ad_count']
        cn_count = train_stats['cn_count']
        total = ad_count + cn_count
        
        # æ”¹è¿›çš„ç±»åˆ«æƒé‡è®¡ç®—ï¼Œä½¿ç”¨æ›´å¹³æ»‘çš„æƒé‡èŒƒå›´
        class_weights = torch.FloatTensor([
            1.5 * total / (2 * ad_count),  # åŸºäºæ ·æœ¬æ¯”ä¾‹çš„æƒé‡
            1.5 * total / (2 * cn_count)
        ]).clamp(0.5, 2.0).to(device)  # é™åˆ¶æƒé‡åœ¨0.5-2.0ä¹‹é—´ï¼Œå‡å°‘æç«¯æ³¢åŠ¨
        
        print(f"âœ… æ•°æ®é›†ç»Ÿè®¡: AD={ad_count}, CN={cn_count}")
        print(f"âœ… ç±»åˆ«æƒé‡: {class_weights}")
        
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è·å–ç±»åˆ«ç»Ÿè®¡ï¼Œä½¿ç”¨é»˜è®¤æƒé‡: {e}")
        class_weights = torch.FloatTensor([1.0, 1.0]).to(device)
    
    # æ”¹è¿›çš„Focal Loss - æ·»åŠ ç±»åˆ«å¹³è¡¡æ­£åˆ™åŒ–
    class ImprovedFocalLoss(nn.Module):
        def __init__(self, alpha=None, gamma=2.0, weight=None, label_smoothing=0.05, balance_reg=0.1):
            super().__init__()
            self.alpha = alpha
            self.gamma = gamma
            self.weight = weight
            self.label_smoothing = label_smoothing
            self.balance_reg = balance_reg  # ç±»åˆ«å¹³è¡¡æ­£åˆ™åŒ–ç³»æ•°
            
        def forward(self, inputs, targets):
            # æ ‡ç­¾å¹³æ»‘
            num_classes = inputs.size(1)
            if self.label_smoothing > 0:
                targets_one_hot = F.one_hot(targets, num_classes).float()
                targets_one_hot = targets_one_hot * (1 - self.label_smoothing) + \
                                self.label_smoothing / num_classes
            else:
                targets_one_hot = F.one_hot(targets, num_classes).float()
            
            # è®¡ç®—äº¤å‰ç†µ
            log_probs = F.log_softmax(inputs, dim=1)
            ce_loss = -(targets_one_hot * log_probs).sum(dim=1)
            
            # è®¡ç®—æ¦‚ç‡
            probs = torch.exp(log_probs)
            target_probs = (targets_one_hot * probs).sum(dim=1)
            
            # Focal loss
            focal_weight = (1 - target_probs) ** self.gamma
            focal_loss = focal_weight * ce_loss
            
            # åº”ç”¨ç±»åˆ«æƒé‡
            if self.weight is not None:
                weight_t = self.weight[targets]
                focal_loss = focal_loss * weight_t
            
            # ç±»åˆ«å¹³è¡¡æ­£åˆ™åŒ–ï¼šæƒ©ç½šé¢„æµ‹æ¦‚ç‡çš„æç«¯åå·®
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡é¢„æµ‹æ¦‚ç‡
            class_probs = probs.mean(dim=0)
            # è®¡ç®—ç±»åˆ«å¹³è¡¡æ­£åˆ™åŒ–é¡¹ï¼šé¼“åŠ±ç±»åˆ«æ¦‚ç‡æ¥è¿‘å‡åŒ€åˆ†å¸ƒ
            balance_loss = torch.sum((class_probs - 1/num_classes) ** 2)
            
            # æ€»æŸå¤± = FocalæŸå¤± + å¹³è¡¡æ­£åˆ™åŒ–
            total_loss = focal_loss.mean() + self.balance_reg * balance_loss
            
            return total_loss
    
    # ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
    criterion = ImprovedFocalLoss(
        gamma=2.5,  # å¢åŠ gammaå€¼ï¼Œæ›´å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬
        weight=class_weights,
        label_smoothing=0.1  # å¢åŠ æ ‡ç­¾å¹³æ»‘ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
    )
    
    # ä¼˜åŒ–å™¨é…ç½®
    lr = 0.0001
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=lr, 
        weight_decay=0.005,  # é™ä½æƒé‡è¡°å‡ï¼Œå‡å°‘æ­£åˆ™åŒ–å¼ºåº¦
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ä¼˜åŒ–
    num_epochs = 60  # å¢åŠ åˆ°60è½®ï¼Œç»™æ¨¡å‹æ›´å¤šæ—¶é—´ä¼˜åŒ–
    warmup_epochs = 8  # å»¶é•¿é¢„çƒ­æœŸ
    
    # ä½¿ç”¨æ›´ç¨³å®šçš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œå¢åŠ é‡å¯æœºåˆ¶
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs - warmup_epochs,
        eta_min=lr * 0.01,  # é€‚å½“æé«˜æœ€å°å­¦ä¹ ç‡ï¼Œé¿å…å­¦ä¹ ç‡è¿‡ä½å¯¼è‡´æ¨¡å‹åœæ»
        verbose=False
    )
    
    # æ¢¯åº¦è£å‰ªé…ç½®
    grad_clip_max_norm = 1.0  # ä¿æŒæ¢¯åº¦è£å‰ªå¼ºåº¦
    print(f"âœ… ä¼˜åŒ–å™¨é…ç½®: AdamW, åˆå§‹LR={lr}, æƒé‡è¡°å‡={optimizer.param_groups[0]['weight_decay']}")
    print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦: CosineAnnealingLR, é¢„çƒ­={warmup_epochs}è½®, T_max={num_epochs-warmup_epochs}")
    print(f"âœ… æ¢¯åº¦è£å‰ª: max_norm={grad_clip_max_norm}")
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = amp.GradScaler()
    
    # è®­ç»ƒç»Ÿè®¡
    stats = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_balanced_acc': [],
        'val_f1_score': [],
        'val_acc_per_class': [],
        'lr': []
    }
    
    # è®­ç»ƒçŠ¶æ€
    best_val_acc = 0.0
    best_balanced_acc = 0.0  # æœ€ä½³å¹³è¡¡å‡†ç¡®ç‡
    best_model_state = None
    patience = 25  # å¢åŠ è€å¿ƒå€¼ï¼Œä»15å¢åˆ°25
    no_improve_epochs = 0
    
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»è½®æ¬¡: {num_epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, æ¢¯åº¦ç´¯ç§¯: {accumulation_steps}æ­¥")
    print(f"ç­‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    
    for epoch in range(num_epochs):
        # å­¦ä¹ ç‡é¢„çƒ­
        if epoch < warmup_epochs:
            # ä½¿ç”¨æ›´å¹³æ»‘çš„é¢„çƒ­æ›²çº¿
            warmup_factor = min(1.0, (epoch + 1) / warmup_epochs)
            current_lr = lr * (0.01 + 0.99 * warmup_factor**2)  # äºŒæ¬¡æ›²çº¿é¢„çƒ­
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr
        else:
            scheduler.step()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
        
        optimizer.zero_grad()
        
        for batch_idx, (inputs, labels) in enumerate(train_pbar):
            inputs = inputs.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            # ç®€å•çš„æ•°æ®å¢å¼ºï¼šéšæœºæ·»åŠ å°‘é‡å™ªå£°
            if torch.rand(1).item() < 0.3:  # 30%æ¦‚ç‡æ·»åŠ å™ªå£°
                noise = torch.randn_like(inputs) * 0.01
                inputs = inputs + noise
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with amp.autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss = loss / accumulation_steps  # å½’ä¸€åŒ–æ¢¯åº¦ç´¯ç§¯
            
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % accumulation_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            
            # ç»Ÿè®¡
            train_loss += loss.item() * accumulation_steps
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            train_pbar.set_postfix({
                'loss': f'{loss.item() * accumulation_steps:.4f}',
                'acc': f'{100.*train_correct/train_total:.2f}%'
            })
            
            # å®šæœŸæ¸…ç†å†…å­˜
        if batch_idx % 10 == 0:
            torch.cuda.empty_cache()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        conf_matrix = torch.zeros(2, 2, dtype=torch.long)
        
        all_labels = []
        all_predictions = []
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                with amp.autocast():
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
                
                # ä¿å­˜æ‰€æœ‰æ ‡ç­¾å’Œé¢„æµ‹ï¼Œç”¨äºè®¡ç®—å¹³è¡¡å‡†ç¡®ç‡å’ŒF1åˆ†æ•°
                all_labels.extend(labels.cpu().numpy())
                all_predictions.extend(predicted.cpu().numpy())
                
                # æ›´æ–°æ··æ·†çŸ©é˜µ
                for t, p in zip(labels.view(-1), predicted.view(-1)):
                    conf_matrix[t.long(), p.long()] += 1
        
        # è®¡ç®—æŒ‡æ ‡
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        
        # è®¡ç®—å¹³è¡¡å‡†ç¡®ç‡å’ŒF1åˆ†æ•°
        val_balanced_acc = 100. * balanced_accuracy_score(all_labels, all_predictions)
        val_f1 = 100. * f1_score(all_labels, all_predictions, average='weighted')
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        val_acc_per_class = []
        for i in range(2):
            correct = conf_matrix[i, i].item()
            total = conf_matrix[i, :].sum().item()
            val_acc_per_class.append(100.0 * correct / max(1, total))
        
        # è®°å½•ç»Ÿè®¡
        stats['train_loss'].append(avg_train_loss)
        stats['train_acc'].append(train_acc)
        stats['val_loss'].append(avg_val_loss)
        stats['val_acc'].append(val_acc)
        stats['val_balanced_acc'].append(val_balanced_acc)
        stats['val_f1_score'].append(val_f1)
        stats['val_acc_per_class'].append(val_acc_per_class)
        stats['lr'].append(optimizer.param_groups[0]['lr'])
        
        # æ‰“å°ä¿¡æ¯
        print(f'\nEpoch [{epoch+1}/{num_epochs}] - å†…å­˜ä¼˜åŒ–ç‰ˆ:')
        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'Val Balanced Acc: {val_balanced_acc:.2f}%, Val F1: {val_f1:.2f}%')
        print(f'Val Acc per class: AD={val_acc_per_class[0]:.2f}%, CN={val_acc_per_class[1]:.2f}%')
        print(f'Learning Rate: {optimizer.param_groups[0]["lr"]:.6f}')
        print(f'æ··æ·†çŸ©é˜µ:\n{conf_matrix}')
        
        # æ£€æŸ¥æ˜¯å¦ä¸¤ä¸ªç±»åˆ«éƒ½æœ‰é¢„æµ‹
        both_classes_predicted = conf_matrix[0, 0] > 0 and conf_matrix[1, 1] > 0
        
        # æ”¹è¿›çš„æœ€ä½³æ¨¡å‹ä¿å­˜æ¡ä»¶ï¼šåŒæ—¶è€ƒè™‘éªŒè¯å‡†ç¡®ç‡å’Œå¹³è¡¡å‡†ç¡®ç‡
        current_score = 0.7 * val_acc + 0.3 * val_balanced_acc  # åŠ æƒç»¼åˆåˆ†æ•°
        best_score = 0.7 * best_val_acc + 0.3 * best_balanced_acc
        
        if current_score > best_score and both_classes_predicted:
            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            best_val_acc = val_acc
            best_balanced_acc = val_balanced_acc
            
            best_model_state = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_balanced_acc': val_balanced_acc,
                'val_f1': val_f1,
                'val_loss': avg_val_loss,
                'stats': stats,
                'conf_matrix': conf_matrix.tolist()
            }
            
            model_path = os.path.join(save_dir, 'best_memory_optimized_early_fusion.pth')
            torch.save(best_model_state, model_path)
            print(f'âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}ï¼Œç»¼åˆåˆ†æ•°: {current_score:.2f}')
            print(f'   - éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%')
            print(f'   - å¹³è¡¡å‡†ç¡®ç‡: {val_balanced_acc:.2f}%')
            no_improve_epochs = 0
        else:
            no_improve_epochs += 1
            if not both_classes_predicted:
                print(f'âš ï¸ æ¨¡å‹åªé¢„æµ‹å•ä¸€ç±»åˆ«ï¼Œè·³è¿‡ä¿å­˜')
            else:
                print(f'âš ï¸ æ¨¡å‹æ€§èƒ½æ— æ”¹å–„ ({no_improve_epochs}/{patience})')
        
        # æ—©åœæ£€æŸ¥
        if no_improve_epochs >= patience:
            print(f'æ—©åœåœ¨epoch {epoch+1}')
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state['model_state_dict'])
        print(f'âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
    
    return {
        'model': model,
        'best_val_acc': best_val_acc,
        'best_epoch': best_model_state['epoch'] if best_model_state else -1,
        'model_path': model_path if best_model_state else None,
        'stats': stats,
        'final_conf_matrix': conf_matrix.tolist(),
        'architecture': 'ImprovedResNetCBAM3D-MemoryOptimized'
    }

if __name__ == "__main__":
    print("å†…å­˜ä¼˜åŒ–çš„æ—©æœŸèåˆè®­ç»ƒè„šæœ¬")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("1. ä»main.pyè°ƒç”¨train_memory_optimized_early_fusionå‡½æ•°")
    print("2. æˆ–è€…å¯¼å…¥æ­¤æ¨¡å—ä½¿ç”¨ç›¸å…³å‡½æ•°") 