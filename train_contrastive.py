"""
ğŸš€ é˜¿å°”èŒ¨æµ·é»˜ç—…å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ è®­ç»ƒè„šæœ¬
========================================

è®­ç»ƒæµç¨‹:
1. ğŸ”„ åŠ è½½é¢„è®­ç»ƒçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨
2. ğŸ“Š åˆ›å»ºå›¾åƒ-æ–‡æœ¬é…å¯¹æ•°æ®é›†
3. ğŸ¯ å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒï¼ˆå›¾åƒ-æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼‰
4. ğŸ“ˆ å¾®è°ƒåˆ†ç±»ï¼ˆç«¯åˆ°ç«¯è®­ç»ƒï¼‰
5. ğŸ“‹ è¯„ä¼°å¤šæ¨¡æ€èåˆæ€§èƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import pickle
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tqdm import tqdm
import argparse
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from contrastive_learning import (
    MultiModalContrastiveModel, 
    PreExtractedFeaturesLoader,
    create_contrastive_model
)
from data_utils import load_early_fusion_data
from optimized_models import ImprovedResNetCBAM3D


class MultiModalDataset(Dataset):
    """
    å¤šæ¨¡æ€æ•°æ®é›† - å›¾åƒå’Œæ–‡æœ¬é…å¯¹
    """
    def __init__(self, 
                 image_data: np.ndarray,
                 text_features: np.ndarray,
                 labels: np.ndarray,
                 text_encoder,
                 mode: str = 'contrastive'):
        """
        Args:
            image_data: å›¾åƒæ•°æ® [N, 3, D, H, W]
            text_features: æ–‡æœ¬ç‰¹å¾ [N, 512] æˆ–æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾ [N]
            text_encoder: æ–‡æœ¬ç¼–ç å™¨ï¼ˆç”¨äºç¼–ç åŸå§‹æ–‡æœ¬ï¼‰
            mode: 'contrastive' æˆ– 'classification'
        """
        self.image_data = image_data
        self.text_features = text_features
        self.labels = labels
        self.text_encoder = text_encoder
        self.mode = mode
        
        print(f"ğŸ“Š å¤šæ¨¡æ€æ•°æ®é›†åˆå§‹åŒ–: {len(self.image_data)} æ ·æœ¬")
        print(f"   å›¾åƒå½¢çŠ¶: {self.image_data.shape}")
        print(f"   æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {self.text_features.shape}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: AD={np.sum(labels==1)}, CN={np.sum(labels==0)}")
    
    def __len__(self):
        return len(self.image_data)
    
    def __getitem__(self, idx):
        # å›¾åƒæ•°æ®
        image = torch.tensor(self.image_data[idx], dtype=torch.float32)
        
        # æ–‡æœ¬ç‰¹å¾ï¼ˆé¢„æå–çš„512ç»´ç‰¹å¾ï¼‰
        text_feature = torch.tensor(self.text_features[idx], dtype=torch.float32)
        
        # æ ‡ç­¾
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        # ä¸ºå¯¹æ¯”å­¦ä¹ ç”Ÿæˆè™šæ‹Ÿçš„input_idså’Œattention_mask
        # å®é™…ä½¿ç”¨ä¸­ï¼Œå¯ä»¥ä»é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨è·å–
        fake_input_ids = torch.zeros(128, dtype=torch.long)
        fake_attention_mask = torch.ones(128, dtype=torch.long)
        
        return {
            'image': image,
            'text_feature': text_feature,
            'input_ids': fake_input_ids,
            'attention_mask': fake_attention_mask,
            'label': label
        }


class ContrastiveTrainer:
    """
    å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨
    """
    def __init__(self, 
                 model: MultiModalContrastiveModel,
                 device: str = 'cuda',
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-5):
        self.model = model
        self.device = device
        
        # ä¸ºä¸åŒéƒ¨åˆ†è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
        self.optimizer = self._setup_optimizer(learning_rate, weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=50, eta_min=1e-6
        )
        
        # æŸå¤±æƒé‡
        self.contrastive_weight = 1.0
        self.classification_weight = 1.0
        
        print(f"ğŸ¯ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {device}")
    
    def _setup_optimizer(self, lr: float, weight_decay: float):
        """è®¾ç½®ä¼˜åŒ–å™¨ - ä¸ºä¸åŒæ¨¡å—ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡"""
        params = []
        
        # å›¾åƒç¼–ç å™¨æŠ•å½±å±‚ - è¾ƒé«˜å­¦ä¹ ç‡
        params.append({
            'params': self.model.image_encoder.image_projector.parameters(),
            'lr': lr * 2,
            'weight_decay': weight_decay
        })
        
        # æ–‡æœ¬ç¼–ç å™¨æŠ•å½±å±‚ - è¾ƒé«˜å­¦ä¹ ç‡
        params.append({
            'params': self.model.text_encoder.text_projector.parameters(),
            'lr': lr * 2,
            'weight_decay': weight_decay
        })
        
        # èåˆåˆ†ç±»å™¨ - æ ‡å‡†å­¦ä¹ ç‡
        params.append({
            'params': self.model.fusion_classifier.parameters(),
            'lr': lr,
            'weight_decay': weight_decay
        })
        
        # å¦‚æœéª¨å¹²ç½‘ç»œæœªå†»ç»“ï¼Œä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        if any(p.requires_grad for p in self.model.image_encoder.backbone.parameters()):
            params.append({
                'params': self.model.image_encoder.backbone.parameters(),
                'lr': lr * 0.1,
                'weight_decay': weight_decay
            })
        
        if any(p.requires_grad for p in self.model.text_encoder.bert.parameters()):
            params.append({
                'params': self.model.text_encoder.bert.parameters(),
                'lr': lr * 0.1,
                'weight_decay': weight_decay
            })
        
        return optim.AdamW(params)
    
    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0
        contrastive_loss_sum = 0
        classification_loss_sum = 0
        correct_predictions = 0
        total_samples = 0
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            images = batch['image'].to(self.device)
            text_features = batch['text_feature'].to(self.device)
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['label'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - å¯¹æ¯”å­¦ä¹ æ¨¡å¼
            contrastive_output = self.model(
                images, input_ids, attention_mask, mode='contrastive'
            )
            contrastive_loss = contrastive_output['contrastive_loss']
            
            # å‰å‘ä¼ æ’­ - åˆ†ç±»æ¨¡å¼
            classification_output = self.model(
                images, input_ids, attention_mask, mode='classification'
            )
            logits = classification_output['logits']
            
            # åˆ†ç±»æŸå¤±
            classification_loss = nn.CrossEntropyLoss()(logits, labels)
            
            # æ€»æŸå¤±
            total_batch_loss = (
                self.contrastive_weight * contrastive_loss + 
                self.classification_weight * classification_loss
            )
            
            # åå‘ä¼ æ’­
            total_batch_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += total_batch_loss.item()
            contrastive_loss_sum += contrastive_loss.item()
            classification_loss_sum += classification_loss.item()
            
            # å‡†ç¡®ç‡è®¡ç®—
            predictions = torch.argmax(logits, dim=1)
            correct_predictions += (predictions == labels).sum().item()
            total_samples += labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{total_batch_loss.item():.4f}',
                'Acc': f'{correct_predictions/total_samples:.4f}',
                'CL': f'{contrastive_loss.item():.4f}',
                'CE': f'{classification_loss.item():.4f}'
            })
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler.step()
        
        return {
            'total_loss': total_loss / len(dataloader),
            'contrastive_loss': contrastive_loss_sum / len(dataloader),
            'classification_loss': classification_loss_sum / len(dataloader),
            'accuracy': correct_predictions / total_samples,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }
    
    def evaluate(self, dataloader: DataLoader) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        all_predictions = []
        all_labels = []
        total_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc='è¯„ä¼°ä¸­'):
                images = batch['image'].to(self.device)
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # åˆ†ç±»æ¨¡å¼
                output = self.model(
                    images, input_ids, attention_mask, mode='classification'
                )
                logits = output['logits']
                
                # æŸå¤±
                loss = nn.CrossEntropyLoss()(logits, labels)
                total_loss += loss.item()
                
                # é¢„æµ‹
                predictions = torch.argmax(logits, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        report = classification_report(
            all_labels, all_predictions, 
            target_names=['CN', 'AD'], 
            output_dict=True
        )
        conf_matrix = confusion_matrix(all_labels, all_predictions)
        
        return {
            'accuracy': accuracy,
            'loss': total_loss / len(dataloader),
            'classification_report': report,
            'confusion_matrix': conf_matrix,
            'predictions': all_predictions,
            'labels': all_labels
        }


def load_multimodal_data(image_data_dir: str, 
                        text_features_path: str,
                        test_size: float = 0.2,
                        random_state: int = 42) -> Tuple:
    """
    åŠ è½½å¤šæ¨¡æ€æ•°æ®
    
    Args:
        image_data_dir: å›¾åƒæ•°æ®ç›®å½•
        text_features_path: æ–‡æœ¬ç‰¹å¾æ–‡ä»¶è·¯å¾„
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
    
    Returns:
        (train_images, train_text, train_labels, val_images, val_text, val_labels)
    """
    print("ğŸ”„ åŠ è½½å¤šæ¨¡æ€æ•°æ®...")
    
    # åŠ è½½å›¾åƒæ•°æ®
    print("ğŸ“¸ åŠ è½½å›¾åƒæ•°æ®...")
    image_data, labels = load_early_fusion_data(image_data_dir)
    print(f"âœ… å›¾åƒæ•°æ®åŠ è½½å®Œæˆ: {image_data.shape}")
    
    # åŠ è½½æ–‡æœ¬ç‰¹å¾
    print("ğŸ“ åŠ è½½æ–‡æœ¬ç‰¹å¾...")
    text_loader = PreExtractedFeaturesLoader(text_features_path)
    text_features = text_loader.features  # [N, 512]
    
    # ç¡®ä¿æ•°æ®å¯¹åº”
    assert len(image_data) == len(text_features), \
        f"å›¾åƒå’Œæ–‡æœ¬æ•°æ®æ•°é‡ä¸åŒ¹é…: {len(image_data)} vs {len(text_features)}"
    
    # æ•°æ®åˆ†å‰²
    train_indices, val_indices = train_test_split(
        range(len(image_data)), 
        test_size=test_size, 
        random_state=random_state,
        stratify=labels
    )
    
    train_images = image_data[train_indices]
    train_text = text_features[train_indices]
    train_labels = labels[train_indices]
    
    val_images = image_data[val_indices]
    val_text = text_features[val_indices]
    val_labels = labels[val_indices]
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_images)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_images)} æ ·æœ¬")
    
    return train_images, train_text, train_labels, val_images, val_text, val_labels


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ è®­ç»ƒ...")
    
    # é…ç½®å‚æ•°
    config = {
        'image_model_path': './models/smart_downsample_spatial_ch12.pth',  # ğŸ”¥ æ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹
        'text_model_path': '/tmp/pycharm_project_194/å¤‡ä»½5.27/æ–‡æœ¬ç¼–ç å™¨/alzheimer_bert_complete_model.pth',
        'text_features_path': '/tmp/pycharm_project_194/å¤‡ä»½5.27/æ–‡æœ¬ç¼–ç å™¨/alzheimer_features_512d.pkl',
        'image_data_dir': '/root/autodl-tmp/DATA_MCI/test_data/',  # ä¿®æ­£ä¸ºå®é™…æœåŠ¡å™¨è·¯å¾„
        'batch_size': 16,
        'num_epochs': 50,
        'learning_rate': 1e-4,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'save_dir': './models/contrastive',
        'freeze_backbones': True,
        'contrastive_weight': 1.0,
        'classification_weight': 2.0  # ç»™åˆ†ç±»ä»»åŠ¡æ›´é«˜æƒé‡
    }
    
    # éªŒè¯å¹¶ä¿®æ­£æ•°æ®è·¯å¾„
    if not os.path.exists(config['image_data_dir']):
        print(f"âš ï¸  é»˜è®¤æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {config['image_data_dir']}")
        
        # å°è¯•è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„
        possible_paths = [
            "/root/autodl-tmp/DATA_MCI/test_data/",
            "/data/alzheimer/mri/", 
            "/tmp/data/alzheimer/",
            "./data/mri/"
        ]
        
        found_path = None
        for path in possible_paths:
            if os.path.exists(path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„å­ç›®å½•ç»“æ„
                expected_subdirs = ["123-AD-MRI", "123-CN-MRI"]
                if all(os.path.exists(os.path.join(path, subdir)) for subdir in expected_subdirs):
                    found_path = path
                    break
        
        if found_path:
            config['image_data_dir'] = found_path
            print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°æ•°æ®è·¯å¾„: {found_path}")
        else:
            print("âŒ æ— æ³•è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š!")
            print("ğŸ’¡ è¯·ç¡®ä¿æ•°æ®ç›®å½•åŒ…å«ä»¥ä¸‹ç»“æ„:")
            print("   123-AD-MRI/ å’Œ 123-CN-MRI/")
            return
    
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config['save_dir'], exist_ok=True)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ¯ åˆ›å»ºå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹...")
    model = create_contrastive_model(
        image_model_path=config['image_model_path'],
        text_model_path=config['text_model_path'],
        device=config['device'],
        freeze_backbones=config['freeze_backbones']
    )
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    try:
        train_images, train_text, train_labels, val_images, val_text, val_labels = load_multimodal_data(
            image_data_dir=config['image_data_dir'],
            text_features_path=config['text_features_path']
        )
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print(f"   - å›¾åƒæ•°æ®ç›®å½•: {config['image_data_dir']}")
        print(f"   - æ–‡æœ¬ç‰¹å¾æ–‡ä»¶: {config['text_features_path']}")
        return
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MultiModalDataset(
        train_images, train_text, train_labels, 
        model.text_encoder, mode='contrastive'
    )
    val_dataset = MultiModalDataset(
        val_images, val_text, val_labels,
        model.text_encoder, mode='contrastive'
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ContrastiveTrainer(
        model=model,
        device=config['device'],
        learning_rate=config['learning_rate']
    )
    
    # è®¾ç½®æŸå¤±æƒé‡
    trainer.contrastive_weight = config['contrastive_weight']
    trainer.classification_weight = config['classification_weight']
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {config['num_epochs']} è½®...")
    best_val_accuracy = 0.0
    training_history = []
    
    for epoch in range(config['num_epochs']):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch+1}/{config['num_epochs']}")
        print(f"{'='*50}")
        
        # è®­ç»ƒ
        train_metrics = trainer.train_epoch(train_loader, epoch)
        
        # éªŒè¯
        val_metrics = trainer.evaluate(val_loader)
        
        # è®°å½•å†å²
        epoch_history = {
            'epoch': epoch + 1,
            'train': train_metrics,
            'val': val_metrics
        }
        training_history.append(epoch_history)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
        print(f"   è®­ç»ƒ - æŸå¤±: {train_metrics['total_loss']:.4f}, å‡†ç¡®ç‡: {train_metrics['accuracy']:.4f}")
        print(f"   éªŒè¯ - æŸå¤±: {val_metrics['loss']:.4f}, å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")
        print(f"   å­¦ä¹ ç‡: {train_metrics['learning_rate']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['accuracy'] > best_val_accuracy:
            best_val_accuracy = val_metrics['accuracy']
            
            # ä¿å­˜æ¨¡å‹
            save_path = os.path.join(config['save_dir'], 'best_contrastive_model.pth')
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'best_accuracy': best_val_accuracy,
                'config': config
            }, save_path)
            
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {save_path} (å‡†ç¡®ç‡: {best_val_accuracy:.4f})")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = os.path.join(config['save_dir'], 'training_history.json')
    with open(history_path, 'w', encoding='utf-8') as f:
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        history_to_save = []
        for epoch_data in training_history:
            epoch_copy = epoch_data.copy()
            # ç§»é™¤æ— æ³•JSONåºåˆ—åŒ–çš„é¡¹ç›®
            if 'confusion_matrix' in epoch_copy['val']:
                epoch_copy['val']['confusion_matrix'] = epoch_copy['val']['confusion_matrix'].tolist()
            history_to_save.append(epoch_copy)
        
        json.dump(history_to_save, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ˆ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {config['save_dir']}")
    print(f"ğŸ“Š è®­ç»ƒå†å²ä¿å­˜: {history_path}")


if __name__ == "__main__":
    main() 