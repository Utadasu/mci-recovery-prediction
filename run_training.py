#!/usr/bin/env python3
"""
ğŸš€ ä¸€é”®å¯åŠ¨è„šæœ¬ - æ™ºèƒ½ä¸‹é‡‡æ · + å¯¹æ¯”å­¦ä¹ è®­ç»ƒ
==============================================

åŠŸèƒ½ç‰¹æ€§:
- ğŸ”¥ æ™ºèƒ½ä¸‹é‡‡æ ·å±‚ImprovedResNetCBAM3Dæ¨¡å‹è®­ç»ƒ
- ğŸ¯ å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ è®­ç»ƒ
- ğŸ“Š è‡ªåŠ¨æ¨¡å‹è·¯å¾„ç®¡ç†
- ğŸ”§ GPUå†…å­˜è‡ªé€‚åº”é…ç½®
"""

import os
import sys
import torch
import argparse
from datetime import datetime

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
        print(f"âœ… GPU: {gpu_name} ({gpu_memory}GB)")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        return False
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_paths = [
        "/root/autodl-tmp/DATA_MCI/test_data/",
        "./test_data/",
        "../test_data/"
    ]
    
    data_path = None
    for path in data_paths:
        if os.path.exists(path):
            data_path = path
            break
    
    if data_path:
        print(f"âœ… æ•°æ®è·¯å¾„: {data_path}")
    else:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®è·¯å¾„")
        return False
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    os.makedirs('./models', exist_ok=True)
    print("âœ… æ¨¡å‹ç›®å½•å·²å‡†å¤‡")
    
    return True, data_path

def train_smart_downsample_model(data_path, device='cuda'):
    """è®­ç»ƒæ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹"""
    print("\nğŸ”¥ æ­¥éª¤1: è®­ç»ƒæ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹")
    print("=" * 50)
    
    from train_smart_downsample import SmartDownsampleTrainer
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SmartDownsampleTrainer(device=device, save_dir='./models')
    
    # è®­ç»ƒæ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹
    model, best_acc, model_path = trainer.train(
        data_path=data_path,
        use_global_pool=False,  # ä½¿ç”¨æ™ºèƒ½ä¸‹é‡‡æ ·
        base_channels=12,
        num_epochs=30,  # é€‚ä¸­çš„è®­ç»ƒè½®æ•°
        batch_size=4,
        learning_rate=1e-4,
        max_samples=None,
        patience=10
    )
    
    print(f"\nâœ… æ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    
    return model_path, best_acc

def train_contrastive_model(image_model_path, data_path, device='cuda'):
    """è®­ç»ƒå¯¹æ¯”å­¦ä¹ æ¨¡å‹"""
    print("\nğŸ¯ æ­¥éª¤2: è®­ç»ƒå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹")
    print("=" * 50)
    
    from contrastive_learning import create_contrastive_model
    from data_utils import create_multimodal_dataset_from_excel
    from torch.utils.data import DataLoader, TensorDataset
    import numpy as np
    
    # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨å¯¹æ¯”å­¦ä¹ ä¸“ç”¨å›¾åƒç¼–ç å™¨
    contrastive_image_paths = [
        './models/contrastive/contrastive_image_encoder_ch12.pth',
        './models/contrastive/contrastive_image_encoder_ch8.pth',
        image_model_path  # å›é€€åˆ°ä¼ å…¥çš„è·¯å¾„
    ]
    
    final_image_model_path = None
    for path in contrastive_image_paths:
        if path and os.path.exists(path):
            final_image_model_path = path
            print(f"âœ… æ‰¾åˆ°å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨: {path}")
            break
    
    if not final_image_model_path:
        print(f"âš ï¸  æœªæ‰¾åˆ°å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
        final_image_model_path = None
    
    # åˆ›å»ºå¯¹æ¯”å­¦ä¹ æ¨¡å‹
    model = create_contrastive_model(
        image_model_path=final_image_model_path,
        text_model_path=None,  # æš‚ä¸ä½¿ç”¨æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹
        device=device,
        freeze_backbones=False  # ä¸å†»ç»“ï¼Œå…è®¸ç«¯åˆ°ç«¯è®­ç»ƒ
    )
    
    # åŠ è½½çœŸå®çš„å¤šæ¨¡æ€æ•°æ®ï¼ˆä»Excelæ–‡ä»¶ï¼‰
    print("ğŸ“Š åŠ è½½çœŸå®å¤šæ¨¡æ€æ•°æ®ï¼ˆä»Excelæ–‡ä»¶ï¼‰...")
    try:
        image_data, texts, labels = create_multimodal_dataset_from_excel(
            image_data_dir=data_path,
            text_data_dir="./æ–‡æœ¬ç¼–ç å™¨",
            max_samples=100  # é™åˆ¶æ ·æœ¬æ•°ç”¨äºæµ‹è¯•
        )
    except Exception as e:
        print(f"âŒ çœŸå®æ–‡æœ¬æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("ğŸ”§ å›é€€åˆ°è™šæ‹Ÿæ–‡æœ¬æ•°æ®...")
        
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨è™šæ‹Ÿæ–‡æœ¬æ•°æ®
        from data_utils import load_early_fusion_data
        image_data, labels = load_early_fusion_data(data_path, max_samples=100)
        
        # åˆ›å»ºè™šæ‹Ÿæ–‡æœ¬æ•°æ®
        print("ğŸ“ åˆ›å»ºè™šæ‹Ÿæ–‡æœ¬æ•°æ®...")
        batch_size = 4
        seq_length = 128
        vocab_size = 30522  # BERTè¯æ±‡è¡¨å¤§å°
        
        num_samples = len(labels)
        input_ids = torch.randint(0, vocab_size, (num_samples, seq_length))
        attention_mask = torch.ones(num_samples, seq_length)
        
        # åˆ›å»ºæ•°æ®é›†
        image_tensor = torch.FloatTensor(image_data)
        label_tensor = torch.LongTensor(labels)
        
        dataset = TensorDataset(image_tensor, input_ids, attention_mask, label_tensor)
        texts = None  # æ ‡è®°ä¸ºè™šæ‹Ÿæ•°æ®
    
    # å¦‚æœä½¿ç”¨çœŸå®æ–‡æœ¬æ•°æ®ï¼Œéœ€è¦è¿›è¡Œæ–‡æœ¬ç¼–ç 
    if texts is not None:
        print("ğŸ“ ç¼–ç çœŸå®æ–‡æœ¬æ•°æ®...")
        
        # è·å–æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œç¼–ç 
        text_encoder = model.text_encoder
        
        # æ‰¹é‡ç¼–ç æ–‡æœ¬
        batch_size = 8  # æ–‡æœ¬ç¼–ç æ‰¹æ¬¡å¤§å°
        all_input_ids = []
        all_attention_masks = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            input_ids, attention_mask = text_encoder.encode_text(batch_texts, max_length=512)
            all_input_ids.append(input_ids)
            all_attention_masks.append(attention_mask)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        input_ids = torch.cat(all_input_ids, dim=0)
        attention_mask = torch.cat(all_attention_masks, dim=0)
        
        print(f"âœ… æ–‡æœ¬ç¼–ç å®Œæˆ: {input_ids.shape}")
        
        # åˆ›å»ºæ•°æ®é›†
        image_tensor = torch.FloatTensor(image_data)
        label_tensor = torch.LongTensor(labels)
        
        dataset = TensorDataset(image_tensor, input_ids, attention_mask, label_tensor)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    batch_size = 4  # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"ğŸ“ˆ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"ğŸ“ˆ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    if texts is not None:
        print(f"ğŸ“ ä½¿ç”¨çœŸå®æ–‡æœ¬æ•°æ®è®­ç»ƒ")
    else:
        print(f"ğŸ“ ä½¿ç”¨è™šæ‹Ÿæ–‡æœ¬æ•°æ®è®­ç»ƒ")
    
    # ç®€å•è®­ç»ƒå¾ªç¯
    import torch.optim as optim
    from tqdm import tqdm
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    criterion = torch.nn.CrossEntropyLoss()
    
    num_epochs = 5  # ç®€çŸ­è®­ç»ƒç”¨äºæµ‹è¯•
    best_val_acc = 0.0
    
    print(f"ğŸš€ å¼€å§‹å¯¹æ¯”å­¦ä¹ è®­ç»ƒ ({num_epochs} è½®)...")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):
            images = images.to(device)
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            
            # å¯¹æ¯”å­¦ä¹  + åˆ†ç±»
            contrastive_output = model(images, input_ids, attention_mask, mode='contrastive')
            classification_output = model(images, input_ids, attention_mask, mode='classification')
            
            # ç»„åˆæŸå¤±
            contrastive_loss = contrastive_output['contrastive_loss']
            classification_loss = criterion(classification_output['logits'], labels)
            
            total_loss = 0.5 * contrastive_loss + 1.0 * classification_loss
            
            total_loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += total_loss.item()
            _, predicted = classification_output['logits'].max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, input_ids, attention_mask, labels in val_loader:
                images = images.to(device)
                input_ids = input_ids.to(device)
                attention_mask = attention_mask.to(device)
                labels = labels.to(device)
                
                output = model(images, input_ids, attention_mask, mode='classification')
                _, predicted = output['logits'].max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f"Epoch {epoch+1}/{num_epochs}:")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'using_real_text': texts is not None,
            }, './models/best_contrastive_model.pth')
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {val_acc:.2f}%")
    
    print(f"\nâœ… å¯¹æ¯”å­¦ä¹ è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    if texts is not None:
        print(f"   âœ… ä½¿ç”¨äº†çœŸå®æ–‡æœ¬æ•°æ®")
    else:
        print(f"   âš ï¸  ä½¿ç”¨äº†è™šæ‹Ÿæ–‡æœ¬æ•°æ®")
    
    return best_val_acc

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ™ºèƒ½ä¸‹é‡‡æ · + å¯¹æ¯”å­¦ä¹ è®­ç»ƒ')
    parser.add_argument('--skip-downsample', action='store_true', 
                       help='è·³è¿‡æ™ºèƒ½ä¸‹é‡‡æ ·è®­ç»ƒï¼Œç›´æ¥è¿›è¡Œå¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--train-contrastive-encoder', action='store_true',
                       help='è®­ç»ƒå¯¹æ¯”å­¦ä¹ ä¸“ç”¨å›¾åƒç¼–ç å™¨')
    parser.add_argument('--data-path', type=str, default=None,
                       help='æ•°æ®è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ æ™ºèƒ½ä¸‹é‡‡æ · + å¯¹æ¯”å­¦ä¹ è®­ç»ƒå¯åŠ¨")
    print("=" * 60)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ£€æŸ¥ç¯å¢ƒ
    env_check = check_environment()
    if isinstance(env_check, tuple):
        success, data_path = env_check
        if not success:
            print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥")
            return
    else:
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥")
        return
    
    # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ•°æ®è·¯å¾„
    if args.data_path:
        data_path = args.data_path
        print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ•°æ®è·¯å¾„: {data_path}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ğŸ”¥ æ–°å¢é€‰é¡¹ï¼šè®­ç»ƒå¯¹æ¯”å­¦ä¹ ä¸“ç”¨å›¾åƒç¼–ç å™¨
    if args.train_contrastive_encoder:
        print("\nğŸ”¥ è®­ç»ƒå¯¹æ¯”å­¦ä¹ ä¸“ç”¨å›¾åƒç¼–ç å™¨")
        print("=" * 50)
        
        try:
            from train_image_encoder_for_contrastive import ContrastiveImageEncoderTrainer
            
            trainer = ContrastiveImageEncoderTrainer(device=device)
            
            # ä½¿ç”¨æ ‡å‡†é…ç½®è®­ç»ƒ
            model, best_acc, model_path = trainer.train(
                data_path=data_path,
                base_channels=12,
                num_epochs=50,
                batch_size=4,
                learning_rate=1e-4,
                max_samples=None,
                patience=15
            )
            
            print(f"\nâœ… å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨è®­ç»ƒå®Œæˆ!")
            print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
            print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
            
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            trainer.save_training_plots(12)
            
            # ç»§ç»­è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ
            image_model_path = model_path
            
        except Exception as e:
            print(f"âŒ å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨è®­ç»ƒå¤±è´¥: {e}")
            print("ğŸ”§ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ")
            image_model_path = None
    else:
        # æ­¥éª¤1: è®­ç»ƒæ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹
        if not args.skip_downsample:
            try:
                image_model_path, downsample_acc = train_smart_downsample_model(data_path, device)
            except Exception as e:
                print(f"âŒ æ™ºèƒ½ä¸‹é‡‡æ ·è®­ç»ƒå¤±è´¥: {e}")
                print("ğŸ”§ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ")
                image_model_path = None
                downsample_acc = 0.0
        else:
            print("â­ï¸  è·³è¿‡æ™ºèƒ½ä¸‹é‡‡æ ·è®­ç»ƒ")
            image_model_path = './models/smart_downsample_spatial_ch12.pth'
            downsample_acc = 0.0
    
    # æ­¥éª¤2: è®­ç»ƒå¯¹æ¯”å­¦ä¹ æ¨¡å‹
    try:
        contrastive_acc = train_contrastive_model(image_model_path, data_path, device)
    except Exception as e:
        print(f"âŒ å¯¹æ¯”å­¦ä¹ è®­ç»ƒå¤±è´¥: {e}")
        contrastive_acc = 0.0
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆæ€»ç»“")
    print("=" * 60)
    
    if args.train_contrastive_encoder:
        print(f"ğŸ”¥ å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨: å·²è®­ç»ƒå¹¶ä¿å­˜åˆ° ./models/contrastive/")
    elif not args.skip_downsample:
        print(f"ğŸ”¥ æ™ºèƒ½ä¸‹é‡‡æ ·æ¨¡å‹: {downsample_acc:.2f}%")
    
    print(f"ğŸ¯ å¯¹æ¯”å­¦ä¹ æ¨¡å‹: {contrastive_acc:.2f}%")
    
    print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•:")
    if args.train_contrastive_encoder:
        print(f"   å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨: ./models/contrastive/")
    print(f"   å¯¹æ¯”å­¦ä¹ æ¨¡å‹: ./models/")
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\nğŸ“ åç»­æ­¥éª¤:")
    if args.train_contrastive_encoder:
        print("1. âœ… å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨å·²è®­ç»ƒå®Œæˆ")
        print("2. ğŸ”— å¯ç›´æ¥ç”¨äºå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ")
    else:
        print("1. ğŸ”¥ å¯é€‰æ‹©è®­ç»ƒä¸“ç”¨å¯¹æ¯”å­¦ä¹ å›¾åƒç¼–ç å™¨:")
        print("   python run_training.py --train-contrastive-encoder")
    print("2. ç­‰å¾…é˜Ÿå‹æä¾›æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹")
    print("3. ä½¿ç”¨çœŸå®æ–‡æœ¬æ•°æ®é‡æ–°è®­ç»ƒå¯¹æ¯”å­¦ä¹ æ¨¡å‹")
    print("4. è¿›è¡Œå®Œæ•´çš„å¤šæ¨¡æ€èåˆè¯„ä¼°")

if __name__ == "__main__":
    main() 