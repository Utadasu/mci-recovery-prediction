#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¾®è°ƒè„šæœ¬ - åŸºäºå·²è®­ç»ƒæ¨¡å‹è¿›è¡Œå¿«é€Ÿæ€§èƒ½æå‡
é€‚ç”¨äºæ—¶é—´ç´§å¼ çš„æƒ…å†µ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import os

def quick_finetune_model(model_path, data_loaders, device, epochs=5):
    """
    å¿«é€Ÿå¾®è°ƒå·²è®­ç»ƒæ¨¡å‹
    
    å‚æ•°:
    - model_path: å·²è®­ç»ƒæ¨¡å‹è·¯å¾„
    - data_loaders: æ•°æ®åŠ è½½å™¨
    - device: è®¡ç®—è®¾å¤‡
    - epochs: å¾®è°ƒè½®æ¬¡(é»˜è®¤5è½®)
    
    è¿”å›:
    - å¾®è°ƒåçš„æ€§èƒ½
    """
    print(f"\n===== å¿«é€Ÿå¾®è°ƒæ¨¡å¼ (ä»…{epochs}è½®) =====")
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    from optimized_models import ImprovedResNetCBAM3D
    model = ImprovedResNetCBAM3D(
        in_channels=3,
        num_classes=2,
        base_channels=12,
        dropout_rate=0.3
    ).to(device)
    
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint)
    print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    
    # æ•°æ®åŠ è½½å™¨
    from early_fusion_fixed import create_memory_optimized_early_fusion_loaders
    fusion_loaders = create_memory_optimized_early_fusion_loaders(
        data_loaders, gpu_memory_gb=32, debug=False
    )
    
    train_loader = fusion_loaders['train']
    val_loader = fusion_loaders['val']
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
    optimizer = optim.AdamW(
        model.parameters(),
        lr=0.00001,  # æ¯”åˆå§‹è®­ç»ƒå°10å€
        weight_decay=0.001
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    best_val_acc = 0.0
    model.train()
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f"å¾®è°ƒ {epoch+1}/{epochs}")
        for batch_idx, (inputs, labels) in enumerate(train_pbar):
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            train_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{train_acc:.2f}%'
            })
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        val_acc = 100. * val_correct / val_total
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), './models/quick_finetuned_model.pth')
            print(f"âœ… ä¿å­˜å¾®è°ƒæ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        
        scheduler.step()
        model.train()
        
        print(f"è½®æ¬¡ [{epoch+1}/{epochs}] - è®­ç»ƒå‡†ç¡®ç‡: {100.*train_correct/train_total:.2f}%, "
              f"éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    
    print(f"\nğŸ‰ å¿«é€Ÿå¾®è°ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    return best_val_acc

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model_path = "./models/best_memory_optimized_early_fusion.pth"
    
    # è¿™é‡Œéœ€è¦ä¼ å…¥å®é™…çš„æ•°æ®åŠ è½½å™¨
    # best_acc = quick_finetune_model(model_path, data_loaders, device)
    print("å¿«é€Ÿå¾®è°ƒè„šæœ¬å°±ç»ªï¼") 