#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å—
======================
åŒ…å«ä»å¯¹æŠ—æ€§å­¦ä¹ è„šæœ¬ä¸­ç§»æ¤è¿‡æ¥çš„æ–‡æœ¬å¤„ç†æ¨¡å—ï¼Œ
ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾æå–ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import re
from transformers import AutoModel, AutoTokenizer

# ==============================================================================
# æ¨¡å— 1: è®¤çŸ¥è¯„ä¼°å¤„ç†å™¨
# ==============================================================================
class CognitiveAssessmentProcessor(nn.Module):
    """ğŸ”¥ è®¤çŸ¥è¯„ä¼°å¤„ç†å™¨ - å¤šå…ƒå›å½’æ ¡æ­£ + CDR-SBæ•´åˆ"""
    
    def __init__(self, device='cuda'):
        super(CognitiveAssessmentProcessor, self).__init__()
        
        self.device = device
        
        # ğŸ¯ MMSEå¤šå…ƒå›å½’æ ¡æ­£å‚æ•° (åŸºäºå¾ªè¯åŒ»å­¦ç ”ç©¶)
        self.mmse_regression_params = {
            'intercept': 29.1, 'age_coef': -0.045, 'age_squared_coef': -0.0003,
            'gender_coef': 0.1, 'education_coef': 0.35, 'education_squared_coef': -0.008
        }
        
        # ğŸ¯ CDR-SBåˆ†ç®±ç­–ç•¥
        self.cdrsb_bins = {
            'normal': [0, 0.5], 'questionable': [0.5, 2.5], 'mild': [2.5, 4.5],
            'moderate': [4.5, 9.0], 'severe': [9.0, 18.0]
        }
        
        # å®šä¹‰ç½‘ç»œå±‚
        self.mmse_encoder = nn.Sequential(nn.Linear(2, 32), nn.LayerNorm(32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32, 64))
        self.cdrsb_bin_embedding = nn.Embedding(5, 32)
        self.cdrsb_encoder = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 32))
        self.cognitive_fusion = nn.Sequential(
            nn.Linear(64 + 32 + 32, 128), nn.LayerNorm(128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 16)
        )
        
    def extract_demographic_info(self, texts):
        demographics = []
        for text in texts:
            age_match = re.search(r'age (\d+\.?\d*)', text)
            gender_match = re.search(r'gender (\d)', text)
            edu_match = re.search(r'education (\d+)', text)
            
            age = float(age_match.group(1)) if age_match else 70.0
            gender = int(gender_match.group(1)) if gender_match else 0
            education = int(edu_match.group(1)) if edu_match else 12
            demographics.append({'age': age, 'gender': gender, 'education': education})
        return demographics

    def extract_mmse_scores(self, texts):
        scores = [float(re.search(r'mmse_score (\d+\.?\d*)', t).group(1)) if re.search(r'mmse_score (\d+\.?\d*)', t) else 25.0 for t in texts]
        return torch.tensor(scores, device=self.device, dtype=torch.float32)

    def extract_cdrsb_scores(self, texts):
        scores = [float(re.search(r'cdrsb_score (\d+\.?\d*)', t).group(1)) if re.search(r'cdrsb_score (\d+\.?\d*)', t) else 1.0 for t in texts]
        return torch.tensor(scores, device=self.device, dtype=torch.float32)

    def compute_mmse_correction(self, demographics):
        p = self.mmse_regression_params
        corrections = []
        for d in demographics:
            correction = (p['age_coef'] * (d['age'] - 70) + p['age_squared_coef'] * ((d['age'] - 70)**2) +
                          p['gender_coef'] * d['gender'] +
                          p['education_coef'] * (d['education'] - 12) + p['education_squared_coef'] * ((d['education'] - 12)**2))
            corrections.append(correction)
        return torch.tensor(corrections, device=self.device, dtype=torch.float32)

    def get_cdrsb_bins(self, cdrsb_scores):
        bins = torch.zeros_like(cdrsb_scores, dtype=torch.long)
        for i, score in enumerate(cdrsb_scores):
            if self.cdrsb_bins['normal'][0] <= score < self.cdrsb_bins['normal'][1]: bins[i] = 0
            elif score < self.cdrsb_bins['questionable'][1]: bins[i] = 1
            elif score < self.cdrsb_bins['mild'][1]: bins[i] = 2
            elif score < self.cdrsb_bins['moderate'][1]: bins[i] = 3
            else: bins[i] = 4
        return bins

    def forward(self, texts):
        demographics = self.extract_demographic_info(texts)
        mmse_scores = self.extract_mmse_scores(texts)
        cdrsb_scores = self.extract_cdrsb_scores(texts)
        
        mmse_corrections = self.compute_mmse_correction(demographics)
        corrected_mmse = mmse_scores - mmse_corrections
        
        mmse_input = torch.stack([(mmse_scores - 15.0) / 15.0, (corrected_mmse - 15.0) / 15.0], dim=1)
        mmse_features = self.mmse_encoder(mmse_input)
        
        cdrsb_bins = self.get_cdrsb_bins(cdrsb_scores)
        cdrsb_bin_features = self.cdrsb_bin_embedding(cdrsb_bins)
        cdrsb_continuous_features = self.cdrsb_encoder( ((cdrsb_scores - 4.5) / 4.5).unsqueeze(1) )
        
        combined_features = torch.cat([mmse_features, cdrsb_bin_features, cdrsb_continuous_features], dim=1)
        return self.cognitive_fusion(combined_features)


# ==============================================================================
# æ¨¡å— 2: å¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç å™¨
# ==============================================================================
class AdversarialTextEncoder(nn.Module):
    """ğŸ”¥ å¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç å™¨ - é€‚é…ä¸‹æ¸¸ä»»åŠ¡"""
    def __init__(self, feature_dim=512, device='cuda', max_length=512, use_cognitive_features=True, bert_model_path='../models/bert-base-uncased-local'):
        super(AdversarialTextEncoder, self).__init__()
        
        self.device = device
        self.max_length = max_length
        self.feature_dim = feature_dim
        self.use_cognitive_features = use_cognitive_features
        
        print("ğŸ”§ åˆå§‹åŒ–å¯¹æŠ—æ€§æ–‡æœ¬ç¼–ç å™¨...")
        # 1. BERTæ¨¡å‹å’Œåˆ†è¯å™¨
        try:
            print(f"   å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½BERT: {bert_model_path}")
            self.bert_model = AutoModel.from_pretrained(bert_model_path)
            self.bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_path)
            print("   âœ… æœ¬åœ°BERTåŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            print(f"   âš ï¸ æœ¬åœ°åŠ è½½å¤±è´¥: {e}ã€‚å›é€€åˆ°åœ¨çº¿ä¸‹è½½'bert-base-uncased'...")
            self.bert_model = AutoModel.from_pretrained('bert-base-uncased')
            self.bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            print("   âœ… åœ¨çº¿BERTåŠ è½½æˆåŠŸã€‚")

        # 2. è®¤çŸ¥è¯„ä¼°å¤„ç†å™¨
        self.mmse_processor = CognitiveAssessmentProcessor(device=device)
        
        # 3. BERTä¸è®¤çŸ¥ç‰¹å¾èåˆå±‚
        fusion_input_dim = 768 + 16 if self.use_cognitive_features else 768
        self.bert_mmse_fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, 1024), nn.LayerNorm(1024), nn.GELU(),
            nn.Dropout(0.1), nn.Linear(1024, 768)
        )
        
        # 4. æœ€ç»ˆæŠ•å½±å±‚
        self.final_projection = nn.Sequential(
            nn.Linear(768, 1024), nn.LayerNorm(1024), nn.GELU(), nn.Dropout(0.1),
            nn.Linear(1024, feature_dim)
        )
        
    def forward(self, texts):
        # BERTç¼–ç 
        inputs = self.bert_tokenizer(
            texts, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length
        ).to(self.device)
        bert_features = self.bert_model(**inputs)[0][:, 0, :]

        if self.use_cognitive_features:
            # ç¡®ä¿åœ¨æ‰¹å¤„ç†å¤§å°ä¸º1æ—¶ï¼Œä¹Ÿèƒ½æ­£ç¡®å¤„ç†
            if bert_features.size(0) == 1 and len(texts) > 0:
                 cognitive_features = self.mmse_processor(texts)
                 if cognitive_features.size(0) > 1:
                     cognitive_features = cognitive_features[0].unsqueeze(0)
            else:
                 cognitive_features = self.mmse_processor(texts)
                 
            features_to_fuse = torch.cat([bert_features, cognitive_features], dim=1)
            fused_features = self.bert_mmse_fusion(features_to_fuse)
        else:
            fused_features = bert_features
        
        projected_features = self.final_projection(fused_features)
        return F.normalize(projected_features, p=2, dim=1) 