 
import os
import numpy as np
import torch
import warnings
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import xgboost as xgb
import json

# å¯¼å…¥æˆ‘ä»¬é¡¹ç›®ä¸­çš„æ¨¡å—
from mci_conversion_prediction import MCIDataLoader, FeatureExtractor, set_seed, RECOMMENDED_CONFIG, RANDOM_SEED

# å°è¯•å¯¼å…¥ONNXç›¸å…³åº“ï¼Œå¦‚æœå¤±è´¥åˆ™æç¤ºç”¨æˆ·å®‰è£…
try:
    import skl2onnx
    from skl2onnx import convert_sklearn
    from skl2onnx.common.data_types import FloatTensorType
    import onnxmltools
except ImportError:
    print("âŒ é”™è¯¯: 'skl2onnx', 'onnxruntime', or 'onnxmltools' æœªå®‰è£…ã€‚")
    print("ğŸ”§ è¯·è¿è¡Œ: pip install skl2onnx onnxruntime onnxmltools")
    exit()

warnings.filterwarnings('ignore')

def main():
    """
    ä¸»å‡½æ•°: è®­ç»ƒå¹¶å¯¼å‡ºæœ€ç»ˆçš„XGBooståˆ†ç±»å™¨ä¸ºONNXæ ¼å¼
    """
    print("ğŸš€ å¼€å§‹æœ€ç»ˆæ¨¡å‹è®­ç»ƒä¸å¯¼å‡ºä»»åŠ¡...")
    
    # ğŸ¯ å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    set_seed(RANDOM_SEED)
    
    # --- 1. åŠ è½½å¹¶å‡†å¤‡å…¨é‡æ•°æ® ---
    print("\n" + "="*20 + " æ­¥éª¤ 1: åŠ è½½å…¨é‡æ•°æ® " + "="*20)
    data_dir = '/root/autodl-tmp/DATA_MCI'
    if not os.path.exists(data_dir):
        print(f"âš ï¸ è­¦å‘Š: æœåŠ¡å™¨æ•°æ®è·¯å¾„ '{data_dir}' åœ¨å½“å‰ç¯å¢ƒä¸å¯ç”¨ã€‚")
        print("   å°†å°è¯•åœ¨æœ¬åœ° './' ç›®å½•å¯»æ‰¾æ›¿ä»£æ•°æ®...")
        # åœ¨æœ¬åœ°å¼€å‘æ—¶ï¼Œå¯ä»¥å°†MCIæ•°æ®æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ 'DATA_MCI' æ–‡ä»¶å¤¹ä¸­
        local_data_dir = './DATA_MCI'
        if os.path.exists(local_data_dir):
             data_dir = local_data_dir
        else:
            print(f"âŒ é”™è¯¯: æœªèƒ½åœ¨ '{data_dir}' æˆ– '{local_data_dir}' æ‰¾åˆ°æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

    data_loader = MCIDataLoader(data_dir=data_dir)
    images, image_labels, image_patient_ids = data_loader.load_mci_images()
    texts, text_labels, text_patient_ids = data_loader.load_mci_text_data()

    if len(images) == 0:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•å›¾åƒæ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    images, texts, labels, _ = data_loader.align_image_text_data(
        images, texts, image_labels, text_labels, image_patient_ids, text_patient_ids
    )

    if len(images) == 0:
        print("âŒ é”™è¯¯: æ•°æ®å¯¹é½åæ— å¯ç”¨æ ·æœ¬ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(images)} ä¸ªå¯¹é½æ ·æœ¬ã€‚")

    # --- 2. æå–å¹¶èåˆç‰¹å¾ ---
    print("\n" + "="*20 + " æ­¥éª¤ 2: æå–å¹¶èåˆç‰¹å¾ " + "="*20)
    # åŠ¨æ€è·å–æœ€ä½³æ¨¡å‹è·¯å¾„
    from mci_conversion_prediction import EnhancedMCIClassifier
    best_model_path = EnhancedMCIClassifier(RECOMMENDED_CONFIG)._get_best_pretrained_model_path()
    
    feature_extractor = FeatureExtractor(
        model_path=best_model_path,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        batch_size=16
    )

    all_image_features = feature_extractor.extract_image_features(images)
    all_text_features = feature_extractor.extract_text_features(texts)

    # ä½¿ç”¨æœ€ä½³æƒé‡èåˆç‰¹å¾
    weight = RECOMMENDED_CONFIG.get('image_feature_weight', 0.8)
    fused_features = weight * all_image_features + (1 - weight) * all_text_features
    print(f"âœ… ç‰¹å¾æå–ä¸èåˆå®Œæˆï¼Œç‰¹å¾ç»´åº¦: {fused_features.shape}")

    # --- 3. è®­ç»ƒæœ€ç»ˆçš„åˆ†ç±»å™¨ ---
    print("\n" + "="*20 + " æ­¥éª¤ 3: è®­ç»ƒæœ€ç»ˆåˆ†ç±»å™¨ " + "="*20)
    
    # ğŸ”¥ æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨skl2onnxåŸç”Ÿæ”¯æŒçš„LogisticRegressionæ›¿æ¢XGBoostï¼Œé¿å…è½¬æ¢é”™è¯¯
    from sklearn.linear_model import LogisticRegression

    # ä½¿ç”¨ä¸€ä¸ªä¸æˆ‘ä»¬ä¹‹å‰è°ƒä¼˜ç›¸ä¼¼çš„æ­£åˆ™åŒ–å¼ºåº¦
    # æ³¨æ„: LogisticRegressionçš„Cæ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°
    regularization_strength = RECOMMENDED_CONFIG.get('regularization_strength', 10.0)
    final_classifier = LogisticRegression(
        C=1.0/regularization_strength,
        max_iter=RECOMMENDED_CONFIG.get('max_iter', 6000),
        solver='liblinear',
        random_state=RANDOM_SEED
    )

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ ‡å‡†åŒ–å’Œåˆ†ç±»çš„Pipeline
    pipeline = Pipeline(steps=[
        ('scaler', StandardScaler()),
        ('classifier', final_classifier)
    ])

    # åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒPipeline
    pipeline.fit(fused_features, labels)
    print("âœ… æœ€ç»ˆåˆ†ç±»å™¨Pipelineè®­ç»ƒå®Œæˆã€‚")

    # --- 4. æå–ç»„ä»¶å¹¶ä¿å­˜Scalerå‚æ•° ---
    print("\n" + "="*20 + " æ­¥éª¤ 4: æå–ç»„ä»¶å¹¶ä¿å­˜Scaler " + "="*20)

    # ä»è®­ç»ƒå¥½çš„Pipelineä¸­æå–scalerå’Œclassifier
    fitted_scaler = pipeline.named_steps['scaler']
    fitted_classifier = pipeline.named_steps['classifier']

    # ä¿å­˜scalerçš„å‚æ•° (mean and scale) åˆ°ä¸€ä¸ªJSONæ–‡ä»¶
    scaler_params = {
        'mean': fitted_scaler.mean_.tolist(),
        'scale': fitted_scaler.scale_.tolist()
    }
    scaler_filename = "scaler_params.json"
    with open(scaler_filename, 'w') as f:
        json.dump(scaler_params, f, indent=4)
    print(f"âœ… Scalerå‚æ•°å·²ä¿å­˜åˆ°: {scaler_filename}")


    # --- 5. è½¬æ¢ä¸ºONNXæ ¼å¼å¹¶ä¿å­˜ ---
    print("\n" + "="*20 + " æ­¥éª¤ 5: è½¬æ¢åˆ†ç±»å™¨ä¸ºONNXæ ¼å¼ " + "="*20)
    
    # å®šä¹‰ONNXæ¨¡å‹çš„è¾“å…¥æ ¼å¼
    # [None, 512] è¡¨ç¤ºå¯ä»¥æ¥å—ä»»æ„æ•°é‡çš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯512ç»´çš„å‘é‡
    initial_type = [('float_input', FloatTensorType([None, fused_features.shape[1]]))]
    
    # è¿›è¡Œè½¬æ¢ - è¿™æ¬¡åªè½¬æ¢åˆ†ç±»å™¨
    try:
        # ç›®æ ‡opsetæ˜¯è§£å†³æŸäº›è½¬æ¢å™¨é—®é¢˜çš„å¸¸ç”¨æ–¹æ³•
        target_opset = 12
        onnx_model = convert_sklearn(
            fitted_classifier, 
            initial_types=initial_type, 
            target_opset={'': target_opset}
        )
        
        # ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
        onnx_filename = "mci_classifier.onnx"
        with open(onnx_filename, "wb") as f:
            f.write(onnx_model.SerializeToString())
            
        print(f"ğŸ‰ æˆåŠŸï¼æ¨¡å‹å·²å¯¼å‡ºä¸º: {onnx_filename}")
        print("   è¿™ä¸ªæ–‡ä»¶ç°åœ¨å¯ä»¥ç”¨äºå‰ç«¯ç½‘é¡µéƒ¨ç½²äº†ã€‚")

    except Exception as e:
        print(f"âŒ å¯¼å‡ºONNXæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 