import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

# ç¨³å®šçš„BatchNormå®ç°
class StableBatchNorm3d(nn.BatchNorm3d):
    def __init__(self, num_features, eps=1e-5, momentum=0.05):
        super(StableBatchNorm3d, self).__init__(
            num_features, eps=eps, momentum=momentum
        )
    
    def forward(self, input):
        self._check_input_dim(input)
        
        # åº”ç”¨æ›´ç¨³å®šçš„è®¡ç®—
        if self.training:
            # è®¡ç®—å‡å€¼å’Œæ–¹å·®æ—¶å¢åŠ epsï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
            mean = input.mean(dim=[0, 2, 3, 4], keepdim=True)
            var = input.var(dim=[0, 2, 3, 4], unbiased=False, keepdim=True) + self.eps
            
            # ä½¿ç”¨ç´¯ç§¯ç§»åŠ¨å¹³å‡
            if self.track_running_stats:
                with torch.no_grad():
                    self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze()
                    self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.squeeze()
            
            # å½’ä¸€åŒ–
            normalized = (input - mean) / torch.sqrt(var)
            return self.weight.view(1, -1, 1, 1, 1) * normalized + self.bias.view(1, -1, 1, 1, 1)
        else:
            # æµ‹è¯•é˜¶æ®µï¼Œä½¿ç”¨ç´¯ç§¯ç»Ÿè®¡é‡
            mean = self.running_mean.view(1, -1, 1, 1, 1)
            var = self.running_var.view(1, -1, 1, 1, 1) + self.eps
            normalized = (input - mean) / torch.sqrt(var)
            return self.weight.view(1, -1, 1, 1, 1) * normalized + self.bias.view(1, -1, 1, 1, 1)



# ä¼˜åŒ–çš„ResNetCBAM3Dæ¨¡å‹ - çœŸæ­£çš„CBAMå®ç°
class ImprovedResNetCBAM3D(nn.Module):
    def __init__(self, in_channels=3, num_classes=2, base_channels=12, dropout_rate=0.3, use_global_pool=True, use_cbam=True):
        super(ImprovedResNetCBAM3D, self).__init__()
        
        # ä¿å­˜è¾“å…¥é€šé“æ•°ä½œä¸ºç±»å±æ€§
        self.in_channels = in_channels
        self.use_global_pool = use_global_pool
        self.use_cbam = use_cbam
        print(f"   æ¨¡å‹é…ç½®: ä½¿ç”¨ CBAM -> {'âœ…' if self.use_cbam else 'âŒ'}")
        
        # æ”¹è¿›çš„åˆå§‹å±‚è®¾è®¡ - æ›´é€‚åˆ3D MRIæ•°æ®
        self.stem = nn.Sequential(
            # ç¬¬ä¸€æ­¥ï¼šå¤§æ ¸å·ç§¯æ•è·æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
            nn.Conv3d(in_channels, base_channels//2, kernel_size=7, stride=1, padding=3, bias=False),
            StableBatchNorm3d(base_channels//2),
            nn.ReLU(inplace=False),
            
            # ç¬¬äºŒæ­¥ï¼šç»†åŒ–ç‰¹å¾å¹¶å¼€å§‹ä¸‹é‡‡æ ·
            nn.Conv3d(base_channels//2, base_channels, kernel_size=3, stride=1, padding=1, bias=False),
            StableBatchNorm3d(base_channels),
            nn.ReLU(inplace=False),
            
            # ç¬¬ä¸‰æ­¥ï¼šæŠ—æ··å ä¸‹é‡‡æ ·
            AntiAliasDownsample3D(base_channels, stride=2, kernel_size=3)
        )
        
        # ä½¿ç”¨çœŸæ­£CBAMæ³¨æ„åŠ›çš„æ®‹å·®å±‚
        self.layer1 = self._make_layer(base_channels, base_channels*2, 3, stride=1, 
                                     dropout_rate=dropout_rate, stochastic_depth_prob=0.0)
        self.layer2 = self._make_layer(base_channels*2, base_channels*4, 4, stride=2, 
                                     dropout_rate=dropout_rate, stochastic_depth_prob=0.1)
        self.layer3 = self._make_layer(base_channels*4, base_channels*8, 3, stride=2, 
                                     dropout_rate=dropout_rate, stochastic_depth_prob=0.2)
        
        # æ–°å¢ï¼šé¢å¤–çš„ä¸‹é‡‡æ ·å±‚ (layer4) - é€æ¸å¢åŠ éšæœºæ·±åº¦æ¦‚ç‡
        self.layer4 = self._make_layer(base_channels*8, base_channels*16, 2, stride=2, 
                                     dropout_rate=dropout_rate, stochastic_depth_prob=0.3)
        
        if use_global_pool:
            # åŸå§‹æ–¹æ¡ˆï¼šå…¨å±€å¹³å‡æ± åŒ–
            self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
            final_feature_dim = base_channels*16
        else:
            # æ–°æ–¹æ¡ˆï¼šLayer5æ™ºèƒ½ä¸‹é‡‡æ ·å±‚
            self.layer5 = self._make_advanced_downsample_layer(
                base_channels*16, base_channels*16, 
                target_size=(2, 2, 2),  # ç›®æ ‡ç©ºé—´å°ºå¯¸ [2,2,2]
                dropout_rate=dropout_rate
            )
            # æœ€ç»ˆç‰¹å¾ç»´åº¦ = é€šé“æ•° Ã— ç©ºé—´å°ºå¯¸
            final_feature_dim = base_channels*16 * 2 * 2 * 2  # 192 * 8 = 1536ç»´
        
        # ç‰¹å¾èåˆå’Œåˆ†ç±» - æ›´é²æ£’çš„è®¾è®¡ï¼Œé¿å…batch_size=1æ—¶BatchNormé—®é¢˜
        self.fusion = nn.Sequential(
            nn.Linear(final_feature_dim, 512),
            nn.LayerNorm(512),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNorm1dï¼Œé¿å…batch_size=1é—®é¢˜
            nn.ReLU(inplace=False),
            nn.Dropout(dropout_rate),
            
            nn.Linear(512, 256),
            nn.LayerNorm(256),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNorm1d
            nn.ReLU(inplace=False),
            nn.Dropout(dropout_rate)
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(256, num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _make_layer(self, in_channels, out_channels, blocks, stride, dropout_rate, stochastic_depth_prob):
        layers = []
        
        # é¦–ä¸ªå—å¤„ç†é€šé“å’Œå°ºå¯¸å˜åŒ–
        layers.append(EnhancedResidualBlock(
            in_channels, out_channels, stride, dropout_rate, 
            stochastic_depth_prob=stochastic_depth_prob,
            use_cbam=self.use_cbam
        ))
        
        # åç»­å—ä¿æŒé€šé“å’Œå°ºå¯¸ï¼Œé€æ¸å¢åŠ éšæœºæ·±åº¦æ¦‚ç‡
        for i in range(1, blocks):
            # åœ¨åŒä¸€å±‚å†…é€æ¸å¢åŠ éšæœºæ·±åº¦æ¦‚ç‡
            block_stochastic_prob = stochastic_depth_prob * (1 + i * 0.1)
            layers.append(EnhancedResidualBlock(
                out_channels, out_channels, 1, dropout_rate,
                stochastic_depth_prob=block_stochastic_prob,
                use_cbam=self.use_cbam
            ))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                # ä½¿ç”¨æ›´å¥½çš„åˆå§‹åŒ–æ–¹æ³•
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm3d, StableBatchNorm3d)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # ä½¿ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çº¿æ€§å±‚
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x, return_features=False):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, C, D, H, W]
        """
        # è¾“å…¥å½¢çŠ¶æ£€æŸ¥å’Œè°ƒæ•´
        if len(x.shape) == 6:  # å¤„ç†å½¢çŠ¶ä¸º [B, C, 1, D, H, W] çš„æƒ…å†µ
            print(f"æ£€æµ‹åˆ°è¾“å…¥å½¢çŠ¶æœ‰é¢å¤–ç»´åº¦: {x.shape}ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º5Då¼ é‡")
            x = x.squeeze(2)  # å»é™¤å¤šä½™çš„ç»´åº¦ï¼Œå˜ä¸º [B, C, D, H, W]
            
        if len(x.shape) != 5:
            raise ValueError(f"æœŸæœ›è¾“å…¥å½¢çŠ¶ä¸º[B, C, D, H, W]ï¼Œä½†å¾—åˆ°: {x.shape}")
            
        # æ£€æŸ¥é€šé“æ•°æ˜¯å¦åŒ¹é…
        if x.shape[1] != self.in_channels:
            print(f"âš ï¸ è­¦å‘Š: è¾“å…¥é€šé“æ•°({x.shape[1]})ä¸æ¨¡å‹æœŸæœ›é€šé“æ•°({self.in_channels})ä¸åŒ¹é…")
            # å¦‚æœé€šé“æ•°å¤šä½™ï¼Œæˆªå–å‰in_channelsä¸ªé€šé“
            if x.shape[1] > self.in_channels:
                print(f"æˆªå–å‰{self.in_channels}ä¸ªé€šé“")
                x = x[:, :self.in_channels, ...]
            # å¦‚æœé€šé“æ•°ä¸è¶³ï¼Œä½¿ç”¨å¤åˆ¶æ‰©å±•é€šé“æ•°
            else:
                repeat_times = math.ceil(self.in_channels / x.shape[1])
                print(f"é€šé“ä¸è¶³ï¼Œå¤åˆ¶{repeat_times}æ¬¡æ‰©å±•é€šé“")
                x = x.repeat(1, repeat_times, 1, 1, 1)[:, :self.in_channels, ...]
                
        # è¾“å…¥å½’ä¸€åŒ– - ä½¿ç”¨æ›´ç¨³å®šçš„å½’ä¸€åŒ–æ–¹å¼
        mean = x.mean(dim=[2, 3, 4], keepdim=True)
        std = x.std(dim=[2, 3, 4], keepdim=True) + 1e-6
        x = (x - mean) / std
        
        # ç‰¹å¾æå– - æ”¹è¿›çš„stem
        x = self.stem(x)
        
        # æ®‹å·®å±‚ - ä½¿ç”¨çœŸæ­£çš„CBAMæ³¨æ„åŠ›
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        
        # æ–°å¢ï¼šé¢å¤–çš„ä¸‹é‡‡æ ·å±‚
        features = self.layer4(x)
        
        if self.use_global_pool:
            # å…¨å±€æ± åŒ– - æ›¿ä»£ç‰¹å¾é‡‘å­—å¡”æ± åŒ–
            x = self.global_pool(features)
            x = x.view(x.size(0), -1)  # å°†ç‰¹å¾å‹å¹³
        else:
            # æ–°æ–¹æ¡ˆï¼šLayer5æ™ºèƒ½ä¸‹é‡‡æ ·å±‚
            x = self.layer5(features)
            # æ™ºèƒ½ä¸‹é‡‡æ ·åéœ€è¦flattenï¼š[B, C, 2, 2, 2] â†’ [B, C*2*2*2]
            x = x.view(x.size(0), -1)  # å°†ç‰¹å¾å‹å¹³
        
        # ç‰¹å¾èåˆ
        x = self.fusion(x)
        
        if return_features:
            return x
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        return logits

    def _make_advanced_downsample_layer(self, in_channels, out_channels, target_size=(2, 2, 2), dropout_rate=0.3):
        """
        ğŸ”¥ æ™ºèƒ½ä¸‹é‡‡æ ·å±‚ - æ›¿ä»£å…¨å±€æ± åŒ–çš„é«˜çº§æ–¹æ¡ˆ
        
        ç‰¹ç‚¹:
        - ğŸ¯ ä¿ç•™ç©ºé—´ä¿¡æ¯çš„åŒæ—¶è¿›è¡Œå°ºå¯¸å‹ç¼©
        - ğŸ”§ æ·±åº¦å¯åˆ†ç¦»å·ç§¯å‡å°‘å‚æ•°é‡
        - ğŸ’¡ é›†æˆæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºé‡è¦ç‰¹å¾
        - ğŸ“Š è‡ªé€‚åº”æ± åŒ–ï¼Œçµæ´»æ§åˆ¶è¾“å‡ºå°ºå¯¸
        - âš–ï¸ å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
        """
        return AdvancedDownsampleLayer(in_channels, out_channels, target_size, dropout_rate)

# å¢å¼ºçš„æ®‹å·®å— - ä½¿ç”¨çœŸæ­£çš„CBAMå’Œæ”¹è¿›çš„éšæœºæ·±åº¦
class EnhancedResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.2, 
                 groups=4, stochastic_depth_prob=0.0, use_cbam=True):
        super(EnhancedResidualBlock, self).__init__()
        
        # ç¡®ä¿é€šé“æ•°å¯è¢«åˆ†ç»„æ•°æ•´é™¤
        groups = min(groups, in_channels//2, out_channels//2)
        if in_channels % groups != 0 or out_channels % groups != 0:
            groups = 1
        
        # ä¸»å·ç§¯è·¯å¾„
        if stride > 1:
            # å¦‚æœæœ‰ä¸‹é‡‡æ ·ï¼Œä½¿ç”¨æŠ—æ··å ä¸‹é‡‡æ ·
            self.conv1 = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, 
                         padding=1, groups=groups, bias=False),
                StableBatchNorm3d(out_channels),
                nn.ReLU(inplace=False),
                AntiAliasDownsample3D(out_channels, stride=stride, kernel_size=3)
            )
        else:
            # æ™®é€šå·ç§¯
            self.conv1 = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, 
                         padding=1, groups=groups, bias=False),
                StableBatchNorm3d(out_channels),
                nn.ReLU(inplace=False)
            )
        
        # æ·»åŠ dropout
        self.dropout1 = nn.Dropout3d(dropout_rate)
        
        # ç¬¬äºŒä¸ªå·ç§¯
        self.conv2 = nn.Sequential(
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, 
                     groups=groups, bias=False),
            StableBatchNorm3d(out_channels)
        )
        
        # çœŸæ­£çš„CBAMæ³¨æ„åŠ›æœºåˆ¶ (å¯æ¶ˆè)
        if use_cbam:
            self.cbam = TrueCBAM3D(out_channels, reduction_ratio=8, spatial_kernel_size=7)
        else:
            self.cbam = nn.Identity()
        
        # Shortcutè¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            if stride > 1:
                # shortcutä¹Ÿä½¿ç”¨æŠ—æ··å ä¸‹é‡‡æ ·
                self.shortcut = nn.Sequential(
                    nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),
                    AntiAliasDownsample3D(out_channels, stride=stride, kernel_size=3),
                    StableBatchNorm3d(out_channels)
                )
            else:
                self.shortcut = nn.Sequential(
                    nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                    StableBatchNorm3d(out_channels)
                )
        
        # å¢å¼ºçš„éšæœºæ·±åº¦
        self.stochastic_depth = StochasticDepth(drop_prob=stochastic_depth_prob, mode='batch')
        
        self.relu = nn.ReLU(inplace=False)
    
    def forward(self, x):
        identity = self.shortcut(x)
        
        # ä¸»è·¯å¾„
        out = self.conv1(x)
        out = self.dropout1(out)
        out = self.conv2(out)
        
        # CBAMæ³¨æ„åŠ›
        out = self.cbam(out)
        
        # ä½¿ç”¨å¢å¼ºçš„éšæœºæ·±åº¦
        out = self.stochastic_depth(out, identity)
        out = self.relu(out)
        
        return out

# åˆ›å»ºæ”¹è¿›çš„ResNetCBAM3Dæ¨¡å‹
def create_improved_resnet3d(in_channels=3, num_classes=2, device='cuda', base_channels=12, dropout_rate=0.3, use_cbam=True):
    """
    åˆ›å»ºæ”¹è¿›ç‰ˆçš„ResNetCBAM3Dæ¨¡å‹ã€‚
    
    âœ¨ æ–°ç‰ˆæœ¬æ”¹è¿›ç‰¹æ€§:
    - ğŸ¯ çœŸæ­£çš„CBAMæ³¨æ„åŠ›æœºåˆ¶ï¼šé€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›ï¼Œæ›´å¥½åœ°å®šä½3D MRIç—…ç¶
    - ğŸ”„ æŠ—æ··å ä¸‹é‡‡æ ·ï¼šä½¿ç”¨é«˜æ–¯æ¨¡ç³Š + ä¸‹é‡‡æ ·ï¼Œå‡å°‘ç‰¹å¾æŸå¤±å’Œæ··å æ•ˆåº”  
    - ğŸ“Š å¢å¼ºçš„éšæœºæ·±åº¦ï¼šé€å±‚é€’å¢çš„éšæœºæ·±åº¦æ¦‚ç‡ï¼Œæé«˜æ­£åˆ™åŒ–æ•ˆæœ
    - ğŸ—ï¸ ä¼˜åŒ–çš„ç½‘ç»œåˆå§‹å±‚ï¼š7x7å¤§æ ¸å·ç§¯ + æŠ—æ··å ä¸‹é‡‡æ ·ï¼Œæ›´é€‚åˆ3D MRIæ•°æ®
    - ğŸšï¸ æ›´ç¨³å®šçš„å½’ä¸€åŒ–ï¼šæŒ‰ç©ºé—´ç»´åº¦å½’ä¸€åŒ–ï¼Œé¿å…æ‰¹æ¬¡é—´å¹²æ‰°
    - ğŸ² åˆ†ç±»å™¨ç®€åŒ–ï¼šç§»é™¤å¤šå¤´è®¾è®¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
    
    æ¶æ„å˜åŒ–:
    - Layer4æ–°å¢é¢å¤–ä¸‹é‡‡æ ·å±‚ï¼Œç‰¹å¾ç»´åº¦: [B,96,15,18,15] â†’ [B,192,8,9,8]
    - ç§»é™¤ç‰¹å¾é‡‘å­—å¡”æ± åŒ–ï¼Œæ”¹ä¸ºç›´æ¥å…¨å±€æ± åŒ–å’Œç‰¹å¾å‹å¹³
    - ç‰¹å¾ç»´åº¦æµ: è¾“å…¥[B,3,113,137,113] â†’ è¾“å‡º[B,2]
    - æ”¯æŒå†…å­˜æ•ˆç‡æ¨¡å¼ï¼Œé€šè¿‡å‡å°‘base_channelsé™ä½å†…å­˜å ç”¨
    
    Args:
        in_channels: è¾“å…¥é€šé“æ•°
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        device: è¿è¡Œè®¾å¤‡
        base_channels: åŸºç¡€é€šé“æ•° (é»˜è®¤12, å†…å­˜æ•ˆç‡æ¨¡å¼æ¨èä½¿ç”¨8)
        dropout_rate: Dropoutæ¯”ç‡
        use_cbam: æ˜¯å¦ä½¿ç”¨CBAMæ³¨æ„åŠ›æ¨¡å—
        
    Returns:
        torch.nn.Module: æ”¹è¿›çš„ResNetCBAM3Dæ¨¡å‹ï¼Œå…·å¤‡çœŸæ­£çš„CBAMæ³¨æ„åŠ›æœºåˆ¶
    """
    # æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯
    print(f"ğŸš€ åˆ›å»ºå¢å¼ºç‰ˆImprovedResNetCBAM3Dæ¨¡å‹")
    print(f"   åŸºç¡€é€šé“æ•°: {base_channels}, Dropoutç‡: {dropout_rate}")
    print(f"   âœ… çœŸæ­£CBAMæ³¨æ„åŠ› âœ… æŠ—æ··å ä¸‹é‡‡æ · âœ… å¢å¼ºéšæœºæ·±åº¦")
    print(f"   ä½¿ç”¨CBAM: {'âœ…' if use_cbam else 'âŒ'}")
    
    # æ£€æµ‹æ˜¯å¦å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼
    if base_channels <= 8:
        print("   ğŸ’¾ å·²å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼ï¼Œæ¨¡å‹å‚æ•°å°†æ˜¾è‘—å‡å°‘")
    
    model = ImprovedResNetCBAM3D(
        in_channels=in_channels,
        num_classes=num_classes,
        base_channels=base_channels,  # å¯é…ç½®åŸºç¡€é€šé“æ•°
        dropout_rate=dropout_rate,    # å¯é…ç½®Dropoutç‡
        use_cbam=use_cbam             # ä¼ é€’CBAMå¼€å…³
    )
    return model.to(device)

# EMAæ¨¡å‹ - ç”¨äºæ¨¡å‹æƒé‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œå‡å°‘æµ‹è¯•æ—¶çš„æ³¢åŠ¨
class EMAModel(nn.Module):
    def __init__(self, model, decay=0.999):
        super(EMAModel, self).__init__()
        self.module = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        # åˆå§‹åŒ–EMAå‚æ•°
        for name, param in self.module.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self, model):
        for name, param in model.named_parameters():
            if param.requires_grad:
                new_average = self.decay * self.shadow[name] + (1 - self.decay) * param.data
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        for name, param in self.module.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.module.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
    
    def forward(self, *inputs, **kwargs):
        return self.module(*inputs, **kwargs)

# çœŸæ­£çš„CBAMæ³¨æ„åŠ›æ¨¡å— - åŒ…å«é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›
class TrueCBAM3D(nn.Module):
    def __init__(self, channels, reduction_ratio=8, spatial_kernel_size=7):
        super(TrueCBAM3D, self).__init__()
        
        # é€šé“æ³¨æ„åŠ›æ¨¡å—
        self.channel_attention = ChannelAttention3D(channels, reduction_ratio)
        
        # ç©ºé—´æ³¨æ„åŠ›æ¨¡å—  
        self.spatial_attention = SpatialAttention3D(spatial_kernel_size)
    
    def forward(self, x):
        # å…ˆåº”ç”¨é€šé“æ³¨æ„åŠ›
        x = self.channel_attention(x) * x
        
        # å†åº”ç”¨ç©ºé—´æ³¨æ„åŠ›
        x = self.spatial_attention(x) * x
        
        return x

class ChannelAttention3D(nn.Module):
    def __init__(self, channels, reduction_ratio=8):
        super(ChannelAttention3D, self).__init__()
        reduced_channels = max(channels // reduction_ratio, 4)
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.max_pool = nn.AdaptiveMaxPool3d(1)
        
        # å…±äº«çš„MLP
        self.mlp = nn.Sequential(
            nn.Conv3d(channels, reduced_channels, kernel_size=1, bias=False),
            nn.ReLU(inplace=False),
            nn.Conv3d(reduced_channels, channels, kernel_size=1, bias=False)
        )
        
    def forward(self, x):
        # é€šè¿‡å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–è·å–å…¨å±€ä¿¡æ¯
        avg_out = self.mlp(self.avg_pool(x))
        max_out = self.mlp(self.max_pool(x))
        
        # ç›¸åŠ åé€šè¿‡sigmoidå¾—åˆ°é€šé“æ³¨æ„åŠ›æƒé‡
        channel_att = torch.sigmoid(avg_out + max_out)
        
        return channel_att

class SpatialAttention3D(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention3D, self).__init__()
        
        self.spatial_conv = nn.Sequential(
            nn.Conv3d(2, 1, kernel_size=kernel_size, padding=kernel_size//2, bias=False),
            nn.BatchNorm3d(1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # åœ¨é€šé“ç»´åº¦ä¸Šè¿›è¡Œå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        avg_out = torch.mean(x, dim=1, keepdim=True)  # [B, 1, D, H, W]
        max_out, _ = torch.max(x, dim=1, keepdim=True)  # [B, 1, D, H, W]
        
        # æ‹¼æ¥ä¸¤ä¸ªç‰¹å¾å›¾
        spatial_input = torch.cat([avg_out, max_out], dim=1)  # [B, 2, D, H, W]
        
        # é€šè¿‡å·ç§¯å¾—åˆ°ç©ºé—´æ³¨æ„åŠ›æƒé‡
        spatial_att = self.spatial_conv(spatial_input)  # [B, 1, D, H, W]
        
        return spatial_att

# æŠ—æ··å ä¸‹é‡‡æ ·æ¨¡å—
class AntiAliasDownsample3D(nn.Module):
    def __init__(self, channels, stride=2, kernel_size=3):
        super(AntiAliasDownsample3D, self).__init__()
        
        # å…ˆè¿›è¡Œè½»å¾®æ¨¡ç³Šï¼Œå†ä¸‹é‡‡æ ·
        self.blur = nn.Sequential(
            nn.ReplicationPad3d(kernel_size//2),  # ä½¿ç”¨å¤åˆ¶å¡«å……æ›¿ä»£åå°„å¡«å……
            nn.Conv3d(channels, channels, kernel_size=kernel_size, stride=1, 
                     padding=0, groups=channels, bias=False)
        )
        
        # ä¸‹é‡‡æ ·
        self.downsample = nn.AvgPool3d(kernel_size=stride, stride=stride)
        
        # åˆå§‹åŒ–æ¨¡ç³Šæ ¸ä¸ºé«˜æ–¯æ ¸
        self._init_blur_kernel(kernel_size)
        
    def _init_blur_kernel(self, kernel_size):
        # åˆ›å»º3Dé«˜æ–¯æ ¸
        sigma = 0.5
        coords = torch.arange(kernel_size, dtype=torch.float32) - kernel_size // 2
        
        # 1Dé«˜æ–¯
        gauss_1d = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
        gauss_1d = gauss_1d / gauss_1d.sum()
        
        # 3Dé«˜æ–¯æ ¸ = 1Dé«˜æ–¯çš„å¤–ç§¯
        gauss_3d = gauss_1d.view(-1, 1, 1) * gauss_1d.view(1, -1, 1) * gauss_1d.view(1, 1, -1)
        gauss_3d = gauss_3d / gauss_3d.sum()
        
        # ä¸ºæ¯ä¸ªé€šé“è®¾ç½®ç›¸åŒçš„é«˜æ–¯æ ¸
        for name, param in self.blur.named_parameters():
            if 'weight' in name:
                with torch.no_grad():
                    for i in range(param.shape[0]):  # æ¯ä¸ªè¾“å‡ºé€šé“
                        param[i, 0] = gauss_3d
                        
    def forward(self, x):
        # ğŸ’¡ æ ¸å¿ƒä¿®å¤: æ£€æŸ¥è¾“å…¥å°ºå¯¸ï¼Œå¦‚æœå°äºä¸‹é‡‡æ ·æ ¸ï¼Œåˆ™è·³è¿‡
        kernel_size = self.downsample.kernel_size
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size,) * 3  # å°†intè½¬æ¢ä¸ºå…ƒç»„

        if any(x.shape[i+2] < k for i, k in enumerate(kernel_size)):
            return x
            
        x = self.blur(x)
        x = self.downsample(x)
        return x

# å¢å¼ºçš„éšæœºæ·±åº¦å®ç°
class StochasticDepth(nn.Module):
    def __init__(self, drop_prob=0.0, mode='batch'):
        super(StochasticDepth, self).__init__()
        self.drop_prob = drop_prob
        self.mode = mode  # 'batch' æˆ– 'sample'
        
    def forward(self, x, residual):
        if not self.training or self.drop_prob == 0.0:
            return x + residual
            
        # ç”Ÿæˆéšæœºmask
        if self.mode == 'batch':
            # æ•´ä¸ªbatchä½¿ç”¨ç›¸åŒçš„éšæœºå†³ç­–
            keep_prob = 1.0 - self.drop_prob
            if torch.rand(1).item() >= self.drop_prob:
                return x + residual
            else:
                return residual
        else:
            # æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å†³ç­–
            batch_size = x.shape[0]
            keep_prob = 1.0 - self.drop_prob
            random_tensor = keep_prob + torch.rand((batch_size, 1, 1, 1, 1), 
                                                 dtype=x.dtype, device=x.device)
            binary_mask = torch.floor(random_tensor)
            return (x / keep_prob) * binary_mask + residual

# ğŸ”¥ æ™ºèƒ½ä¸‹é‡‡æ ·å±‚ - æ›¿ä»£å…¨å±€æ± åŒ–çš„æ ¸å¿ƒç»„ä»¶
class AdvancedDownsampleLayer(nn.Module):
    """
    æ™ºèƒ½ä¸‹é‡‡æ ·å±‚ - æ›¿ä»£å…¨å±€æ± åŒ–ï¼Œä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯
    
    æ ¸å¿ƒä¼˜åŠ¿:
    - ğŸ¯ ä¿ç•™ç©ºé—´ç»“æ„ä¿¡æ¯ï¼Œé¿å…å…¨å±€æ± åŒ–çš„ä¿¡æ¯æŸå¤±
    - ğŸ”§ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œå‚æ•°æ•ˆç‡é«˜
    - ğŸ’¡ é›†æˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œçªå‡ºé‡è¦ç‰¹å¾
    - ğŸ“Š è‡ªé€‚åº”æ± åŒ–ï¼Œçµæ´»æ§åˆ¶è¾“å‡ºå°ºå¯¸
    - âš–ï¸ å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
    """
    def __init__(self, in_channels, out_channels, target_size=(2, 2, 2), dropout_rate=0.3):
        super(AdvancedDownsampleLayer, self).__init__()
        
        self.target_size = target_size
        
        # æ–¹æ¡ˆ1: æ·±åº¦å¯åˆ†ç¦»å·ç§¯ä¸‹é‡‡æ · (å‚æ•°é«˜æ•ˆ)
        self.depthwise_downsample = nn.Sequential(
            # æ·±åº¦å·ç§¯ï¼šæ¯ä¸ªé€šé“ç‹¬ç«‹å¤„ç†
            nn.Conv3d(in_channels, in_channels, kernel_size=3, stride=2, 
                     padding=1, groups=in_channels, bias=False),
            StableBatchNorm3d(in_channels),
            nn.ReLU(inplace=False),
            
            # ç‚¹å·ç§¯ï¼šé€šé“é—´ä¿¡æ¯äº¤äº’
            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),
            StableBatchNorm3d(out_channels),
            nn.ReLU(inplace=False),
            nn.Dropout3d(dropout_rate)
        )
        
        # æ–¹æ¡ˆ2: æ³¨æ„åŠ›å¼•å¯¼çš„ç‰¹å¾é€‰æ‹©
        self.spatial_attention = SpatialSelectionAttention3D(out_channels)
        
        # æ–¹æ¡ˆ3: è‡ªé€‚åº”æ± åŒ–åˆ°ç›®æ ‡å°ºå¯¸
        self.adaptive_pool = nn.AdaptiveAvgPool3d(target_size)
        
        # å¯é€‰ï¼šé¢å¤–çš„ç‰¹å¾å¢å¼º
        self.feature_enhance = nn.Sequential(
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, 
                     groups=out_channels//4 if out_channels >= 4 else 1, bias=False),
            StableBatchNorm3d(out_channels),
            nn.ReLU(inplace=False)
        )
        
        print(f"ğŸ”¥ AdvancedDownsampleLayer: {in_channels}â†’{out_channels}é€šé“ï¼Œç›®æ ‡å°ºå¯¸{target_size}")
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼š[B, C, 8, 9, 8] â†’ [B, C, 2, 2, 2]
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, in_channels, D, H, W]
            
        Returns:
            features: ä¸‹é‡‡æ ·ç‰¹å¾ [B, out_channels, target_D, target_H, target_W]
        """
        # æ­¥éª¤1: æ·±åº¦å¯åˆ†ç¦»å·ç§¯ä¸‹é‡‡æ ·
        x = self.depthwise_downsample(x)  # [B, C, 4, 5, 4] (å¤§çº¦å‡åŠ)
        
        # æ­¥éª¤2: ç©ºé—´æ³¨æ„åŠ›å¢å¼ºé‡è¦åŒºåŸŸ
        x = self.spatial_attention(x)
        
        # æ­¥éª¤3: è‡ªé€‚åº”æ± åŒ–åˆ°ç²¾ç¡®ç›®æ ‡å°ºå¯¸
        x = self.adaptive_pool(x)  # [B, C, 2, 2, 2]
        
        # æ­¥éª¤4: ç‰¹å¾å¢å¼ºï¼ˆå¯é€‰ï¼‰
        x = self.feature_enhance(x)
        
        # è¿”å›æ—¶ä¿æŒç©ºé—´ç»´åº¦ï¼Œç”±ä¸Šçº§å†³å®šæ˜¯å¦flatten
        return x

# ğŸ¯ ç©ºé—´é€‰æ‹©æ³¨æ„åŠ›æ¨¡å—
class SpatialSelectionAttention3D(nn.Module):
    """
    ç©ºé—´é€‰æ‹©æ³¨æ„åŠ› - ä¸“é—¨ç”¨äºä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„é‡è¦åŒºåŸŸé€‰æ‹©
    
    åŠŸèƒ½:
    - è¯†åˆ«ç©ºé—´ä¸­æœ€é‡è¦çš„åŒºåŸŸ
    - åœ¨ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­ä¿ç•™å…³é”®ä¿¡æ¯
    - é€‚ç”¨äºåŒ»å­¦å›¾åƒçš„ç—…ç¶å®šä½
    """
    def __init__(self, channels, reduction_ratio=8):
        super(SpatialSelectionAttention3D, self).__init__()
        
        # é€šé“å‹ç¼©
        reduced_channels = max(channels // reduction_ratio, 4)
        
        # ç©ºé—´æ³¨æ„åŠ›ç”Ÿæˆç½‘ç»œ
        self.attention_conv = nn.Sequential(
            # é™ç»´
            nn.Conv3d(channels, reduced_channels, kernel_size=1, bias=False),
            nn.ReLU(inplace=False),
            
            # ç©ºé—´æ„ŸçŸ¥å·ç§¯
            nn.Conv3d(reduced_channels, reduced_channels, kernel_size=7, 
                     padding=3, groups=reduced_channels, bias=False),
            nn.ReLU(inplace=False),
            
            # å‡ç»´ + æ³¨æ„åŠ›æƒé‡ç”Ÿæˆ
            nn.Conv3d(reduced_channels, 1, kernel_size=1, bias=False),
            nn.Sigmoid()
        )
        
        # å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥
        self.global_context = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Conv3d(channels, channels // 4, kernel_size=1),
            nn.ReLU(inplace=False),
            nn.Conv3d(channels // 4, channels, kernel_size=1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, C, D, H, W]
        
        Returns:
            enhanced_x: æ³¨æ„åŠ›å¢å¼ºçš„ç‰¹å¾ [B, C, D, H, W]
        """
        # è®¡ç®—ç©ºé—´æ³¨æ„åŠ›æƒé‡
        spatial_att = self.attention_conv(x)  # [B, 1, D, H, W]
        
        # è®¡ç®—å…¨å±€ä¸Šä¸‹æ–‡æƒé‡
        global_att = self.global_context(x)  # [B, C, 1, 1, 1]
        
        # ç»“åˆç©ºé—´å’Œé€šé“æ³¨æ„åŠ›
        enhanced_x = x * spatial_att * global_att
        
        return enhanced_x 