"""
ğŸ”§ ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ æ¨¡å‹ - é—®é¢˜ä¿®å¤ç‰ˆæœ¬
==================================

ä¸»è¦æ”¹è¿›:
1. ğŸ¯ ä¿®å¤ç‰¹å¾ç©ºé—´å¯¹é½é—®é¢˜ - ä½¿ç”¨æ›´å¼ºçš„æŠ•å½±å±‚
2. âš–ï¸ é‡æ–°å¹³è¡¡æŸå¤±æƒé‡ - é™ä½å¯¹æ¯”å­¦ä¹ æƒé‡
3. ğŸ“Š æ”¹è¿›å­¦ä¹ ç‡ç­–ç•¥ - å·®å¼‚åŒ–å­¦ä¹ ç‡
4. ğŸ”„ æ·»åŠ ç‰¹å¾å½’ä¸€åŒ– - æå‡å¯¹æ¯”å­¦ä¹ æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pickle
import os
from tqdm import tqdm
import json
import pandas as pd


class ImprovedImageEncoder(nn.Module):
    """æ”¹è¿›çš„å›¾åƒç¼–ç å™¨ - å…¼å®¹å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹"""
    
    def __init__(self, pretrained_model_path, feature_dim=512, device='cuda'):
        super(ImprovedImageEncoder, self).__init__()
        
        self.device = device  # ä¿å­˜è®¾å¤‡ä¿¡æ¯
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        from optimized_models import ImprovedResNetCBAM3D
        
        print(f"ğŸ”§ åŠ è½½å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨...")
        print(f"   æ¨¡å‹è·¯å¾„: {pretrained_model_path}")
        
        # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹çš„æ¶æ„ä¿¡æ¯
        checkpoint = torch.load(pretrained_model_path, map_location=device)
        
        # ä»checkpointä¸­è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
        if 'config' in checkpoint:
            config = checkpoint['config']
            base_channels = config.get('base_channels', 12)
            print(f"   æ£€æµ‹åˆ°base_channels: {base_channels}")
        else:
            # é»˜è®¤é…ç½®
            base_channels = 12
            print(f"   ä½¿ç”¨é»˜è®¤base_channels: {base_channels}")
        
        # åˆ›å»ºä¸é¢„è®­ç»ƒæ¨¡å‹åŒ¹é…çš„æ¶æ„
        self.backbone = ImprovedResNetCBAM3D(
            in_channels=3,
            num_classes=2,
            base_channels=base_channels,
            dropout_rate=0.3,
            use_global_pool=False  # ğŸ”¥ ä½¿ç”¨æ™ºèƒ½ä¸‹é‡‡æ ·å±‚
        )
        
        # é‡å»ºfusionå±‚ä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        fusion_input_dim = base_channels * 16 * 2 * 2 * 2  # æ™ºèƒ½ä¸‹é‡‡æ ·å±‚è¾“å‡ºç»´åº¦
        
        self.backbone.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, 1024),
            nn.LayerNorm(1024),
            nn.ReLU(inplace=False),
            nn.Dropout(0.3),
            
            nn.Linear(1024, 512),  # è¾“å‡º512ç»´ç‰¹å¾
            nn.LayerNorm(512),
            nn.ReLU(inplace=False),
            nn.Dropout(0.3)
        )
        
        # é‡å»ºåˆ†ç±»å¤´
        self.backbone.classifier = nn.Linear(512, 2)
        
        # ğŸ”§ å…ˆç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ï¼Œå†åŠ è½½æƒé‡
        self.backbone.to(device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        try:
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
            
            # å°è¯•åŠ è½½æƒé‡ï¼Œå¿½ç•¥ä¸åŒ¹é…çš„é”®
            model_dict = self.backbone.state_dict()
            
            # è¿‡æ»¤æ‰ä¸åŒ¹é…çš„é”®
            filtered_state_dict = {}
            for k, v in state_dict.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    filtered_state_dict[k] = v
                else:
                    print(f"   è·³è¿‡ä¸åŒ¹é…çš„é”®: {k}")
            
            # åŠ è½½è¿‡æ»¤åçš„æƒé‡
            model_dict.update(filtered_state_dict)
            self.backbone.load_state_dict(model_dict)
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(filtered_state_dict)}/{len(state_dict)} ä¸ªæƒé‡")
            
        except Exception as e:
            print(f"âš ï¸  æƒé‡åŠ è½½è­¦å‘Š: {e}")
            print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        
        # å†»ç»“éª¨å¹²ç½‘ç»œï¼ˆé™¤äº†fusionå±‚å’Œåˆ†ç±»å¤´ï¼‰
        for name, param in self.backbone.named_parameters():
            if 'fusion' not in name and 'classifier' not in name:
                param.requires_grad = False
        
        print(f"   å·²å†»ç»“éª¨å¹²ç½‘ç»œï¼Œä¿ç•™fusionå±‚å’Œåˆ†ç±»å¤´å¯è®­ç»ƒ")
        
        # è·å–ç‰¹å¾ç»´åº¦ï¼ˆä»fusionå±‚è¾“å‡ºï¼‰
        self.backbone.eval()
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, 113, 137, 113).to(device)
            # ä½¿ç”¨return_features=Trueè·å–fusionå±‚è¾“å‡º
            features = self.backbone(dummy_input, return_features=True)
            backbone_feature_dim = features.size(1)
        
        print(f"   éª¨å¹²ç½‘ç»œç‰¹å¾ç»´åº¦: {backbone_feature_dim}")
        
        # è½»é‡çº§æŠ•å½±å±‚ - å› ä¸ºbackboneå·²ç»è¾“å‡º512ç»´
        if backbone_feature_dim == feature_dim:
            # å¦‚æœç»´åº¦å·²ç»åŒ¹é…ï¼Œä½¿ç”¨ç®€å•çš„æŠ•å½±å±‚
            self.projection = nn.Sequential(
                nn.Linear(backbone_feature_dim, feature_dim),
                nn.LayerNorm(feature_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(feature_dim, feature_dim),
                nn.LayerNorm(feature_dim)
            )
        else:
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æ›´å¤æ‚çš„æŠ•å½±å±‚
            self.projection = nn.Sequential(
                nn.Linear(backbone_feature_dim, 1024),
                nn.LayerNorm(1024),
                nn.ReLU(),
                nn.Dropout(0.2),
                
                nn.Linear(1024, 512),
                nn.LayerNorm(512),
                nn.ReLU(),
                nn.Dropout(0.1),
                
                nn.Linear(512, feature_dim),
                nn.LayerNorm(feature_dim)
            )
        
        # ğŸ”§ ç¡®ä¿æŠ•å½±å±‚ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.projection.to(device)
        
        print(f"   æŠ•å½±å±‚: {backbone_feature_dim} â†’ {feature_dim}")
        
        # åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡
        self._init_projection_weights()
    
    def _init_projection_weights(self):
        """åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡"""
        for m in self.projection.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ğŸ”§ ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        x = x.to(self.device)
        
        # ä½¿ç”¨return_features=Trueæå–éª¨å¹²ç‰¹å¾
        features = self.backbone(x, return_features=True)  # [B, 512]
        
        # æŠ•å½±åˆ°ç»Ÿä¸€ç‰¹å¾ç©ºé—´
        projected = self.projection(features)
        
        # L2æ ‡å‡†åŒ–ï¼ˆå¯¹æ¯”å­¦ä¹ å¿…éœ€ï¼‰
        projected = F.normalize(projected, p=2, dim=1)
        
        return projected


class ImprovedTextEncoder(nn.Module):
    """æ”¹è¿›çš„æ–‡æœ¬ç¼–ç å™¨ - åŸºäºé¢„æå–çš„512ç»´ç‰¹å¾"""
    
    def __init__(self, pretrained_features, feature_dim=512, device='cuda'):
        super(ImprovedTextEncoder, self).__init__()
        
        # é¢„æå–çš„æ–‡æœ¬ç‰¹å¾ (å·²ç»æ˜¯ç»è¿‡BERTè®­ç»ƒçš„512ç»´ç‰¹å¾)
        if isinstance(pretrained_features, np.ndarray):
            self.features = torch.FloatTensor(pretrained_features)
        else:
            self.features = torch.FloatTensor(pretrained_features)
        
        print(f"ğŸ“ é¢„è®­ç»ƒæ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {self.features.shape}")
        
        # å°†ç‰¹å¾ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.device = device
        self.features = self.features.to(device)
        print(f"ğŸ“± æ–‡æœ¬ç‰¹å¾å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        
        # è½»é‡çº§æŠ•å½±å±‚ - å› ä¸ºè¾“å…¥å·²ç»æ˜¯é«˜è´¨é‡çš„512ç»´ç‰¹å¾
        # ä¸»è¦ä½œç”¨æ˜¯é€‚é…å¯¹æ¯”å­¦ä¹ ï¼Œä¸éœ€è¦å¤ªå¤æ‚çš„å˜æ¢
        self.projection = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.1),  # é™ä½dropout
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        print(f"ğŸ”§ æ–‡æœ¬æŠ•å½±å±‚: {feature_dim} â†’ {feature_dim} (è½»é‡çº§)")
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.projection.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.5)  # é™ä½åˆå§‹åŒ–èŒƒå›´
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, indices):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            indices: æ ·æœ¬ç´¢å¼• [B]
            
        Returns:
            projected: L2æ ‡å‡†åŒ–çš„ç‰¹å¾ [B, feature_dim]
        """
        # ç¡®ä¿ç´¢å¼•åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if isinstance(indices, torch.Tensor):
            indices = indices.to(self.device)
        else:
            indices = torch.tensor(indices, device=self.device, dtype=torch.long)
        
        # æ ¹æ®ç´¢å¼•è·å–é¢„è®­ç»ƒç‰¹å¾
        batch_features = self.features[indices]
        
        # è½»é‡çº§æŠ•å½± - ä¿æŒé¢„è®­ç»ƒç‰¹å¾çš„è´¨é‡
        projected = self.projection(batch_features)
        
        # L2æ ‡å‡†åŒ–
        projected = F.normalize(projected, p=2, dim=1)
        
        return projected


class EndToEndTextEncoder(nn.Module):
    """ç«¯åˆ°ç«¯æ–‡æœ¬ç¼–ç å™¨ - ç›´æ¥å¤„ç†Excelæ–‡æœ¬æ•°æ®"""
    
    def __init__(self, feature_dim=512, device='cuda', max_length=512):
        super(EndToEndTextEncoder, self).__init__()
        
        self.device = device
        self.max_length = max_length
        self.feature_dim = feature_dim
        
        # åˆå§‹åŒ–BERTæ¨¡å‹å’Œåˆ†è¯å™¨
        from transformers import BertModel, BertTokenizer
        
        print(f"ğŸ”§ åˆå§‹åŒ–BERTæ–‡æœ¬ç¼–ç å™¨...")
        
        # ä½¿ç”¨æœ¬åœ°BERTæ¨¡å‹è·¯å¾„
        bert_model_path = '/root/autodl-tmp/bert-base-uncased'
        
        try:
            self.tokenizer = BertTokenizer.from_pretrained(bert_model_path)
            self.bert_model = BertModel.from_pretrained(bert_model_path)
            print(f"âœ… æœ¬åœ°BERTæ¨¡å‹åŠ è½½æˆåŠŸ: {bert_model_path}")
        except:
            # å¤‡ç”¨ï¼šä½¿ç”¨åœ¨çº¿æ¨¡å‹
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            self.bert_model = BertModel.from_pretrained('bert-base-uncased')
            print(f"âœ… åœ¨çº¿BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # ç§»åŠ¨BERTåˆ°è®¾å¤‡
        self.bert_model.to(device)
        
        # æŠ•å½±å±‚ï¼š768ç»´BERTç‰¹å¾ â†’ 512ç»´å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        self.projection = nn.Sequential(
            nn.Linear(768, 1024),
            nn.LayerNorm(1024),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(1024, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        # ğŸ”§ ç¡®ä¿æŠ•å½±å±‚åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.projection.to(device)
        
        print(f"ğŸ”§ æ–‡æœ¬æŠ•å½±å±‚: 768 â†’ {feature_dim}")
        
        # åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡
        self._init_projection_weights()
    
    def _init_projection_weights(self):
        """åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡"""
        for m in self.projection.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def encode_texts(self, texts):
        """
        ç¼–ç æ–‡æœ¬åˆ—è¡¨
        
        Args:
            texts: List[str] æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            features: [B, feature_dim] L2æ ‡å‡†åŒ–çš„æ–‡æœ¬ç‰¹å¾
        """
        # åˆ†è¯å’Œç¼–ç 
        encoded = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # BERTç¼–ç 
        with torch.no_grad() if not self.training else torch.enable_grad():
            outputs = self.bert_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            
            # ä½¿ç”¨[CLS]æ ‡è®°çš„ç‰¹å¾
            bert_features = outputs.last_hidden_state[:, 0, :]  # [B, 768]
        
        # ğŸ”§ ç¡®ä¿BERTç‰¹å¾åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        bert_features = bert_features.to(self.device)
        
        # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç‰¹å¾ç©ºé—´
        projected = self.projection(bert_features)  # [B, feature_dim]
        
        # L2æ ‡å‡†åŒ–
        projected = F.normalize(projected, p=2, dim=1)
        
        return projected
    
    def forward(self, texts):
        """å‰å‘ä¼ æ’­"""
        return self.encode_texts(texts)


class TripleLossSystem(nn.Module):
    """ä¸‰é‡æŸå¤±ç³»ç»Ÿ - ç«¯åˆ°ç«¯ç‰ˆæœ¬"""
    
    def __init__(self, temperature=0.5, margin=0.2):
        super(TripleLossSystem, self).__init__()
        self.temperature = temperature
        self.margin = margin
        self.eps = 1e-8
    
    def classification_loss(self, logits, labels):
        """
        1. AD/CNåˆ†ç±»æŸå¤±
        
        Args:
            logits: [B, 2] åˆ†ç±»logits
            labels: [B] çœŸå®æ ‡ç­¾ (0=CN, 1=AD)
            
        Returns:
            loss: åˆ†ç±»æŸå¤±
        """
        return F.cross_entropy(logits, labels)
    
    def cross_modal_alignment_loss(self, image_features, text_features):
        """
        2. å›¾åƒæ–‡æœ¬å¯¹é½æŸå¤± (InfoNCEå¯¹æ¯”å­¦ä¹ )
        
        Args:
            image_features: [B, D] L2æ ‡å‡†åŒ–çš„å›¾åƒç‰¹å¾
            text_features: [B, D] L2æ ‡å‡†åŒ–çš„æ–‡æœ¬ç‰¹å¾
            
        Returns:
            loss: è·¨æ¨¡æ€å¯¹é½æŸå¤±
        """
        batch_size = image_features.size(0)
        device = image_features.device
        
        # ç¡®ä¿ç‰¹å¾å·²æ ‡å‡†åŒ–
        image_features = F.normalize(image_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(image_features, text_features.T) / self.temperature
        
        # æ•°å€¼ç¨³å®šæ€§
        similarity_matrix = torch.clamp(similarity_matrix, -10, 10)
        
        # æ­£æ ·æœ¬æ ‡ç­¾ (å¯¹è§’çº¿)
        labels = torch.arange(batch_size, device=device)
        
        # å›¾åƒåˆ°æ–‡æœ¬çš„æŸå¤±
        loss_i2t = F.cross_entropy(similarity_matrix, labels)
        
        # æ–‡æœ¬åˆ°å›¾åƒçš„æŸå¤±
        loss_t2i = F.cross_entropy(similarity_matrix.T, labels)
        
        # å¹³å‡æŸå¤±
        return (loss_i2t + loss_t2i) / 2
    
    def intra_modal_contrastive_loss(self, image_features, labels):
        """
        3. å›¾åƒå†…éƒ¨å¯¹æ¯”æŸå¤± (ä¼˜åŒ–ç›¸åŒæ¨¡æ€å†…ç‰¹å¾åˆ†å¸ƒ)
        
        Args:
            image_features: [B, D] L2æ ‡å‡†åŒ–çš„å›¾åƒç‰¹å¾
            labels: [B] çœŸå®æ ‡ç­¾ (0=CN, 1=AD)
            
        Returns:
            loss: æ¨¡æ€å†…å¯¹æ¯”æŸå¤±
        """
        batch_size = image_features.size(0)
        device = image_features.device
        
        # ç¡®ä¿ç‰¹å¾å·²æ ‡å‡†åŒ–
        image_features = F.normalize(image_features, p=2, dim=1)
        
        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(image_features, image_features.T) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾æ©ç 
        labels = labels.contiguous().view(-1, 1)
        label_mask = torch.eq(labels, labels.T).float().to(device)
        
        # ç§»é™¤å¯¹è§’çº¿ (è‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦)
        identity_mask = torch.eye(batch_size, device=device)
        label_mask = label_mask - identity_mask
        
        # æ­£æ ·æœ¬æ©ç  (åŒç±»æ ·æœ¬ï¼Œæ’é™¤è‡ªå·±)
        positive_mask = label_mask
        
        # è´Ÿæ ·æœ¬æ©ç  (å¼‚ç±»æ ·æœ¬)
        negative_mask = 1 - label_mask - identity_mask
        
        # è®¡ç®—æ­£æ ·æœ¬æŸå¤± (åŒç±»æ ·æœ¬åº”è¯¥ç›¸ä¼¼)
        positive_similarities = similarity_matrix * positive_mask
        positive_count = positive_mask.sum(dim=1, keepdim=True)
        positive_count = torch.clamp(positive_count, min=1)  # é¿å…é™¤é›¶
        
        # æ­£æ ·æœ¬çš„å¹³å‡ç›¸ä¼¼åº¦
        positive_mean = torch.sum(positive_similarities, dim=1, keepdim=True) / positive_count
        positive_loss = -positive_mean  # è´Ÿå·ï¼šæœ€å¤§åŒ–æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        
        # è®¡ç®—è´Ÿæ ·æœ¬æŸå¤± (å¼‚ç±»æ ·æœ¬åº”è¯¥ä¸ç›¸ä¼¼)
        negative_similarities = similarity_matrix * negative_mask
        negative_count = negative_mask.sum(dim=1, keepdim=True)
        negative_count = torch.clamp(negative_count, min=1)
        
        # è´Ÿæ ·æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦ (æœ€å›°éš¾çš„è´Ÿæ ·æœ¬)
        negative_max = torch.max(negative_similarities + (1 - negative_mask) * (-100), dim=1, keepdim=True)[0]
        
        # ä½¿ç”¨margin-based loss: max(0, negative_sim - positive_sim + margin)
        triplet_loss = torch.clamp(negative_max - positive_mean + self.margin, min=0)
        
        # æ€»çš„æ¨¡æ€å†…æŸå¤±
        total_loss = (positive_loss + triplet_loss).mean()
        
        return total_loss
    
    def forward(self, logits, image_features, text_features, labels):
        """
        è®¡ç®—ä¸‰é‡æŸå¤±
        
        Args:
            logits: [B, 2] åˆ†ç±»logits
            image_features: [B, D] å›¾åƒç‰¹å¾
            text_features: [B, D] æ–‡æœ¬ç‰¹å¾
            labels: [B] çœŸå®æ ‡ç­¾
            
        Returns:
            dict: åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
        """
        # 1. AD/CNåˆ†ç±»æŸå¤±
        cls_loss = self.classification_loss(logits, labels)
        
        # 2. å›¾åƒæ–‡æœ¬å¯¹é½æŸå¤±
        alignment_loss = self.cross_modal_alignment_loss(image_features, text_features)
        
        # 3. å›¾åƒå†…éƒ¨å¯¹æ¯”æŸå¤±
        intra_loss = self.intra_modal_contrastive_loss(image_features, labels)
        
        return {
            'classification_loss': cls_loss,
            'alignment_loss': alignment_loss,
            'intra_modal_loss': intra_loss,
            'total_loss': cls_loss + alignment_loss + intra_loss
        }


class ImprovedContrastiveLoss(nn.Module):
    """æ”¹è¿›çš„å¯¹æ¯”å­¦ä¹ æŸå¤± - ä¿®å¤æ¸©åº¦å‚æ•°"""
    
    def __init__(self, temperature=0.5, reduction='mean'):  # æé«˜æ¸©åº¦ä»0.2åˆ°0.5
        super(ImprovedContrastiveLoss, self).__init__()
        self.temperature = temperature
        self.reduction = reduction
        self.eps = 1e-8
    
    def forward(self, image_features, text_features):
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤± - æ›´åˆç†çš„æ¸©åº¦å‚æ•°
        
        Args:
            image_features: [B, D] L2æ ‡å‡†åŒ–çš„å›¾åƒç‰¹å¾
            text_features: [B, D] L2æ ‡å‡†åŒ–çš„æ–‡æœ¬ç‰¹å¾
        
        Returns:
            loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        batch_size = image_features.size(0)
        device = image_features.device
        
        # ç¡®ä¿ç‰¹å¾å·²ç»æ ‡å‡†åŒ–
        image_features = F.normalize(image_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)
        
        # å‡å°‘å™ªå£°ï¼Œé¿å…ç ´åé¢„è®­ç»ƒç‰¹å¾è´¨é‡
        if self.training:
            noise_std = 0.005  # ä»0.01é™ä½åˆ°0.005
            image_features = image_features + torch.randn_like(image_features) * noise_std
            text_features = text_features + torch.randn_like(text_features) * noise_std
            # é‡æ–°å½’ä¸€åŒ–
            image_features = F.normalize(image_features, p=2, dim=1)
            text_features = F.normalize(text_features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(image_features, text_features.T) / self.temperature
        
        # æ•°å€¼ç¨³å®šæ€§å¤„ç†
        similarity_matrix = torch.clamp(similarity_matrix, -10, 10)  # è¿›ä¸€æ­¥ç¼©å°èŒƒå›´
        
        # åˆ›å»ºæ­£æ ·æœ¬æ ‡ç­¾
        labels = torch.arange(batch_size, device=device)
        
        # å›¾åƒåˆ°æ–‡æœ¬çš„æŸå¤±
        loss_i2t = F.cross_entropy(similarity_matrix, labels, reduction=self.reduction)
        
        # æ–‡æœ¬åˆ°å›¾åƒçš„æŸå¤±  
        loss_t2i = F.cross_entropy(similarity_matrix.T, labels, reduction=self.reduction)
        
        # æ€»æŸå¤±
        total_loss = (loss_i2t + loss_t2i) / 2
        
        return total_loss


class ImprovedMultiModalModel(nn.Module):
    """æ”¹è¿›çš„å¤šæ¨¡æ€æ¨¡å‹ - ä¸‰é‡æŸå¤±ç«¯åˆ°ç«¯è®­ç»ƒç‰ˆæœ¬"""
    
    def __init__(self, image_model_path, all_texts, feature_dim=512, num_classes=2, device='cuda'):
        super(ImprovedMultiModalModel, self).__init__()
        
        self.device = device  # ä¿å­˜è®¾å¤‡ä¿¡æ¯
        
        # ç¼–ç å™¨
        self.image_encoder = ImprovedImageEncoder(image_model_path, feature_dim, device)
        self.text_encoder = EndToEndTextEncoder(feature_dim, device)
        
        # ä¸‰é‡æŸå¤±ç³»ç»Ÿ
        self.triple_loss = TripleLossSystem(temperature=0.5, margin=0.2)
        
        # æ”¹è¿›çš„èåˆåˆ†ç±»å™¨
        self.fusion_classifier = nn.Sequential(
            nn.Linear(feature_dim * 2, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(128, num_classes)
        )
        
        # ğŸ”§ ç¡®ä¿èåˆåˆ†ç±»å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.fusion_classifier.to(device)
        self.triple_loss.to(device)
        
        # åˆå§‹åŒ–åˆ†ç±»å™¨æƒé‡
        self._init_classifier_weights()
    
    def _init_classifier_weights(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨æƒé‡"""
        for m in self.fusion_classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, images, texts, labels=None, mode='both'):
        """
        å‰å‘ä¼ æ’­ - ä¸‰é‡æŸå¤±ç«¯åˆ°ç«¯ç‰ˆæœ¬
        
        Args:
            images: [B, 3, 113, 137, 113] å›¾åƒæ•°æ®
            texts: List[str] æ–‡æœ¬æ•°æ®åˆ—è¡¨
            labels: [B] çœŸå®æ ‡ç­¾ (ç”¨äºè®¡ç®—æŸå¤±)
            mode: 'classification', 'losses', 'both'
        
        Returns:
            dict: åŒ…å«ä¸åŒè¾“å‡ºçš„å­—å…¸
        """
        # ğŸ”§ ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        images = images.to(self.device)
        if labels is not None:
            labels = labels.to(self.device)
        
        # ç¼–ç ç‰¹å¾
        image_features = self.image_encoder(images)  # [B, 512]
        text_features = self.text_encoder(texts)     # [B, 512]
        
        results = {
            'image_features': image_features,
            'text_features': text_features
        }
        
        if mode in ['classification', 'both']:
            # ç‰¹å¾èåˆ
            fused_features = torch.cat([image_features, text_features], dim=1)  # [B, 1024]
            
            # åˆ†ç±»
            logits = self.fusion_classifier(fused_features)  # [B, 2]
            results['logits'] = logits
        
        if mode in ['losses', 'both'] and labels is not None:
            # è®¡ç®—ä¸‰é‡æŸå¤±
            if 'logits' not in results:
                # å¦‚æœåªè®¡ç®—æŸå¤±ï¼Œä¹Ÿéœ€è¦logits
                fused_features = torch.cat([image_features, text_features], dim=1)
                logits = self.fusion_classifier(fused_features)
                results['logits'] = logits
            
            # ä¸‰é‡æŸå¤±è®¡ç®—
            loss_dict = self.triple_loss(results['logits'], image_features, text_features, labels)
            results.update(loss_dict)
        
        return results


class EndToEndMultiModalDataset(Dataset):
    """ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ•°æ®é›† - ç›´æ¥å¤„ç†Excelæ–‡æœ¬æ•°æ®"""
    
    def __init__(self, images, texts, labels):
        """
        Args:
            images: numpy array [N, 3, 113, 137, 113] å›¾åƒæ•°æ®
            texts: List[str] æ–‡æœ¬æ•°æ®åˆ—è¡¨
            labels: numpy array [N] æ ‡ç­¾æ•°æ®
        """
        self.images = torch.FloatTensor(images)
        self.texts = texts  # ä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
        self.labels = torch.LongTensor(labels)
        
        assert len(self.images) == len(self.texts) == len(self.labels)
        print(f"ğŸ“Š æ•°æ®é›†åˆ›å»º: {len(self.labels)} æ ·æœ¬")
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return {
            'image': self.images[idx],
            'text': self.texts[idx],  # è¿”å›åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
            'label': self.labels[idx],
            'index': idx
        }


class OptimizedMultiModalDataset(Dataset):
    """ä¼˜åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†"""
    
    def __init__(self, images, text_indices, labels):
        """
        Args:
            images: numpy array [N, 3, 113, 137, 113]
            text_indices: numpy array [N] æ–‡æœ¬ç‰¹å¾ç´¢å¼•
            labels: numpy array [N]
        """
        self.images = torch.FloatTensor(images)
        self.text_indices = torch.LongTensor(text_indices)
        self.labels = torch.LongTensor(labels)
        
        assert len(self.images) == len(self.text_indices) == len(self.labels)
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return {
            'image': self.images[idx],
            'text_index': self.text_indices[idx],
            'label': self.labels[idx],
            'index': idx  # ç”¨äºè°ƒè¯•
        }


class OptimizedContrastiveTrainer:
    """ä¼˜åŒ–çš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨ - ä¸‰é‡æŸå¤±ç‰ˆæœ¬"""
    
    def __init__(self, model, device, config):
        self.model = model.to(device)
        self.device = device
        self.config = config
        
        # å·®å¼‚åŒ–å­¦ä¹ ç‡ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config['num_epochs']
        )
        
        # ä¸‰é‡æŸå¤±æƒé‡é…ç½®
        self.classification_weight = config.get('classification_weight', 1.0)
        self.alignment_weight = config.get('alignment_weight', 0.5)
        self.intra_modal_weight = config.get('intra_modal_weight', 0.3)
        
        print(f"ğŸ¯ ä¸‰é‡æŸå¤±æƒé‡é…ç½®:")
        print(f"   åˆ†ç±»æŸå¤±æƒé‡: {self.classification_weight}")
        print(f"   å›¾åƒæ–‡æœ¬å¯¹é½æƒé‡: {self.alignment_weight}")
        print(f"   å›¾åƒå†…éƒ¨å¯¹æ¯”æƒé‡: {self.intra_modal_weight}")
    
    def _create_optimizer(self):
        """åˆ›å»ºå·®å¼‚åŒ–å­¦ä¹ ç‡ä¼˜åŒ–å™¨"""
        param_groups = [
            # å›¾åƒæŠ•å½±å±‚ - é«˜å­¦ä¹ ç‡
            {
                'params': self.model.image_encoder.projection.parameters(),
                'lr': self.config['learning_rate'] * 2,
                'name': 'image_projection'
            },
            # æ–‡æœ¬æŠ•å½±å±‚ - é«˜å­¦ä¹ ç‡
            {
                'params': self.model.text_encoder.projection.parameters(),
                'lr': self.config['learning_rate'] * 2,
                'name': 'text_projection'
            },
            # BERTå‚æ•° - ä½å­¦ä¹ ç‡
            {
                'params': self.model.text_encoder.bert_model.parameters(),
                'lr': self.config['learning_rate'] * 0.1,
                'name': 'bert_backbone'
            },
            # èåˆåˆ†ç±»å™¨ - æ ‡å‡†å­¦ä¹ ç‡
            {
                'params': self.model.fusion_classifier.parameters(),
                'lr': self.config['learning_rate'],
                'name': 'fusion_classifier'
            }
        ]
        
        return torch.optim.AdamW(param_groups, weight_decay=self.config['weight_decay'])
    
    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - ä¸‰é‡æŸå¤±ç‰ˆæœ¬"""
        self.model.train()
        
        total_loss = 0.0
        total_classification_loss = 0.0
        total_alignment_loss = 0.0
        total_intra_modal_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        
        for batch in progress_bar:
            images = batch['image'].to(self.device)
            texts = batch['text']  # æ–‡æœ¬åˆ—è¡¨
            labels = batch['label'].to(self.device)
            
            # å‰å‘ä¼ æ’­ - è®¡ç®—æ‰€æœ‰æŸå¤±
            outputs = self.model(images, texts, labels=labels, mode='both')
            
            # æå–ä¸‰é‡æŸå¤±
            classification_loss = outputs['classification_loss']
            alignment_loss = outputs['alignment_loss']
            intra_modal_loss = outputs['intra_modal_loss']
            
            # åŠ æƒæ€»æŸå¤±
            total_batch_loss = (
                self.classification_weight * classification_loss +
                self.alignment_weight * alignment_loss +
                self.intra_modal_weight * intra_modal_loss
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_batch_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['gradient_clip'])
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += total_batch_loss.item()
            total_classification_loss += classification_loss.item()
            total_alignment_loss += alignment_loss.item()
            total_intra_modal_loss += intra_modal_loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            logits = outputs['logits']
            _, predicted = torch.max(logits.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            accuracy = 100. * correct / total
            progress_bar.set_postfix({
                'Loss': f'{total_batch_loss.item():.4f}',
                'Acc': f'{accuracy:.2f}%',
                'CLS': f'{classification_loss.item():.4f}',
                'ALN': f'{alignment_loss.item():.4f}',
                'INT': f'{intra_modal_loss.item():.4f}'
            })
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()
        
        return {
            'total_loss': total_loss / len(dataloader),
            'classification_loss': total_classification_loss / len(dataloader),
            'alignment_loss': total_alignment_loss / len(dataloader),
            'intra_modal_loss': total_intra_modal_loss / len(dataloader),
            'accuracy': accuracy / 100.0,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹ - ä¸‰é‡æŸå¤±ç‰ˆæœ¬"""
        self.model.eval()
        
        total_loss = 0.0
        total_classification_loss = 0.0
        total_alignment_loss = 0.0
        total_intra_modal_loss = 0.0
        correct = 0
        total = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc='è¯„ä¼°ä¸­'):
                images = batch['image'].to(self.device)
                texts = batch['text']  # æ–‡æœ¬åˆ—è¡¨
                labels = batch['label'].to(self.device)
                
                # å‰å‘ä¼ æ’­ - è®¡ç®—æ‰€æœ‰æŸå¤±
                outputs = self.model(images, texts, labels=labels, mode='both')
                
                # æå–æŸå¤±
                classification_loss = outputs['classification_loss']
                alignment_loss = outputs['alignment_loss']
                intra_modal_loss = outputs['intra_modal_loss']
                
                # åŠ æƒæ€»æŸå¤±
                total_batch_loss = (
                    self.classification_weight * classification_loss +
                    self.alignment_weight * alignment_loss +
                    self.intra_modal_weight * intra_modal_loss
                )
                
                total_loss += total_batch_loss.item()
                total_classification_loss += classification_loss.item()
                total_alignment_loss += alignment_loss.item()
                total_intra_modal_loss += intra_modal_loss.item()
                
                # é¢„æµ‹
                logits = outputs['logits']
                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        accuracy = 100. * correct / total
        report = classification_report(all_labels, all_predictions, target_names=['CN', 'AD'], output_dict=True)
        conf_matrix = confusion_matrix(all_labels, all_predictions)
        
        return {
            'loss': total_loss / len(dataloader),
            'classification_loss': total_classification_loss / len(dataloader),
            'alignment_loss': total_alignment_loss / len(dataloader),
            'intra_modal_loss': total_intra_modal_loss / len(dataloader),
            'accuracy': accuracy / 100.0,
            'classification_report': report,
            'confusion_matrix': conf_matrix,
            'predictions': all_predictions,
            'labels': all_labels
        }


def load_end_to_end_data(image_data_dir, text_data_dir, test_size=0.2, random_state=42):
    """
    åŠ è½½ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ•°æ® - ä¸¥æ ¼é˜²æ•°æ®æ³„éœ²ç‰ˆæœ¬ (ä¿®å¤æ‚£è€…IDæå–é€»è¾‘)
    
    Args:
        image_data_dir: å›¾åƒæ•°æ®ç›®å½•
        text_data_dir: æ–‡æœ¬æ•°æ®ç›®å½•  
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
        
    Returns:
        è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    """
    print("ğŸ”„ åŠ è½½ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ•°æ® (ä¿®å¤æ‚£è€…IDæå–é€»è¾‘ç‰ˆæœ¬)...")
    
    # 1. åŠ è½½å›¾åƒæ•°æ®
    from data_utils import load_early_fusion_data
    image_data, image_labels = load_early_fusion_data(image_data_dir)
    
    # 2. åŠ è½½æ–‡æœ¬æ•°æ®
    text_data, text_labels, patient_ids = load_text_data_from_excel_clean(text_data_dir)
    
    # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    assert len(image_data) == len(text_data), \
        f"å›¾åƒæ•°æ®({len(image_data)})å’Œæ–‡æœ¬æ•°æ®({len(text_data)})æ•°é‡ä¸åŒ¹é…"
    assert len(image_labels) == len(text_labels), \
        f"å›¾åƒæ ‡ç­¾({len(image_labels)})å’Œæ–‡æœ¬æ ‡ç­¾({len(text_labels)})æ•°é‡ä¸åŒ¹é…"
    assert np.array_equal(image_labels, text_labels), \
        "å›¾åƒæ ‡ç­¾å’Œæ–‡æœ¬æ ‡ç­¾ä¸ä¸€è‡´"
    
    print(f"ğŸ“Š æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡:")
    print(f"   å›¾åƒæ•°æ®: {image_data.shape}")
    print(f"   æ–‡æœ¬æ•°æ®: {len(text_data)} æ¡")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: AD={np.sum(text_labels==1)}, CN={np.sum(text_labels==0)}")
    
    # 4. ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æå–å›¾åƒæ–‡ä»¶çš„æ‚£è€…ID
    print(f"ğŸ”§ ä¿®å¤å›¾åƒæ–‡ä»¶æ‚£è€…IDæå–é€»è¾‘...")
    
    def extract_patient_id_from_filename(filename):
        """
        ä»å›¾åƒæ–‡ä»¶åæå–æ‚£è€…ID
        æ–‡ä»¶åæ ¼å¼: mwp3MRI_002_S_0619_3-2016-01-29_12_25_03.0.nii
        ç›®æ ‡æå–: 002_S_0619
        """
        basename = filename.split('.')[0]  # ç§»é™¤.niiåç¼€
        
        # åˆ†å‰²æ–‡ä»¶å
        parts = basename.split('_')
        
        # æŸ¥æ‰¾æ¨¡å¼: mwp3MRI_XXX_S_YYYY
        if len(parts) >= 4 and parts[0].startswith('mwp') and parts[2] == 'S':
            # æå– XXX_S_YYYY éƒ¨åˆ†
            patient_id = f"{parts[1]}_{parts[2]}_{parts[3]}"
            return patient_id
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾ XXX_S_YYYY æ¨¡å¼
        for i in range(len(parts) - 2):
            if parts[i+1] == 'S' and parts[i].isdigit() and parts[i+2].isdigit():
                return f"{parts[i]}_{parts[i+1]}_{parts[i+2]}"
        
        # æœ€åå¤‡ç”¨ï¼šè¿”å›åŸå§‹æ–‡ä»¶å
        return basename
    
    # é‡æ–°æ„å»ºå›¾åƒæ•°æ®çš„æ‚£è€…IDåˆ—è¡¨
    print(f"ğŸ“Š é‡æ–°æ„å»ºå›¾åƒæ•°æ®çš„æ‚£è€…IDæ˜ å°„...")
    
    # æ„å»ºå›¾åƒæ–‡ä»¶åˆ°æ‚£è€…IDçš„æ˜ å°„
    image_patient_ids = []
    
    # é‡æ–°æ‰«æå›¾åƒæ–‡ä»¶è·å–æ‚£è€…ID
    ad_csf_dir = os.path.join(image_data_dir, "123-AD-MRI", "ADfinalCSF")
    cn_csf_dir = os.path.join(image_data_dir, "123-CN-MRI", "CNfinalCSF")
    
    # è·å–ADæ–‡ä»¶çš„æ‚£è€…ID
    ad_files = sorted([f for f in os.listdir(ad_csf_dir) if f.endswith('.nii')])
    for filename in ad_files:
        patient_id = extract_patient_id_from_filename(filename)
        image_patient_ids.append(patient_id)
    
    # è·å–CNæ–‡ä»¶çš„æ‚£è€…ID
    cn_files = sorted([f for f in os.listdir(cn_csf_dir) if f.endswith('.nii')])
    for filename in cn_files:
        patient_id = extract_patient_id_from_filename(filename)
        image_patient_ids.append(patient_id)
    
    print(f"ğŸ“Š ä¿®å¤åçš„æ‚£è€…IDç¤ºä¾‹:")
    print(f"   å›¾åƒæ‚£è€…IDç¤ºä¾‹: {image_patient_ids[:5]}")
    print(f"   æ–‡æœ¬æ‚£è€…IDç¤ºä¾‹: {patient_ids[:5]}")
    
    # 5. åˆ›å»ºæ‚£è€…IDåˆ°æ•°æ®çš„æ˜ å°„
    image_id_to_data = {}
    text_id_to_data = {}
    
    # å›¾åƒæ•°æ®æ˜ å°„
    for i, pid in enumerate(image_patient_ids):
        image_id_to_data[pid] = {
            'data': image_data[i],
            'label': image_labels[i],
            'index': i
        }
    
    # æ–‡æœ¬æ•°æ®æ˜ å°„
    for i, pid in enumerate(patient_ids):
        text_id_to_data[pid] = {
            'data': text_data[i],
            'label': text_labels[i],
            'index': i
        }
    
    # 6. æ‰¾åˆ°å…±åŒçš„æ‚£è€…ID
    image_patient_set = set(image_patient_ids)
    text_patient_set = set(patient_ids)
    common_patients = image_patient_set & text_patient_set
    
    print(f"ğŸ“Š ä¿®å¤åæ‚£è€…IDå¯¹é½ç»Ÿè®¡:")
    print(f"   å›¾åƒæ‚£è€…æ•°: {len(image_patient_set)}")
    print(f"   æ–‡æœ¬æ‚£è€…æ•°: {len(text_patient_set)}")
    print(f"   å…±åŒæ‚£è€…æ•°: {len(common_patients)}")
    
    if len(common_patients) < len(image_patient_set) * 0.8:
        print(f"âš ï¸  è­¦å‘Š: å…±åŒæ‚£è€…æ¯”ä¾‹ä»ç„¶è¾ƒä½ ({len(common_patients)}/{len(image_patient_set)})")
        
        # æ˜¾ç¤ºä¸åŒ¹é…çš„æ‚£è€…IDè¿›è¡Œè°ƒè¯•
        image_only = image_patient_set - text_patient_set
        text_only = text_patient_set - image_patient_set
        
        if image_only:
            print(f"   ä»…åœ¨å›¾åƒä¸­: {sorted(list(image_only))[:10]}...")
        if text_only:
            print(f"   ä»…åœ¨æ–‡æœ¬ä¸­: {sorted(list(text_only))[:10]}...")
    else:
        print(f"âœ… æ‚£è€…IDå¯¹é½æˆåŠŸç‡: {len(common_patients)/len(image_patient_set)*100:.1f}%")
    
    # 7. æŒ‰å…±åŒæ‚£è€…IDé‡æ–°ç»„ç»‡æ•°æ®
    common_patients_list = sorted(list(common_patients))  # æ’åºç¡®ä¿ä¸€è‡´æ€§
    
    aligned_image_data = []
    aligned_text_data = []
    aligned_labels = []
    aligned_patient_ids = []
    
    for patient_id in common_patients_list:
        # éªŒè¯æ ‡ç­¾ä¸€è‡´æ€§
        img_label = image_id_to_data[patient_id]['label']
        txt_label = text_id_to_data[patient_id]['label']
        
        if img_label != txt_label:
            print(f"âš ï¸  æ‚£è€… {patient_id} æ ‡ç­¾ä¸ä¸€è‡´: å›¾åƒ={img_label}, æ–‡æœ¬={txt_label}")
            continue
        
        aligned_image_data.append(image_id_to_data[patient_id]['data'])
        aligned_text_data.append(text_id_to_data[patient_id]['data'])
        aligned_labels.append(img_label)
        aligned_patient_ids.append(patient_id)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    aligned_image_data = np.array(aligned_image_data)
    aligned_labels = np.array(aligned_labels)
    
    print(f"âœ… ä¿®å¤åæ‚£è€…IDå¯¹é½å®Œæˆ:")
    print(f"   å¯¹é½åæ ·æœ¬æ•°: {len(aligned_labels)}")
    print(f"   å›¾åƒæ•°æ®å½¢çŠ¶: {aligned_image_data.shape}")
    print(f"   æ–‡æœ¬æ•°æ®æ•°é‡: {len(aligned_text_data)}")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: AD={np.sum(aligned_labels==1)}, CN={np.sum(aligned_labels==0)}")
    
    # 8. éªŒè¯å¯¹é½æ•ˆæœ
    print(f"ğŸ” éªŒè¯å‰5ä¸ªæ ·æœ¬çš„æ‚£è€…IDå¯¹é½:")
    for i in range(min(5, len(aligned_patient_ids))):
        print(f"   æ ·æœ¬{i}: æ‚£è€…ID={aligned_patient_ids[i]}, æ ‡ç­¾={aligned_labels[i]}")
    
    # 9. ä¸¥æ ¼çš„æ‚£è€…çº§åˆ«æ•°æ®åˆ†å‰²
    from sklearn.model_selection import StratifiedShuffleSplit
    
    # ä½¿ç”¨æ‚£è€…IDç¡®ä¿åŒä¸€æ‚£è€…çš„æ•°æ®ä¸ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’Œæµ‹è¯•é›†
    splitter = StratifiedShuffleSplit(
        n_splits=1, 
        test_size=test_size, 
        random_state=random_state
    )
    
    train_idx, test_idx = next(splitter.split(aligned_image_data, aligned_labels))
    
    # åˆ†å‰²æ•°æ®
    train_images = aligned_image_data[train_idx]
    test_images = aligned_image_data[test_idx]
    train_texts = [aligned_text_data[i] for i in train_idx]
    test_texts = [aligned_text_data[i] for i in test_idx]
    train_labels = aligned_labels[train_idx]
    test_labels = aligned_labels[test_idx]
    
    # éªŒè¯åˆ†å‰²ç»“æœ
    print(f"ğŸ“Š ä¸¥æ ¼æ•°æ®åˆ†å‰²ç»“æœ:")
    print(f"   è®­ç»ƒé›†: {len(train_labels)} æ ·æœ¬ (AD={np.sum(train_labels==1)}, CN={np.sum(train_labels==0)})")
    print(f"   æµ‹è¯•é›†: {len(test_labels)} æ ·æœ¬ (AD={np.sum(test_labels==1)}, CN={np.sum(test_labels==0)})")
    print(f"   è®­ç»ƒé›†ç±»åˆ«æ¯”ä¾‹: {np.sum(train_labels==1)/len(train_labels):.3f}")
    print(f"   æµ‹è¯•é›†ç±»åˆ«æ¯”ä¾‹: {np.sum(test_labels==1)/len(test_labels):.3f}")
    
    # æ£€æŸ¥æ•°æ®æ³„éœ²
    train_patient_ids = [aligned_patient_ids[i] for i in train_idx]
    test_patient_ids = [aligned_patient_ids[i] for i in test_idx]
    overlap = set(train_patient_ids) & set(test_patient_ids)
    
    if overlap:
        print(f"âš ï¸  è­¦å‘Š: å‘ç°æ‚£è€…IDé‡å : {overlap}")
    else:
        print(f"âœ… æ•°æ®æ³„éœ²æ£€æŸ¥é€šè¿‡: è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ— æ‚£è€…é‡å ")
    
    return (train_images, train_texts, train_labels, 
            test_images, test_texts, test_labels)


def load_text_data_from_excel_clean(text_data_dir):
    """
    ä»Excelæ–‡ä»¶åŠ è½½æ–‡æœ¬æ•°æ® - å®Œå…¨æ¸…æ´ç‰ˆæœ¬ï¼ˆæ— è¯Šæ–­ä¿¡æ¯æ³„éœ²ï¼‰
    
    Args:
        text_data_dir: æ–‡æœ¬æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        all_texts: List[str] æ‰€æœ‰æ–‡æœ¬æ•°æ®ï¼ˆä¸åŒ…å«è¯Šæ–­ä¿¡æ¯ï¼‰
        all_labels: numpy array æ‰€æœ‰æ ‡ç­¾
        patient_ids: List[str] æ‚£è€…IDåˆ—è¡¨ (ä½¿ç”¨NAMEåˆ—)
    """
    print(f"ğŸ“ ä»Excelæ–‡ä»¶åŠ è½½æ–‡æœ¬æ•°æ® (å®Œå…¨æ¸…æ´ç‰ˆæœ¬ - æ— è¯Šæ–­æ³„éœ²)...")
    
    # æ–‡ä»¶è·¯å¾„
    ad_file = os.path.join(text_data_dir, 'final_AD_updated.xlsx')
    cn_file = os.path.join(text_data_dir, 'final_CN_updated.xlsx')
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(ad_file):
        raise FileNotFoundError(f"ADæ–‡ä»¶ä¸å­˜åœ¨: {ad_file}")
    if not os.path.exists(cn_file):
        raise FileNotFoundError(f"CNæ–‡ä»¶ä¸å­˜åœ¨: {cn_file}")
    
    # åŠ è½½æ•°æ®
    ad_df = pd.read_excel(ad_file)
    cn_df = pd.read_excel(cn_file)
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   ADæ ·æœ¬: {len(ad_df)} è¡Œ")
    print(f"   CNæ ·æœ¬: {len(cn_df)} è¡Œ")
    print(f"   ADåˆ—å: {list(ad_df.columns)}")
    print(f"   CNåˆ—å: {list(cn_df.columns)}")
    
    def create_clean_clinical_text(row):
        """
        åˆ›å»ºå®Œå…¨æ¸…æ´çš„ä¸´åºŠæ–‡æœ¬æè¿° - ç»å¯¹ä¸åŒ…å«è¯Šæ–­ä¿¡æ¯
        
        âš ï¸ é‡è¦ï¼šæ­¤å‡½æ•°ç»å¯¹ä¸èƒ½åŒ…å«ä»»ä½•å¯èƒ½æ³„éœ²è¯Šæ–­çš„ä¿¡æ¯
        """
        text_parts = []
        
        # åŸºæœ¬äººå£ç»Ÿè®¡å­¦ä¿¡æ¯
        if 'Age' in row and pd.notna(row['Age']):
            text_parts.append(f"Patient age: {row['Age']} years")
        
        if 'Gender' in row and pd.notna(row['Gender']):
            # è½¬æ¢æ€§åˆ«ç¼–ç 
            gender = "male" if row['Gender'] == 1 else "female"
            text_parts.append(f"Gender: {gender}")
        
        if 'Edu' in row and pd.notna(row['Edu']):
            text_parts.append(f"Education level: {row['Edu']} years")
        
        # è®¤çŸ¥è¯„ä¼°åˆ†æ•° - è¿™äº›æ˜¯å®¢è§‚æµ‹é‡ï¼Œä¸ç›´æ¥æ³„éœ²è¯Šæ–­
        cognitive_scores = []
        
        if 'MMSE' in row and pd.notna(row['MMSE']):
            cognitive_scores.append(f"MMSE score: {row['MMSE']}")
        
        if 'CDRSB' in row and pd.notna(row['CDRSB']):
            cognitive_scores.append(f"CDR-SB score: {row['CDRSB']}")
        
        # æ·»åŠ å…¶ä»–å¯ç”¨çš„è®¤çŸ¥æµ‹è¯•åˆ†æ•°
        additional_scores = []
        for col in row.index:
            if col in ['ADAS11', 'ADAS13', 'RAVLT_immediate', 'RAVLT_learning', 'RAVLT_forgetting', 'RAVLT_perc_forgetting']:
                if pd.notna(row[col]):
                    additional_scores.append(f"{col}: {row[col]}")
        
        # ç»„åˆæ‰€æœ‰ä¿¡æ¯
        if cognitive_scores:
            text_parts.append("Cognitive assessment: " + ", ".join(cognitive_scores))
        
        if additional_scores:
            text_parts.append("Additional measures: " + ", ".join(additional_scores))
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œåˆ›å»ºåŸºæœ¬æè¿°
        if len(text_parts) == 0:
            text_parts = ["Clinical assessment data available for analysis"]
        
        # ğŸ”¥ å…³é”®ï¼šç»å¯¹ä¸æ·»åŠ ä»»ä½•è¯Šæ–­ç›¸å…³ä¿¡æ¯
        final_text = " ".join(text_parts)
        
        # é¢å¤–å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡æœ¬ä¸­ä¸åŒ…å«è¯Šæ–­å…³é”®è¯
        diagnosis_keywords = ['alzheimer', 'dementia', 'ad', 'normal', 'cn', 'cognitively normal', 'diagnosis', 'disease']
        final_text_lower = final_text.lower()
        
        for keyword in diagnosis_keywords:
            if keyword in final_text_lower:
                print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å¯èƒ½çš„è¯Šæ–­æ³„éœ²å…³é”®è¯ '{keyword}' åœ¨æ–‡æœ¬ä¸­")
                # ç§»é™¤åŒ…å«å…³é”®è¯çš„éƒ¨åˆ†
                words = final_text.split()
                filtered_words = [word for word in words if keyword not in word.lower()]
                final_text = " ".join(filtered_words)
        
        return final_text
    
    # å¤„ç†ADæ•°æ®
    ad_texts = []
    ad_patient_ids = []
    for idx, row in ad_df.iterrows():
        text = create_clean_clinical_text(row)
        ad_texts.append(text)
        
        # æå–æ‚£è€…ID - ä½¿ç”¨NAMEåˆ—
        if 'NAME' in row and pd.notna(row['NAME']):
            ad_patient_ids.append(str(row['NAME']))
        else:
            # å¤‡ç”¨ï¼šä»wholecodeæå–NAMEéƒ¨åˆ†
            if 'wholecode' in row and pd.notna(row['wholecode']):
                wholecode = str(row['wholecode'])
                # ä»wholecodeæå–NAME: "029_S_4385_3-2016-01-29_12_25_03.0.nii" -> "029_S_4385"
                parts = wholecode.split('_')
                if len(parts) >= 3:
                    name = f"{parts[0]}_{parts[1]}_{parts[2]}"
                    ad_patient_ids.append(name)
                else:
                    ad_patient_ids.append(f"AD_{idx}")
            else:
                ad_patient_ids.append(f"AD_{idx}")
    
    # å¤„ç†CNæ•°æ®
    cn_texts = []
    cn_patient_ids = []
    for idx, row in cn_df.iterrows():
        text = create_clean_clinical_text(row)
        cn_texts.append(text)
        
        # æå–æ‚£è€…ID - ä½¿ç”¨NAMEåˆ—
        if 'NAME' in row and pd.notna(row['NAME']):
            cn_patient_ids.append(str(row['NAME']))
        else:
            # å¤‡ç”¨ï¼šä»wholecodeæå–NAMEéƒ¨åˆ†
            if 'wholecode' in row and pd.notna(row['wholecode']):
                wholecode = str(row['wholecode'])
                # ä»wholecodeæå–NAME: "029_S_4385_3-2016-01-29_12_25_03.0.nii" -> "029_S_4385"
                parts = wholecode.split('_')
                if len(parts) >= 3:
                    name = f"{parts[0]}_{parts[1]}_{parts[2]}"
                    cn_patient_ids.append(name)
                else:
                    cn_patient_ids.append(f"CN_{idx}")
            else:
                cn_patient_ids.append(f"CN_{idx}")
    
    # åˆå¹¶æ•°æ®
    all_texts = ad_texts + cn_texts
    all_labels = np.array([1] * len(ad_texts) + [0] * len(cn_texts))  # AD=1, CN=0
    patient_ids = ad_patient_ids + cn_patient_ids
    
    print(f"âœ… æ¸…æ´æ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆ (æ— è¯Šæ–­æ³„éœ²):")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_texts)}")
    print(f"   ADæ ·æœ¬: {len(ad_texts)}, CNæ ·æœ¬: {len(cn_texts)}")
    print(f"   ç¤ºä¾‹æ‚£è€…ID: {patient_ids[:5]}")
    print(f"   ç¤ºä¾‹ADæ–‡æœ¬: {all_texts[0][:150]}...")
    print(f"   ç¤ºä¾‹CNæ–‡æœ¬: {all_texts[len(ad_texts)][:150]}...")
    
    # ğŸ”¥ æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ‰€æœ‰æ–‡æœ¬éƒ½ä¸åŒ…å«è¯Šæ–­ä¿¡æ¯
    diagnosis_leak_count = 0
    diagnosis_keywords = ['alzheimer', 'dementia', 'ad', 'normal', 'cn', 'cognitively normal', 'diagnosis', 'disease']
    
    for i, text in enumerate(all_texts):
        text_lower = text.lower()
        for keyword in diagnosis_keywords:
            if keyword in text_lower:
                diagnosis_leak_count += 1
                print(f"âš ï¸  å‘ç°è¯Šæ–­æ³„éœ²: æ ·æœ¬{i} åŒ…å«å…³é”®è¯ '{keyword}'")
                break
    
    if diagnosis_leak_count == 0:
        print(f"âœ… è¯Šæ–­æ³„éœ²æ£€æŸ¥é€šè¿‡: æ‰€æœ‰{len(all_texts)}ä¸ªæ–‡æœ¬æ ·æœ¬éƒ½ä¸åŒ…å«è¯Šæ–­ä¿¡æ¯")
    else:
        print(f"âŒ å‘ç°{diagnosis_leak_count}ä¸ªæ ·æœ¬å­˜åœ¨è¯Šæ–­æ³„éœ²é£é™©")
    
    return all_texts, all_labels, patient_ids


def load_end_to_end_data_for_cv(image_data_dir, text_data_dir, holdout_test_size=0.2, random_state=42):
    """
    ä¸ºäº¤å‰éªŒè¯åŠ è½½æ•°æ® - é˜²æ­¢æ•°æ®æ³„éœ²ç‰ˆæœ¬
    
    ç­–ç•¥:
    1. é¦–å…ˆåˆ†ç¦»å‡ºç‹¬ç«‹çš„holdoutæµ‹è¯•é›† (20%)
    2. å‰©ä½™80%æ•°æ®ç”¨äº5æŠ˜äº¤å‰éªŒè¯
    3. ç¡®ä¿holdoutæµ‹è¯•é›†åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¸è¢«ä½¿ç”¨
    
    Args:
        image_data_dir: å›¾åƒæ•°æ®ç›®å½•
        text_data_dir: æ–‡æœ¬æ•°æ®ç›®å½•  
        holdout_test_size: ç‹¬ç«‹æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤20%)
        random_state: éšæœºç§å­
        
    Returns:
        cv_images: äº¤å‰éªŒè¯ç”¨å›¾åƒæ•°æ® (80%)
        cv_texts: äº¤å‰éªŒè¯ç”¨æ–‡æœ¬æ•°æ® (80%)
        cv_labels: äº¤å‰éªŒè¯ç”¨æ ‡ç­¾ (80%)
        holdout_images: ç‹¬ç«‹æµ‹è¯•é›†å›¾åƒ (20%)
        holdout_texts: ç‹¬ç«‹æµ‹è¯•é›†æ–‡æœ¬ (20%)
        holdout_labels: ç‹¬ç«‹æµ‹è¯•é›†æ ‡ç­¾ (20%)
    """
    print("ğŸ”„ åŠ è½½äº¤å‰éªŒè¯æ•°æ® (é˜²æ•°æ®æ³„éœ²ç‰ˆæœ¬)...")
    print("="*60)
    
    # 1. ç›´æ¥åŠ è½½æ‰€æœ‰å¯¹é½çš„æ•°æ® - ä¸è¿›è¡Œåˆ†å‰²
    print("ğŸ“Š åŠ è½½æ‰€æœ‰å¯¹é½æ•°æ®...")
    
    # åŠ è½½å›¾åƒæ•°æ®
    from data_utils import load_early_fusion_data
    image_data, image_labels = load_early_fusion_data(image_data_dir)
    
    # åŠ è½½æ–‡æœ¬æ•°æ®
    text_data, text_labels, patient_ids = load_text_data_from_excel_clean(text_data_dir)
    
    # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    assert len(image_data) == len(text_data), \
        f"å›¾åƒæ•°æ®({len(image_data)})å’Œæ–‡æœ¬æ•°æ®({len(text_data)})æ•°é‡ä¸åŒ¹é…"
    assert len(image_labels) == len(text_labels), \
        f"å›¾åƒæ ‡ç­¾({len(image_labels)})å’Œæ–‡æœ¬æ ‡ç­¾({len(text_labels)})æ•°é‡ä¸åŒ¹é…"
    assert np.array_equal(image_labels, text_labels), \
        "å›¾åƒæ ‡ç­¾å’Œæ–‡æœ¬æ ‡ç­¾ä¸ä¸€è‡´"
    
    print(f"ğŸ“Š æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡:")
    print(f"   å›¾åƒæ•°æ®: {image_data.shape}")
    print(f"   æ–‡æœ¬æ•°æ®: {len(text_data)} æ¡")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: AD={np.sum(text_labels==1)}, CN={np.sum(text_labels==0)}")
    
    # 2. ä¿®å¤ï¼šæ­£ç¡®æå–å›¾åƒæ–‡ä»¶çš„æ‚£è€…ID
    print(f"ğŸ”§ ä¿®å¤å›¾åƒæ–‡ä»¶æ‚£è€…IDæå–é€»è¾‘...")
    
    def extract_patient_id_from_filename(filename):
        """
        ä»å›¾åƒæ–‡ä»¶åæå–æ‚£è€…ID
        æ–‡ä»¶åæ ¼å¼: mwp3MRI_002_S_0619_3-2016-01-29_12_25_03.0.nii
        ç›®æ ‡æå–: 002_S_0619
        """
        basename = filename.split('.')[0]  # ç§»é™¤.niiåç¼€
        
        # åˆ†å‰²æ–‡ä»¶å
        parts = basename.split('_')
        
        # æŸ¥æ‰¾æ¨¡å¼: mwp3MRI_XXX_S_YYYY
        if len(parts) >= 4 and parts[0].startswith('mwp') and parts[2] == 'S':
            # æå– XXX_S_YYYY éƒ¨åˆ†
            patient_id = f"{parts[1]}_{parts[2]}_{parts[3]}"
            return patient_id
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾ XXX_S_YYYY æ¨¡å¼
        for i in range(len(parts) - 2):
            if parts[i+1] == 'S' and parts[i].isdigit() and parts[i+2].isdigit():
                return f"{parts[i]}_{parts[i+1]}_{parts[i+2]}"
        
        # æœ€åå¤‡ç”¨ï¼šè¿”å›åŸå§‹æ–‡ä»¶å
        return basename
    
    # é‡æ–°æ„å»ºå›¾åƒæ•°æ®çš„æ‚£è€…IDåˆ—è¡¨
    print(f"ğŸ“Š é‡æ–°æ„å»ºå›¾åƒæ•°æ®çš„æ‚£è€…IDæ˜ å°„...")
    
    # æ„å»ºå›¾åƒæ–‡ä»¶åˆ°æ‚£è€…IDçš„æ˜ å°„
    image_patient_ids = []
    
    # é‡æ–°æ‰«æå›¾åƒæ–‡ä»¶è·å–æ‚£è€…ID
    ad_csf_dir = os.path.join(image_data_dir, "123-AD-MRI", "ADfinalCSF")
    cn_csf_dir = os.path.join(image_data_dir, "123-CN-MRI", "CNfinalCSF")
    
    # è·å–ADæ–‡ä»¶çš„æ‚£è€…ID
    ad_files = sorted([f for f in os.listdir(ad_csf_dir) if f.endswith('.nii')])
    for filename in ad_files:
        patient_id = extract_patient_id_from_filename(filename)
        image_patient_ids.append(patient_id)
    
    # è·å–CNæ–‡ä»¶çš„æ‚£è€…ID
    cn_files = sorted([f for f in os.listdir(cn_csf_dir) if f.endswith('.nii')])
    for filename in cn_files:
        patient_id = extract_patient_id_from_filename(filename)
        image_patient_ids.append(patient_id)
    
    print(f"ğŸ“Š ä¿®å¤åçš„æ‚£è€…IDç¤ºä¾‹:")
    print(f"   å›¾åƒæ‚£è€…IDç¤ºä¾‹: {image_patient_ids[:5]}")
    print(f"   æ–‡æœ¬æ‚£è€…IDç¤ºä¾‹: {patient_ids[:5]}")
    
    # 3. åˆ›å»ºæ‚£è€…IDåˆ°æ•°æ®çš„æ˜ å°„
    image_id_to_data = {}
    text_id_to_data = {}
    
    # å›¾åƒæ•°æ®æ˜ å°„
    for i, pid in enumerate(image_patient_ids):
        image_id_to_data[pid] = {
            'data': image_data[i],
            'label': image_labels[i],
            'index': i
        }
    
    # æ–‡æœ¬æ•°æ®æ˜ å°„
    for i, pid in enumerate(patient_ids):
        text_id_to_data[pid] = {
            'data': text_data[i],
            'label': text_labels[i],
            'index': i
        }
    
    # 4. æ‰¾åˆ°å…±åŒçš„æ‚£è€…ID
    image_patient_set = set(image_patient_ids)
    text_patient_set = set(patient_ids)
    common_patients = image_patient_set & text_patient_set
    
    print(f"ğŸ“Š ä¿®å¤åæ‚£è€…IDå¯¹é½ç»Ÿè®¡:")
    print(f"   å›¾åƒæ‚£è€…æ•°: {len(image_patient_set)}")
    print(f"   æ–‡æœ¬æ‚£è€…æ•°: {len(text_patient_set)}")
    print(f"   å…±åŒæ‚£è€…æ•°: {len(common_patients)}")
    
    if len(common_patients) < len(image_patient_set) * 0.8:
        print(f"âš ï¸  è­¦å‘Š: å…±åŒæ‚£è€…æ¯”ä¾‹ä»ç„¶è¾ƒä½ ({len(common_patients)}/{len(image_patient_set)})")
        
        # æ˜¾ç¤ºä¸åŒ¹é…çš„æ‚£è€…IDè¿›è¡Œè°ƒè¯•
        image_only = image_patient_set - text_patient_set
        text_only = text_patient_set - image_patient_set
        
        if image_only:
            print(f"   ä»…åœ¨å›¾åƒä¸­: {sorted(list(image_only))[:10]}...")
        if text_only:
            print(f"   ä»…åœ¨æ–‡æœ¬ä¸­: {sorted(list(text_only))[:10]}...")
    else:
        print(f"âœ… æ‚£è€…IDå¯¹é½æˆåŠŸç‡: {len(common_patients)/len(image_patient_set)*100:.1f}%")
    
    # 5. æŒ‰å…±åŒæ‚£è€…IDé‡æ–°ç»„ç»‡æ•°æ®
    common_patients_list = sorted(list(common_patients))  # æ’åºç¡®ä¿ä¸€è‡´æ€§
    
    aligned_image_data = []
    aligned_text_data = []
    aligned_labels = []
    aligned_patient_ids = []
    
    for patient_id in common_patients_list:
        # éªŒè¯æ ‡ç­¾ä¸€è‡´æ€§
        img_label = image_id_to_data[patient_id]['label']
        txt_label = text_id_to_data[patient_id]['label']
        
        if img_label != txt_label:
            print(f"âš ï¸  æ‚£è€… {patient_id} æ ‡ç­¾ä¸ä¸€è‡´: å›¾åƒ={img_label}, æ–‡æœ¬={txt_label}")
            continue
        
        aligned_image_data.append(image_id_to_data[patient_id]['data'])
        aligned_text_data.append(text_id_to_data[patient_id]['data'])
        aligned_labels.append(img_label)
        aligned_patient_ids.append(patient_id)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    aligned_image_data = np.array(aligned_image_data)
    aligned_labels = np.array(aligned_labels)
    
    print(f"âœ… ä¿®å¤åæ‚£è€…IDå¯¹é½å®Œæˆ:")
    print(f"   å¯¹é½åæ ·æœ¬æ•°: {len(aligned_labels)}")
    print(f"   å›¾åƒæ•°æ®å½¢çŠ¶: {aligned_image_data.shape}")
    print(f"   æ–‡æœ¬æ•°æ®æ•°é‡: {len(aligned_text_data)}")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: AD={np.sum(aligned_labels==1)}, CN={np.sum(aligned_labels==0)}")
    
    # 6. éªŒè¯å¯¹é½æ•ˆæœ
    print(f"ğŸ” éªŒè¯å‰5ä¸ªæ ·æœ¬çš„æ‚£è€…IDå¯¹é½:")
    for i in range(min(5, len(aligned_patient_ids))):
        print(f"   æ ·æœ¬{i}: æ‚£è€…ID={aligned_patient_ids[i]}, æ ‡ç­¾={aligned_labels[i]}")
    
    print(f"\nğŸ“Š æ€»æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬: {len(aligned_labels)}")
    print(f"   ADæ ·æœ¬: {np.sum(aligned_labels==1)}")
    print(f"   CNæ ·æœ¬: {np.sum(aligned_labels==0)}")
    
    # 7. æ‚£è€…çº§åˆ«åˆ†å‰² - å…ˆåˆ†ç¦»holdoutæµ‹è¯•é›†
    from sklearn.model_selection import StratifiedShuffleSplit
    
    # ç¬¬ä¸€æ¬¡åˆ†å‰²: åˆ†ç¦»holdoutæµ‹è¯•é›†
    holdout_splitter = StratifiedShuffleSplit(
        n_splits=1,
        test_size=holdout_test_size,
        random_state=random_state
    )
    
    cv_idx, holdout_idx = next(holdout_splitter.split(aligned_image_data, aligned_labels))
    
    # åˆ†ç¦»æ•°æ®
    cv_images = aligned_image_data[cv_idx]
    cv_texts = [aligned_text_data[i] for i in cv_idx]
    cv_labels = aligned_labels[cv_idx]
    
    holdout_images = aligned_image_data[holdout_idx]
    holdout_texts = [aligned_text_data[i] for i in holdout_idx]
    holdout_labels = aligned_labels[holdout_idx]
    
    print(f"\nğŸ¯ æ•°æ®åˆ†å‰²ç»“æœ (é˜²æ³„éœ²):")
    print(f"   äº¤å‰éªŒè¯é›†: {len(cv_labels)} æ ·æœ¬ (AD={np.sum(cv_labels==1)}, CN={np.sum(cv_labels==0)})")
    print(f"   ç‹¬ç«‹æµ‹è¯•é›†: {len(holdout_labels)} æ ·æœ¬ (AD={np.sum(holdout_labels==1)}, CN={np.sum(holdout_labels==0)})")
    print(f"   äº¤å‰éªŒè¯æ¯”ä¾‹: {len(cv_labels)/len(aligned_labels)*100:.1f}%")
    print(f"   ç‹¬ç«‹æµ‹è¯•æ¯”ä¾‹: {len(holdout_labels)/len(aligned_labels)*100:.1f}%")
    
    # 8. éªŒè¯æ•°æ®åˆ†ç¦»
    cv_patient_ids_list = [aligned_patient_ids[i] for i in cv_idx]
    holdout_patient_ids_list = [aligned_patient_ids[i] for i in holdout_idx]
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰é‡å 
    cv_indices = set(cv_idx)
    holdout_indices = set(holdout_idx)
    overlap = cv_indices & holdout_indices
    
    if overlap:
        print(f"âš ï¸  è­¦å‘Š: å‘ç°æ•°æ®é‡å : {len(overlap)} ä¸ªæ ·æœ¬")
    else:
        print(f"âœ… æ•°æ®åˆ†ç¦»éªŒè¯é€šè¿‡: äº¤å‰éªŒè¯é›†å’Œç‹¬ç«‹æµ‹è¯•é›†æ— é‡å ")
    
    # æ£€æŸ¥æ‚£è€…IDé‡å 
    cv_patient_set = set(cv_patient_ids_list)
    holdout_patient_set = set(holdout_patient_ids_list)
    patient_overlap = cv_patient_set & holdout_patient_set
    
    if patient_overlap:
        print(f"âš ï¸  è­¦å‘Š: å‘ç°æ‚£è€…IDé‡å : {patient_overlap}")
    else:
        print(f"âœ… æ‚£è€…çº§åˆ«åˆ†ç¦»éªŒè¯é€šè¿‡: æ— æ‚£è€…é‡å ")
    
    return (cv_images, cv_texts, cv_labels,
            holdout_images, holdout_texts, holdout_labels)


def main():
    """ä¸»å‡½æ•° - ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ è®­ç»ƒ"""
    print("ğŸ”§ ä¸‰é‡æŸå¤±ç«¯åˆ°ç«¯å¯¹æ¯”å­¦ä¹ æ¨¡å‹è®­ç»ƒ - ä¿®å¤æ‚£è€…IDç‰ˆæœ¬")
    print("="*50)
    
    # ğŸ”¥ åŠ¨æ€è·å–æœ€ä½³å›¾åƒæ¨¡å‹è·¯å¾„
    def get_best_image_model_path():
        """æ™ºèƒ½è·å–æœ€ä½³å›¾åƒæ¨¡å‹è·¯å¾„"""
        print("ğŸ” æœç´¢æœ€ä½³å›¾åƒç¼–ç å™¨æ¨¡å‹...")
        
        # ğŸ”¥ ä¼˜å…ˆçº§1: 5æŠ˜äº¤å‰éªŒè¯çš„æœ€ä½³æ¨¡å‹
        cv_models = [
            ('./models/contrastive/fold_1_best_model.pth', 'ç¬¬2æŠ˜æœ€ä½³æ¨¡å‹ (94.74%)'),
            ('./models/contrastive/fold_0_best_model.pth', 'ç¬¬1æŠ˜æ¨¡å‹'),
            ('./models/contrastive/fold_2_best_model.pth', 'ç¬¬3æŠ˜æ¨¡å‹'),
            ('./models/contrastive/fold_3_best_model.pth', 'ç¬¬4æŠ˜æ¨¡å‹'),
            ('./models/contrastive/fold_4_best_model.pth', 'ç¬¬5æŠ˜æ¨¡å‹'),
            ('./models/contrastive/best_contrastive_image_encoder.pth', 'æ€»ä½“æœ€ä½³æ¨¡å‹'),
            ('./models/å¤‡ä»½1/contrastive_image_encoder_ch12.pth', 'å¤‡ä»½æ¨¡å‹ch12'),  # ğŸ”¥ ä¿®å¤è·¯å¾„
            ('./models/å¤‡ä»½1/contrastive_image_encoder_ch8.pth', 'å¤‡ä»½æ¨¡å‹ch8')   # ğŸ”¥ æ–°å¢å¤‡ä»½è·¯å¾„
        ]
        
        for model_path, description in cv_models:
            if os.path.exists(model_path):
                try:
                    checkpoint = torch.load(model_path, map_location='cpu')
                    # å°è¯•ä»ä¸åŒçš„é”®è·å–å‡†ç¡®ç‡
                    val_acc = checkpoint.get('best_val_accuracy', 
                              checkpoint.get('val_accuracy', 
                              checkpoint.get('val_acc', 0)))
                    
                    print(f"âœ… ä½¿ç”¨ä¼˜é€‰å›¾åƒæ¨¡å‹: {description}")
                    print(f"   æ–‡ä»¶è·¯å¾„: {model_path}")
                    print(f"   éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
                    return model_path
                except Exception as e:
                    print(f"   âš ï¸ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥ {model_path}: {e}")
                    continue
        
        # ğŸ”¥ ä¼˜å…ˆçº§2: å¤‡ç”¨æ¨¡å‹ï¼ˆæ›´æ–°è·¯å¾„ï¼‰
        backup_models = [
            './models/contrastive/fold_1_best_model.pth',  # æœ€ä½³æ¨¡å‹é‡å¤æ£€æŸ¥
            './models/smart_downsample_global_ch12.pth',
            './models/best_memory_optimized_early_fusion.pth'
        ]
        
        for model_path in backup_models:
            if os.path.exists(model_path):
                try:
                    checkpoint = torch.load(model_path, map_location='cpu')
                    val_acc = checkpoint.get('val_acc', 0)
                    print(f"âœ… ä½¿ç”¨å¤‡ç”¨å›¾åƒæ¨¡å‹: {os.path.basename(model_path)}")
                    print(f"   éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
                    return model_path
                except:
                    continue
        
        raise FileNotFoundError("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„å›¾åƒæ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒå›¾åƒç¼–ç å™¨")
    
    # è·å–å›¾åƒæ¨¡å‹è·¯å¾„
    try:
        image_model_path = get_best_image_model_path()
    except FileNotFoundError as e:
        print(str(e))
        print("ğŸ’¡ è¯·è¿è¡Œ: python run_contrastive_image_encoder.py")
        return
    
    # ä¿®å¤é…ç½® - é‡æ–°è°ƒæ•´å‚æ•°
    config = {
        'image_model_path': image_model_path,  # ğŸ”¥ ä½¿ç”¨åŠ¨æ€æ£€æµ‹çš„è·¯å¾„
        'text_data_dir': './æ–‡æœ¬ç¼–ç å™¨/',
        'image_data_dir': '/root/autodl-tmp/DATA_MCI/test_data/',
        'save_dir': './models/triple_loss_contrastive',
        
        # ç«¯åˆ°ç«¯è®­ç»ƒå‚æ•°
        'batch_size': 8,        # å°æ‰¹æ¬¡ï¼Œé˜²è¿‡æ‹Ÿåˆ
        'num_epochs': 20,       # å‡å°‘è½®æ•°
        'learning_rate': 1e-4,  # ä¿å®ˆå­¦ä¹ ç‡
        'weight_decay': 1e-3,   # å¼ºæ­£åˆ™åŒ–
        'gradient_clip': 1.0,
        
        # ä¸‰é‡æŸå¤±æƒé‡é…ç½®
        'classification_weight': 1.0,    # AD/CNåˆ†ç±»æŸå¤±æƒé‡
        'alignment_weight': 0.5,         # å›¾åƒæ–‡æœ¬å¯¹é½æŸå¤±æƒé‡
        'intra_modal_weight': 0.3,       # å›¾åƒå†…éƒ¨å¯¹æ¯”æŸå¤±æƒé‡
        
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print(f"ğŸ¯ ä¸‰é‡æŸå¤±ç³»ç»Ÿç‰¹ç‚¹:")
    print(f"   1ï¸âƒ£ AD/CNåˆ†ç±»æŸå¤±: {config['classification_weight']} (ä¸»è¦ä»»åŠ¡)")
    print(f"   2ï¸âƒ£ å›¾åƒæ–‡æœ¬å¯¹é½æŸå¤±: {config['alignment_weight']} (è·¨æ¨¡æ€å¯¹é½)")
    print(f"   3ï¸âƒ£ å›¾åƒå†…éƒ¨å¯¹æ¯”æŸå¤±: {config['intra_modal_weight']} (ç‰¹å¾åˆ†å¸ƒä¼˜åŒ–)")
    print(f"   âœ… ç«¯åˆ°ç«¯BERTè®­ç»ƒ")
    print(f"   âœ… ä¸¥æ ¼æ•°æ®åˆ†ç¦»")
    print(f"   ğŸ”§ ä¿®å¤æ‚£è€…IDå¯¹é½é—®é¢˜")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config['save_dir'], exist_ok=True)
    
    try:
        # åŠ è½½æ•°æ® - ä¸¥æ ¼é˜²æ•°æ®æ³„éœ²
        (train_images, train_texts, train_labels,
         test_images, test_texts, test_labels) = load_end_to_end_data(
            config['image_data_dir'], config['text_data_dir']
        )
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ¯ åˆ›å»ºä¸‰é‡æŸå¤±å¤šæ¨¡æ€æ¨¡å‹...")
        model = ImprovedMultiModalModel(
            config['image_model_path'],
            train_texts + test_texts,  # ä¼ å…¥æ‰€æœ‰æ–‡æœ¬æ•°æ®
            feature_dim=512,
            device=config['device']
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = EndToEndMultiModalDataset(train_images, train_texts, train_labels)
        test_dataset = EndToEndMultiModalDataset(test_images, test_texts, test_labels)
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=False)
        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=False)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = OptimizedContrastiveTrainer(model, config['device'], config)
        
        print(f"ğŸš€ å¼€å§‹ä¸‰é‡æŸå¤±è®­ç»ƒï¼Œå…± {config['num_epochs']} è½®...")
        print(f"ğŸ¯ ç›®æ ‡: ä¼˜åŒ–ä¸‰ç§æŸå¤±ï¼Œæå‡å¤šæ¨¡æ€æ€§èƒ½")
        
        best_test_accuracy = 0.0
        training_history = []
        
        for epoch in range(config['num_epochs']):
            # è®­ç»ƒ
            train_metrics = trainer.train_epoch(train_loader, epoch)
            
            # æµ‹è¯•
            test_metrics = trainer.evaluate(test_loader)
            
            # è®°å½•å†å²
            epoch_history = {
                'epoch': epoch + 1,
                'train': train_metrics,
                'test': test_metrics
            }
            training_history.append(epoch_history)
            
            # æ‰“å°è¯¦ç»†ç»“æœ
            print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"   è®­ç»ƒ - æ€»æŸå¤±: {train_metrics['total_loss']:.4f}, å‡†ç¡®ç‡: {train_metrics['accuracy']:.4f}")
            print(f"   è®­ç»ƒ - åˆ†ç±»: {train_metrics['classification_loss']:.4f}, å¯¹é½: {train_metrics['alignment_loss']:.4f}, å†…éƒ¨: {train_metrics['intra_modal_loss']:.4f}")
            print(f"   æµ‹è¯• - æ€»æŸå¤±: {test_metrics['loss']:.4f}, å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
            print(f"   æµ‹è¯• - åˆ†ç±»: {test_metrics['classification_loss']:.4f}, å¯¹é½: {test_metrics['alignment_loss']:.4f}, å†…éƒ¨: {test_metrics['intra_modal_loss']:.4f}")
            print(f"   å­¦ä¹ ç‡: {train_metrics['learning_rate']:.6f}")
            
            # æŸå¤±åˆ†æ
            if train_metrics['alignment_loss'] < 1.0:
                print(f"   âœ… å›¾åƒæ–‡æœ¬å¯¹é½è‰¯å¥½: {train_metrics['alignment_loss']:.4f}")
            else:
                print(f"   âš ï¸  å›¾åƒæ–‡æœ¬å¯¹é½éœ€æ”¹è¿›: {train_metrics['alignment_loss']:.4f}")
            
            if train_metrics['intra_modal_loss'] < 0.5:
                print(f"   âœ… å›¾åƒç‰¹å¾åˆ†å¸ƒä¼˜åŒ–è‰¯å¥½: {train_metrics['intra_modal_loss']:.4f}")
            else:
                print(f"   ğŸ”„ å›¾åƒç‰¹å¾åˆ†å¸ƒæŒç»­ä¼˜åŒ–: {train_metrics['intra_modal_loss']:.4f}")
            
            # æ³›åŒ–èƒ½åŠ›æ£€æŸ¥
            train_acc = train_metrics['accuracy']
            test_acc = test_metrics['accuracy']
            generalization_gap = train_acc - test_acc
            
            if generalization_gap > 0.1:
                print(f"   âš ï¸  è¿‡æ‹Ÿåˆè­¦å‘Š: è®­ç»ƒ-æµ‹è¯•å·®è· {generalization_gap:.3f}")
            elif generalization_gap > 0.05:
                print(f"   ğŸ”„ è½»å¾®è¿‡æ‹Ÿåˆ: è®­ç»ƒ-æµ‹è¯•å·®è· {generalization_gap:.3f}")
            else:
                print(f"   âœ… æ³›åŒ–è‰¯å¥½: è®­ç»ƒ-æµ‹è¯•å·®è· {generalization_gap:.3f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_metrics['accuracy'] > best_test_accuracy:
                best_test_accuracy = test_metrics['accuracy']
                save_path = os.path.join(config['save_dir'], 'best_triple_loss_model.pth')
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': trainer.optimizer.state_dict(),
                    'best_test_accuracy': best_test_accuracy,
                    'config': config,
                    'train_metrics': train_metrics,
                    'test_metrics': test_metrics
                }, save_path)
                
                print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: æµ‹è¯•å‡†ç¡®ç‡ {best_test_accuracy:.4f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(config['save_dir'], 'triple_loss_training_history.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨
            serializable_history = []
            for epoch_data in training_history:
                epoch_copy = {}
                for key, value in epoch_data.items():
                    if key in ['train', 'test']:
                        metrics_copy = {}
                        for metric_key, metric_value in value.items():
                            if hasattr(metric_value, 'tolist'):
                                metrics_copy[metric_key] = metric_value.tolist()
                            elif isinstance(metric_value, (int, float, str, bool)):
                                metrics_copy[metric_key] = metric_value
                            elif isinstance(metric_value, dict):
                                # å¤„ç†åˆ†ç±»æŠ¥å‘Šç­‰å­—å…¸
                                metrics_copy[metric_key] = {k: v for k, v in metric_value.items() 
                                                          if isinstance(v, (int, float, str, bool, dict))}
                        epoch_copy[key] = metrics_copy
                    else:
                        epoch_copy[key] = value
                serializable_history.append(epoch_copy)
            
            json.dump(serializable_history, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ ä¸‰é‡æŸå¤±è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ˆ æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_accuracy:.4f}")
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {config['save_dir']}")
        
        # æ€§èƒ½åˆ†æ
        print(f"\nğŸ“Š ä¸‰é‡æŸå¤±æ€§èƒ½è¯„ä¼°:")
        print(f"   å›¾åƒå•æ¨¡æ€åŸºçº¿: 77.42%")
        print(f"   ä¸‰é‡æŸå¤±å¤šæ¨¡æ€: {best_test_accuracy*100:.2f}%")
        
        if best_test_accuracy > 0.7742:
            improvement = (best_test_accuracy - 0.7742) * 100
            print(f"   ğŸ† ç›¸å¯¹åŸºçº¿æå‡: +{improvement:.2f}%")
        else:
            gap = (0.7742 - best_test_accuracy) * 100
            print(f"   ğŸ“‰ è·ç¦»åŸºçº¿å·®è·: -{gap:.2f}%")
        
        print(f"\nâœ… ä¸‰é‡æŸå¤±ç³»ç»Ÿä¼˜åŠ¿:")
        print(f"   1ï¸âƒ£ AD/CNåˆ†ç±»æŸå¤±: ç¡®ä¿ä¸»è¦ä»»åŠ¡æ€§èƒ½")
        print(f"   2ï¸âƒ£ å›¾åƒæ–‡æœ¬å¯¹é½æŸå¤±: ä¼˜åŒ–è·¨æ¨¡æ€ç‰¹å¾å¯¹é½")
        print(f"   3ï¸âƒ£ å›¾åƒå†…éƒ¨å¯¹æ¯”æŸå¤±: ä¼˜åŒ–ç‰¹å¾åˆ†å¸ƒè´¨é‡")
        print(f"   ğŸ›¡ï¸ ä¸¥æ ¼æ•°æ®åˆ†ç¦»: çœŸå®æ³›åŒ–èƒ½åŠ›è¯„ä¼°")
        print(f"   ğŸ”§ ä¿®å¤æ‚£è€…IDå¯¹é½: ç¡®ä¿æ•°æ®ä¸€è‡´æ€§")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 